# Task 95 Implementation Progress Log

## Implementation Phases

1. **Phase 1: Bug Fix** - Scope monkey-patch to planner-only path (CURRENT)
2. **Phase 2: Discovery Migration** - Remove monkey-patch from discovery commands
3. **Phase 3: Smart Filter Migration** - Use configurable model
4. **Phase 4: Configuration** - Deferred to later task

---

## [2025-01-XX 00:00] - Starting Phase 1 Implementation

### Goal
Move `_install_anthropic_model_if_needed(verbose)` from line 3550 (before workflow type determination) to after the file/saved workflow branch (only for natural language planner path).

### Key Files
- `src/pflow/cli/main.py` - Main change location

### Understanding the Code Flow

Reading `main.py` workflow_command() to understand exact branching:

```
Line 3550: _install_anthropic_model_if_needed(verbose)  # TOO EARLY
Line 3577: if _try_execute_named_workflow(...): return  # File/saved exit here
Line 3580: if not _is_valid_natural_language_input(...): return  # Invalid input exits
Line 3589: _execute_with_planner(...)  # Natural language path
```

The fix: Move line 3550 to just before line 3589.

---

## [2025-01-XX 00:05] - Implementing the Fix

Attempting to move the monkey-patch installation...

### Changes Made

**File**: `src/pflow/cli/main.py`

1. **Removed** early monkey-patch call (was at line 3549-3550):
   ```python
   # REMOVED from here (too early - affects ALL workflows)
   # Install Anthropic model wrapper for planning models
   _install_anthropic_model_if_needed(verbose)
   ```

2. **Added** monkey-patch call just before planner execution (now at line 3584-3587):
   ```python
   # Install Anthropic model wrapper ONLY for planner path
   # This provides caching, thinking tokens, and structured output features
   # that the planner requires. File/saved workflows use standard llm library.
   _install_anthropic_model_if_needed(verbose)
   ```

### Code Flow After Fix

```
workflow_command()
  â”œâ”€â”€ _initialize_context(...)
  â”œâ”€â”€ _auto_discover_mcp_servers(...)
  â”œâ”€â”€ _read_stdin_data()
  â”œâ”€â”€ _validate_workflow_flags(...)
  â”‚
  â”œâ”€â”€ if _try_execute_named_workflow(...):
  â”‚     return  # â† File/saved workflows EXIT HERE (no monkey-patch)
  â”‚
  â”œâ”€â”€ if not _is_valid_natural_language_input(...):
  â”‚     return  # â† Invalid input exits here
  â”‚
  â”œâ”€â”€ _install_anthropic_model_if_needed(verbose)  # â† NOW HERE (planner only)
  â”‚
  â””â”€â”€ _execute_with_planner(...)  # â† Natural language continues
```

âœ… **Fix applied successfully**

---

## [2025-01-XX 00:10] - Running Test Suite

Running `make test` to verify fix doesn't break existing functionality...

### Results

```
make test
======================= 3367 passed, 7 skipped in 8.34s ========================
```

âœ… All tests pass!

```
make check
ðŸš€ Linting code: Running pre-commit ... Passed
ðŸš€ Static type checking: Running mypy ... Success: no issues found in 156 source files
ðŸš€ Checking for obsolete dependencies: Running deptry ... Success!
```

âœ… All code quality checks pass!

---

## [2025-01-XX 00:15] - Phase 1 Complete

### Summary of Changes

**File Modified**: `src/pflow/cli/main.py`

**Change**: Moved `_install_anthropic_model_if_needed(verbose)` from line 3550 (before workflow type determination) to line 3587 (after file/saved workflow branch, only for natural language planner path).

### Behavior Change

| Workflow Type | Before (Buggy) | After (Fixed) |
|---------------|----------------|---------------|
| File (`pflow workflow.json`) | Monkey-patch installed, user model ignored | No patch, uses specified model via llm library |
| Saved (`pflow my-workflow`) | Monkey-patch installed, user model ignored | No patch, uses specified model via llm library |
| Natural language (`pflow "do X"`) | Monkey-patch installed, planner works | Monkey-patch installed, planner works |

### Testing Notes

- All 3367 existing tests pass
- The fix affects runtime behavior for file/saved workflows
- Proper integration testing would require real LLM API calls
- Existing LLM node tests validate the llm library integration works correctly

### Ready for User Review

Phase 1 is complete. The bug fix has been implemented and all tests pass.

**Next steps** (pending user review):
- Phase 2: Migrate discovery commands to use standard llm library
- Phase 3: Migrate smart_filter.py to use configurable model

---

## Phase 2: Discovery Commands Migration

### [Time] - Key Discovery: Nodes Already Support Non-Anthropic Models!

Investigated `src/pflow/planning/nodes.py` and found that **the planning nodes already handle non-Anthropic models gracefully**.

**Evidence**:

1. **Helper function at lines 105-153**: `_build_llm_kwargs()` detects model type and handles differences:
   ```python
   if _is_anthropic_model(model_name):
       # Use cache_blocks parameter
   else:
       # Flatten cache_blocks into prompt, disable streaming
   ```

2. **Model detection at lines 38-40**: `_is_anthropic_model()` checks the model name prefix.

3. **Parse function at llm_helpers.py:15-85**: `parse_structured_response()` uses the universal `text()` method, works with ANY LLM provider.

**Conclusion**: We just need to:
1. Remove the monkey-patch
2. Set `model_name` to user's configured model (via `get_default_llm_model()`)
3. The existing code handles the rest!

### Implementation Approach

For each discovery command:
1. Remove `install_anthropic_model()` call
2. Add `model_name` to shared store using `get_default_llm_model()` with Anthropic fallback
3. The nodes will use `_build_llm_kwargs()` to handle provider differences

---

### [Time] - Implementing Discovery Command Changes

**Files Modified**:

1. **`src/pflow/cli/registry.py`** (registry discover):
   - Removed `import os` and `install_anthropic_model()` call
   - Added `get_default_llm_model()` import
   - Added `model_name` to shared store

2. **`src/pflow/cli/commands/workflow.py`** (workflow discover):
   - Removed `import os` and `install_anthropic_model()` call
   - Added `get_default_llm_model()` import
   - Added `model_name` to shared store

3. **`src/pflow/cli/commands/workflow.py`** (metadata generation):
   - Removed `import os` and `install_anthropic_model()` call
   - Added `get_default_llm_model()` import
   - Pass `model_name` to `generate_workflow_metadata()`

4. **`src/pflow/core/workflow_save_service.py`**:
   - Added optional `model_name` parameter to `generate_workflow_metadata()`
   - Pass `model_name` to shared store if provided

### [Time] - Phase 2 Verification

```
make check â†’ All checks passed âœ…
make test  â†’ 3367 passed, 7 skipped âœ…
```

âœ… **Phase 2 Complete**

---

## Phase 3: Smart Filter Migration

### [Time] - Starting Phase 3

Goal: Replace hardcoded `anthropic/claude-haiku-4-5-20251001` in smart_filter.py with configurable model.

### Implementation

**Combined Phase 3+4**: Implemented settings-based model configuration for all non-planner LLM usage.

**Files Modified**:

1. **`src/pflow/core/settings.py`**:
   - Added `LLMSettings` class with `discovery_model` and `filtering_model` fields
   - Added to `PflowSettings` as `llm: LLMSettings`

2. **`src/pflow/core/llm_config.py`**:
   - Added `get_model_for_feature(feature)` helper function
   - Implements resolution order: settings â†’ auto-detect â†’ fallback

3. **`src/pflow/cli/registry.py`**:
   - Updated to use `get_model_for_feature("discovery")`

4. **`src/pflow/cli/commands/workflow.py`**:
   - Updated discover command to use `get_model_for_feature("discovery")`
   - Updated metadata generation to use `get_model_for_feature("discovery")`

5. **`src/pflow/core/smart_filter.py`**:
   - Updated to use `get_model_for_feature("filtering")`

6. **`tests/shared/llm_mock.py`**:
   - Added wildcard model support (`"*"`) for flexible test mocking
   - Added default response for `FilteredFields`

7. **Test files updated**:
   - `tests/test_core/test_smart_filter.py` - use `"*"` wildcard model
   - `tests/test_execution/formatters/test_node_output_formatter.py` - use `"*"` wildcard model

### Settings Structure

```json
{
  "version": "1.0.0",
  "llm": {
    "discovery_model": null,    // null = auto-detect
    "filtering_model": null     // null = auto-detect
  }
}
```

### Model Resolution Order

```
get_model_for_feature("discovery") or get_model_for_feature("filtering")
  â†“
1. Check settings.llm.{feature}_model
  â†“ (if null)
2. Check get_default_llm_model() (auto-detect from API keys)
  â†“ (if null)
3. Fallback to "anthropic/claude-sonnet-4-5"
```

### Verification

```
make check â†’ All checks passed âœ…
make test  â†’ 3367 passed, 7 skipped âœ…
```

âœ… **Phase 3+4 Complete**

---

## Implementation Summary

### All Changes Made

| Phase | Component | Change |
|-------|-----------|--------|
| 1 | `main.py` | Moved monkey-patch to planner-only path |
| 2 | `registry.py` | Use `get_model_for_feature("discovery")` |
| 2 | `workflow.py` | Use `get_model_for_feature("discovery")` |
| 3+4 | `settings.py` | Added `LLMSettings` with `discovery_model` and `filtering_model` |
| 3+4 | `llm_config.py` | Added `get_model_for_feature()` helper |
| 3+4 | `smart_filter.py` | Use `get_model_for_feature("filtering")` |
| 3+4 | `workflow_save_service.py` | Accept optional `model_name` parameter |
| Tests | `llm_mock.py` | Added wildcard model support |
| Tests | Test files | Updated to use `"*"` wildcard |

### Behavior Before/After

**Before (Buggy)**:
- ALL workflows (file, saved, natural language) got monkey-patch
- User model choice ignored for Claude models
- Discovery commands required Anthropic API key
- Smart filter hardcoded to Haiku model

**After (Fixed)**:
- Only natural language planner gets monkey-patch
- User model choice respected for file/saved workflows
- Discovery commands work with any configured provider
- All non-planner LLM usage configurable via settings

### Configuration Options

Users can now configure models in `~/.pflow/settings.json`:

```json
{
  "llm": {
    "discovery_model": "gemini-2.5-flash",
    "filtering_model": "gemini-2.5-flash-lite"
  }
}
```

Or leave as `null` for auto-detection based on available API keys.

---

## [2025-01-XX 00:20] - Manual Verification

### Test 1: Valid Model (Gemini) in File Workflow

```bash
$ cat /tmp/test-model-fix.json
{"ir_version": "0.1.0", "nodes": [{"id": "test-llm", "type": "llm", "params": {"prompt": "Say exactly: Hello from the specified model", "model": "gemini-2.5-flash-lite"}}]}

$ pflow /tmp/test-model-fix.json
âœ“ Workflow completed in 0.984s
Nodes executed (1):
  âœ“ test-llm (633ms)
ðŸ’° Cost: $0.0000

Workflow output:
Hello from the specified model
```

âœ… **PASS** - File workflow uses the specified Gemini model (not redirected to Claude)

### Test 2: Invalid Model Name in File Workflow

```bash
$ cat /tmp/test-invalid-model.json
{"ir_version": "0.1.0", "nodes": [{"id": "test-llm", "type": "llm", "params": {"prompt": "Say hello", "model": "totally-fake-model-12345"}}]}

$ pflow /tmp/test-invalid-model.json
âŒ Workflow execution failed
Error 1 at node 'test-llm':
  Category: execution_failure
  Message: LLM call failed after 3 attempts. Model: totally-fake-model-12345
```

âœ… **PASS** - Invalid model now properly fails (before fix: would silently "succeed" with wrong model)

### Test 3: Consistency Between File Workflow and Registry Run

```bash
$ pflow registry run llm prompt="Say hi" model="gemini-2.5-flash-lite"
âœ“ Node executed successfully

$ pflow registry run llm prompt="Say hi" model="totally-fake-model-12345"
âŒ Node execution failed
Error: LLM call failed after 3 attempts. Model: totally-fake-model-12345
```

âœ… **PASS** - Both `registry run` and file workflows behave identically

### Verification Summary

| Scenario | Before Fix | After Fix | Status |
|----------|------------|-----------|--------|
| Valid Gemini model in file workflow | Redirected to Claude | Uses Gemini | âœ… FIXED |
| Invalid model in file workflow | Silently used hardcoded Claude | Proper error | âœ… FIXED |
| registry run vs file workflow consistency | Inconsistent | Consistent | âœ… FIXED |

**Phase 1 bug fix is verified and working correctly.**

---

## Final Verification (Post Phase 3+4)

After implementing settings-based model configuration, re-verified:

1. **Settings integration**: `pflow settings show` displays new `llm` section with `discovery_model` and `filtering_model`

2. **Model resolution chain**: Verified via Python REPL that resolution order works:
   - Settings file â†’ Auto-detect â†’ Fallback
   - When `discovery_model: "gemini-2.5-flash"` is set in settings, `get_model_for_feature("discovery")` returns it

3. **File workflow still works**: Re-ran LLM workflow with `model: "gemini-2.5-flash-lite"` - correctly used Gemini (cost $0.0000, not Anthropic pricing)

**All phases verified and working.**

---

## Key Decisions Made

1. **Wildcard model support in tests**: Added `"*"` pattern to `MockGetModel` to allow tests to set responses that match any model name. This was necessary because `get_model_for_feature()` returns different models based on settings/API keys.

2. **Combined Phase 3+4**: Implemented settings configuration alongside smart_filter migration for cleaner, single-pass implementation.

3. **Resolution order**: Settings â†’ Auto-detect â†’ Fallback. This lets users override when needed while providing sensible defaults.

---

## Pending: Update to Latest Model Versions

User requested updating to newest model versions. Research completed:

### Current State
| Provider | Current Model | Current Plugin Version |
|----------|---------------|----------------------|
| Anthropic | `anthropic/claude-sonnet-4-5` | llm-anthropic 0.20 |
| Gemini | `gemini/gemini-2.0-flash-lite` | llm-gemini 0.25 |
| OpenAI | `gpt-4o-mini` | llm 0.27.1 (built-in) |

### Target Models (per user request)
| Provider | Target Model | Notes |
|----------|-------------|-------|
| Anthropic | `anthropic/claude-haiku-4-5-20251001` | Already available in current plugin |
| Gemini | `gemini-3-flash-preview` | Requires llm-gemini 0.27+ |
| OpenAI | `gpt-5.2` | Available in current llm version |

### Required Changes

1. **Update pyproject.toml**: Bump `llm-gemini>=0.27` to get Gemini 3 support

2. **Update llm_config.py** (`_detect_default_model`):
   - Anthropic: `anthropic/claude-sonnet-4-5` â†’ `anthropic/claude-haiku-4-5-20251001`
   - Gemini: `gemini/gemini-2.0-flash-lite` â†’ `gemini/gemini-3-flash-preview`
   - OpenAI: `gpt-4o-mini` â†’ `gpt-5.2`

3. **Update LLM node default** (`nodes/llm/llm.py`):
   - `gemini-2.5-flash-lite` â†’ `gemini-3-flash-preview` (or keep as cheaper option)

4. **Update planning nodes default** (`planning/nodes.py`):
   - Keep `anthropic/claude-sonnet-4-0` for planner (needs advanced features)

### Sources
- [llm-gemini releases](https://github.com/simonw/llm-gemini/releases) - v0.27 adds Gemini 3 support
- [Anthropic Claude Haiku 4.5](https://www.anthropic.com/news/claude-haiku-4-5)
- [OpenAI GPT-5.2](https://openai.com/index/introducing-gpt-5-2/)
- [Gemini 3 Flash](https://blog.google/products/gemini/gemini-3-flash/)

### Status: âœ… IMPLEMENTED

---

## [2025-12-19] - Update to Latest Model Versions

### Changes Made

1. **Updated pyproject.toml**:
   - `llm-gemini>=0.25` â†’ `llm-gemini>=0.28.1` (for Gemini 3 Flash support)

2. **Updated llm_config.py** (`_detect_default_model`):
   - Gemini: `gemini/gemini-2.0-flash-lite` â†’ `gemini/gemini-3-flash-preview`
   - OpenAI: `gpt-4o-mini` â†’ `gpt-5.2`
   - Anthropic: Kept `anthropic/claude-sonnet-4-5` (still best for planning)

3. **Updated llm_pricing.py**:
   - Added `gpt-5.2` pricing: $1.75/M input, $14/M output
   - Added `gpt-5.2-pro` pricing: $21/M input, $168/M output
   - Added `gemini-3-flash-preview` pricing: $0.50/M input, $3/M output
   - Added aliases: `5.2`, `gpt5`, `5.2-pro`, `gemini-3-flash`
   - Updated `PRICING_VERSION` to `2025-12-19`

4. **Updated LLM node default** (`nodes/llm/llm.py`):
   - `gemini-2.5-flash-lite` â†’ `gemini-3-flash-preview`

5. **Updated test** (`test_cost_in_trace.py`):
   - Updated pricing version assertion to `2025-12-19`

6. **Regenerated lockfile**:
   - `uv lock` â†’ Updated llm-gemini v0.25 -> v0.28.1

### Verification

```
make check â†’ All checks passed âœ…
make test  â†’ 3367 passed, 7 skipped âœ…
```

### Model Summary (After Update)

| Usage | Model | Notes |
|-------|-------|-------|
| Auto-detect (Anthropic) | `anthropic/claude-sonnet-4-5` | Best for planning |
| Auto-detect (Gemini) | `gemini/gemini-3-flash-preview` | NEW - 78% SWE-bench |
| Auto-detect (OpenAI) | `gpt-5.2` | NEW - flagship model |
| LLM node default | `gemini-3-flash-preview` | NEW - reliable JSON |
| Planner nodes | `anthropic/claude-sonnet-4-0` | Unchanged - needs cache/thinking |

---

## Task 95 Complete

All phases implemented and verified:

1. âœ… **Phase 1**: Bug fix - Scoped monkey-patch to planner-only path
2. âœ… **Phase 2**: Discovery migration - Removed monkey-patch from discovery commands
3. âœ… **Phase 3+4**: Smart filter + Settings - Configurable model settings
4. âœ… **Model Updates**: Latest models (GPT-5.2, Gemini 3 Flash)
5. âœ… **Default Model Configuration**: Require explicit model configuration for LLM nodes

---

## [2025-12-19] - Default Model Configuration (Option A)

### Problem Solved

Previously, the LLM node had a hardcoded default (`gemini-3-flash-preview`) that would fail at runtime if the user didn't have a Gemini API key configured. This was confusing because:
- User might have OpenAI/Anthropic/OpenRouter keys but not Gemini
- Error happened at runtime, not compile time
- No guidance on how to fix

### Solution: Require Explicit Configuration

Now the compiler requires explicit model configuration. Resolution order:

```
IR params["model"] (explicit in workflow)
    â†“ not set
settings.llm.default_model (user's preference)
    â†“ not set
llm CLI default (llm models default)
    â†“ not set
CompilationError with helpful setup instructions
```

### Changes Made

1. **`src/pflow/core/settings.py`**:
   - Added `default_model` field to `LLMSettings`
   - Updated docstring with resolution order

2. **`src/pflow/core/llm_config.py`**:
   - Added `get_llm_cli_default_model()` - checks `llm models default`
   - Added `get_default_workflow_model()` - settings â†’ llm CLI â†’ None
   - Added `get_model_not_configured_help()` - helpful error message

3. **`src/pflow/runtime/compiler.py`**:
   - Added model injection in `_create_single_node()` for LLM nodes
   - Fails with `CompilationError` if no model configured

4. **`src/pflow/nodes/llm/llm.py`**:
   - Updated docstrings to clarify model resolution

5. **Tests**:
   - `tests/test_core/test_llm_config_workflow_model.py` (new)
   - `tests/test_runtime/test_compiler_llm_model.py` (new)
   - Updated `test_template_integration.py` to include model param

### Error Message (when nothing configured)

```
Error: No model configured for LLM node 'my-llm'

Configure a default model using one of these methods:

  1. Specify in workflow (per-node):
     {"id": "my-llm", "type": "llm", "params": {"model": "gpt-5.2", "prompt": "..."}}

  2. Set pflow default (recommended, applies to all workflows):
     Add to ~/.pflow/settings.json:
     {"llm": {"default_model": "gpt-5.2"}}

  3. Set llm library default (applies to all llm usage):
     llm models default gpt-5.2

To see available models: llm models list
To see configured keys: llm keys list
```

### Settings Structure

```json
{
  "llm": {
    "default_model": "gpt-5.2",
    "discovery_model": null,
    "filtering_model": null
  }
}
```

### Verification

```
make check â†’ All checks passed âœ…
make test  â†’ 3388 passed, 7 skipped âœ…
```

### Benefits

1. **Provider agnostic** - Works with ANY llm-supported provider
2. **Fail early** - Compile time error, not runtime
3. **Clear guidance** - Error message shows exactly how to fix
4. **Explicit over implicit** - No surprises about which model is used
5. **Node independence** - LLM node still works standalone

---

## [2025-12-19] - Dependency Version Updates

Updated LLM library dependencies to latest versions:

| Package | Before | After |
|---------|--------|-------|
| `llm` | >=0.27.1 | >=0.28 |
| `llm-anthropic` | ==0.20 | ==0.23 |
| `anthropic` | >=0.40.0 | >=0.75 |

**Note**: `anthropic>=0.75` kept as direct dependency because we import it directly in `anthropic_structured_client.py`. Version updated to match `llm-anthropic==0.23`'s requirement (no longer redundant).

**Verification**: `make test` (3388 passed), `make check` (all passed)

---

## [2025-12-19] - Final Verification Complete

Comprehensive verification of all Task 95 changes:

### Automated Tests
- `make test`: 3388 passed, 7 skipped âœ…
- `make check`: All checks pass âœ…
- New tests (45 total): All pass âœ…

### Manual Verification
- **Phase 1**: File workflows use specified model (Gemini worked, invalid models error properly)
- **Phase 2**: Discovery commands use `get_model_for_feature()`, no monkey-patch
- **Phase 3+4**: Smart filter uses configurable model, settings structure correct
- **Phase 5**: Resolution chain works (IR â†’ settings â†’ llm CLI â†’ error)
- **Phase 6**: New models in pricing, PRICING_VERSION updated
- **Edge cases**: Missing settings handled, LLM node works standalone

### Task 95 Status: âœ… COMPLETE

All implementation phases verified and working correctly.

---

## [2025-12-19] - CLI Commands for LLM Settings

### Problem

Users had to manually edit `~/.pflow/settings.json` to configure LLM models. No CLI interface existed for the new `LLMSettings` fields added in Task 95.

### Implementation

Added `pflow settings llm` subgroup with 5 commands:

```bash
pflow settings llm show              # Show settings with resolution status
pflow settings llm set-default MODEL # Set default_model
pflow settings llm set-discovery MODEL
pflow settings llm set-filtering MODEL
pflow settings llm unset SETTING     # SETTING: default|discovery|filtering|all
```

### Key Design Decision: Unified Default Model

During implementation, the user identified that `default_model` should logically be the default for *all* LLM usage, not just workflow nodes. The name implies shared behavior.

**Before:**
```
discovery:  discovery_model â†’ auto-detect â†’ fallback
filtering:  filtering_model â†’ auto-detect â†’ fallback
```

**After:**
```
discovery:  discovery_model â†’ default_model â†’ auto-detect â†’ fallback
filtering:  filtering_model â†’ default_model â†’ auto-detect â†’ fallback
```

This means setting `default_model` once configures everything, with feature-specific overrides still available.

### CLI Output Example

```
$ pflow settings llm show
LLM Model Settings:

  default_model:    gemini-3-flash-preview (configured)
  discovery_model:  (using default_model â†’ gemini-3-flash-preview)
  filtering_model:  (using default_model â†’ gemini-3-flash-preview)

Resolution order:
  default:    workflow params â†’ default_model â†’ llm CLI default â†’ error
  discovery:  discovery_model â†’ default_model â†’ auto-detect â†’ fallback
  filtering:  filtering_model â†’ default_model â†’ auto-detect â†’ fallback
```

### Files Modified

| File | Change |
|------|--------|
| `src/pflow/cli/commands/settings.py` | Added `llm` subgroup with 5 commands |
| `src/pflow/core/llm_config.py` | Added `default_model` fallback in `get_model_for_feature()` |
| `src/pflow/core/settings.py` | Updated `LLMSettings` docstring |
| `tests/test_cli/test_settings_cli.py` | Added 22 tests for LLM commands |
| `tests/test_core/test_llm_config_workflow_model.py` | Added 8 tests for resolution chain |

### Verification

- `make check`: All passed âœ…
- `make test`: 3418 passed, 7 skipped âœ…
