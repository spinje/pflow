# Subtask 8.4 Handoff: Binary Detection and Large File Streaming

**TO THE IMPLEMENTING AGENT**: Read this entire memo before starting. When done, acknowledge you're ready to begin - DO NOT start implementing immediately.

## üö® Critical Context You Need

### Current stdin Implementation is Text-Only
The entire stdin pipeline currently assumes UTF-8 text:
- `read_stdin()` in `src/pflow/core/shell_integration.py` uses `sys.stdin.read()` (line 26)
- It returns `None` for empty stdin or decoding errors
- The CLI stores stdin as a string in `shared["stdin"]`
- **Everything downstream expects text in shared["stdin"]**

### The Architecture You're Building On

1. **Shell Integration Module** (`src/pflow/core/shell_integration.py`):
   - `detect_stdin()` - checks if stdin is piped
   - `read_stdin()` - reads text only, returns None on any issue
   - `determine_stdin_mode()` - classifies as "workflow" or "data"
   - `populate_shared_store()` - simple dict mutation (not used by CLI)

2. **CLI Flow** (`src/pflow/cli/main.py`):
   - `get_input_source()` (lines 42-86) - returns `(workflow, source, stdin_data)`
   - stdin_data is passed through the entire call chain
   - Injected at line 124: `shared_storage["stdin"] = stdin_data`

## üí£ Landmines and Edge Cases

### 1. Empty stdin vs No stdin
I spent time on this - `read_stdin()` explicitly checks for empty string:
```python
if content == "":
    return None
```
This is CRITICAL because CliRunner makes stdin look piped even when empty. Don't break this!

### 2. The Dual-Mode Complexity
stdin can be either:
- **Workflow**: JSON with `ir_version` (old pattern)
- **Data**: Everything else (new pattern with --file)

Your binary detection must NOT interfere with workflow detection. A binary file that happens to start with `{"ir_version"` must still be treated as data when --file is used.

### 3. Backward Compatibility is Sacred
The pattern `echo '{"ir_version": "1.0"}' | pflow` MUST continue to work. Binary detection should only apply to data mode, never workflow mode.

## üéØ What You're Actually Building

You need to enhance stdin handling in THREE places:

1. **In `shell_integration.py`**:
   - Add `detect_binary_content(sample)` - check first N bytes for null bytes
   - Add `read_stdin_with_limit(max_size)` - handle large files
   - Modify `read_stdin()` to use these new functions

2. **In `cli/main.py`**:
   - Modify how stdin_data is stored in shared storage
   - Add `shared["stdin_binary"]` for binary data
   - Add `shared["stdin_path"]` for large files
   - Handle cleanup of temp files

3. **New Decisions Needed**:
   - How to detect binary? (null bytes? failed decode? both?)
   - When to use temp files? (10MB is suggested but is it right?)
   - How to clean up temp files? (atexit? context manager? manual?)

## üîó Key Code Locations

**Primary targets**:
- `src/pflow/core/shell_integration.py` - lines 18-29 (`read_stdin` function)
- `src/pflow/cli/main.py` - line 55 (where `read_stdin_content()` is called)
- `src/pflow/cli/main.py` - lines 122-126 (where stdin is injected)

**Test files to update**:
- `tests/test_shell_integration.py` - add binary tests
- `tests/test_cli/test_dual_mode_stdin.py` - test new keys in shared store

## ‚ö° Performance Considerations

1. **Don't read large files multiple times** - stdin is a stream!
2. **Binary detection should use a small sample** - maybe first 8KB?
3. **Temp file creation is expensive** - only do it for truly large files
4. **Memory usage** - current implementation loads entire stdin into memory

## üß™ Test Patterns That Work

From my experience:
```python
# Mock binary stdin
with patch('sys.stdin.buffer.read', return_value=b'\x00\x01\x02'):
    # Your test here

# Mock large stdin (use io.BytesIO for efficiency)
large_data = io.BytesIO(b'x' * 20_000_000)  # 20MB
with patch('sys.stdin.buffer', large_data):
    # Your test here
```

## üìö Essential Docs

1. **Edge case documentation I created**: `.taskmaster/tasks/task_8/subtask_8.2/implementation/edge-cases-future.md`
   - Has a whole section on "Binary Data Handling"
   - Lists the proposed approach you're implementing

2. **Shell pipes spec**: `docs/features/shell-pipes.md`
   - Section 7.2 discusses binary handling
   - Section 7.3 covers large file streaming

3. **The handoff I received**: The beginning of my session had critical context about validation traps and Click patterns

## ‚ö†Ô∏è Warnings

1. **Don't break text mode!** Most users will still pipe text. Binary should be an enhancement, not a replacement.

2. **Test subprocess behavior**: The subprocess integration tests in `test_dual_mode_stdin.py` use real `pflow` command. These are fragile but important.

3. **Windows compatibility**: `sys.stdin.buffer` behaves differently on Windows. The task mentions Unix pipes, but don't completely break Windows.

4. **Cleanup is critical**: If you create temp files and don't clean them up, users will hate us.

## üéÅ What You're Enabling

When you're done, these should work:
```bash
# Binary data
cat image.jpg | pflow --file process-image.json

# Large files
cat huge.log | pflow --file analyze-logs.json

# Mixed mode (large text file)
cat 100mb.txt | pflow --file transform.json
```

## üìù Final Notes

- The shell integration module I built is clean and minimal - try to maintain that
- Binary mode is explicitly listed as a v2.0 feature in the edge cases doc, but you're implementing it now
- Consider making binary detection optional with a flag like `--binary`
- The 10MB limit is arbitrary - make it configurable if possible

Good luck! This is extending the dual-mode stdin to handle real-world use cases beyond simple text.

**REMEMBER**: Acknowledge you've read this and are ready to begin. Do not start implementing until you've understood everything above.
