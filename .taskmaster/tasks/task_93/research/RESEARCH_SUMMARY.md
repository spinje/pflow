# Research Summary: Template Validation Error Messages for Agents

**Date**: 2025-12-12
**Research Question**: How does pflow provide "available fields" and "Did you mean?" suggestions when agents reference non-existent template variables?

## Executive Summary

pflow has a sophisticated 3-tier error suggestion system specifically designed to help AI agents understand what went wrong and how to fix it:

1. **Available Fields Display** - Shows ALL valid output paths from a node (up to 20)
2. **"Did you mean?" Suggestions** - Finds similar fields using substring matching
3. **Common Fix Examples** - Provides concrete template correction examples

This system was implemented in **Task 71** and is CRITICAL for agent self-correction during workflow repair.

## Core Components

### 1. Template Validator (`src/pflow/runtime/template_validator.py`)

**Purpose**: Pre-execution validation with rich error context for agents

**Key Functions**:
- `_flatten_output_structure()` (lines 247-346) - Recursively flattens nested output structures
- `_find_similar_paths()` (lines 349-383) - Substring matching for typo detection
- `_format_enhanced_node_error()` (lines 386-463) - Multi-section error formatting
- `_get_node_outputs_description()` (lines 466-525) - Builds complete error messages

**Display Limits** (lines 42-45):
```python
MAX_DISPLAYED_FIELDS = 20      # Balance info vs overwhelming
MAX_DISPLAYED_SUGGESTIONS = 3  # Cognitive limit for alternatives
MAX_FLATTEN_DEPTH = 5          # Prevent infinite recursion
```

### 2. Error Message Structure

When an agent references a non-existent field like `${node.wrong_field}`, they see:

```
Node 'fetch-messages' (type: mcp-slack-composio-SLACK_FETCH_CONVERSATION_HISTORY)
does not output 'msg'

Available outputs from 'fetch-messages':
  ‚úì ${fetch-messages.result} (dict)
  ‚úì ${fetch-messages.result.messages} (array)
  ‚úì ${fetch-messages.result.messages[0]} (dict)
  ‚úì ${fetch-messages.result.messages[0].text} (string)
  ‚úì ${fetch-messages.result.messages[0].user} (string)
  ‚úì ${fetch-messages.result.messages[0].ts} (string)
  ... and 14 more outputs

Did you mean one of these?
  - ${fetch-messages.result.messages}

Common fix: Change ${fetch-messages.msg} to ${fetch-messages.result.messages}
```

**Section Breakdown**:
1. **Problem Statement** - What node, what attempted field
2. **Available Outputs** - Complete structure with types (limited to 20)
3. **Suggestions** - Up to 3 similar matches via substring matching
4. **Common Fix** - Concrete example using best suggestion

### 3. How "Available Fields" Are Extracted

**Source**: Node interface metadata in the registry

**Flow**:
```
Registry scan (node registration)
  ‚Üí Metadata extraction from docstrings
  ‚Üí Enhanced Interface Format parsing
  ‚Üí Structure storage in registry.json
  ‚Üí Template validator loads structure
  ‚Üí Flattens to show all paths
```

**Example Structure** (from MCP node):
```python
{
  "key": "result",
  "type": "dict",
  "structure": {
    "messages": {
      "type": "array",
      "items": {
        "type": "dict",
        "structure": {
          "text": {"type": "string"},
          "user": {"type": "string"},
          "ts": {"type": "string"}
        }
      }
    }
  }
}
```

**Flattened Output**:
```
result ‚Üí dict
result.messages ‚Üí array
result.messages[0] ‚Üí dict
result.messages[0].text ‚Üí string
result.messages[0].user ‚Üí string
result.messages[0].ts ‚Üí string
```

### 4. How "Did You Mean?" Works

**Algorithm**: Simple substring matching (lines 349-383)

```python
def _find_similar_paths(attempted_key: str, available_paths: list[tuple[str, str]]) -> list[tuple[str, str]]:
    """Find paths similar to the attempted key."""
    attempted_lower = attempted_key.lower()
    matches = []

    for path, path_type in available_paths:
        # Extract last component: "result.messages" ‚Üí "messages"
        last_component = path.split(".")[-1].split("[")[0]

        # Substring match (case-insensitive)
        if attempted_lower in last_component.lower():
            # Calculate match quality (longer match = better)
            match_quality = len(attempted_lower) / len(last_component)
            matches.append((path, path_type, match_quality))

    # Sort by quality, return top 3
    matches.sort(key=lambda x: (-x[2], x[0]))
    return [(path, path_type) for path, path_type, _ in matches[:3]]
```

**Examples**:
- `"msg"` ‚Üí finds `"messages"` (substring match)
- `"usr"` ‚Üí finds `"user"` (substring match)
- `"txt"` ‚Üí finds `"text"` (substring match)

### 5. Type Validation with Suggestions

**New in Task 84** - Type mismatch errors also show available fields:

```python
Type mismatch in node 'fetch' parameter 'text':
template ${fetch.issue_data} has type 'dict'
but parameter expects 'str'

üí° Available fields with correct type:
   - ${fetch.issue_data.title}
   - ${fetch.issue_data.body}
   - ${fetch.issue_data.url}
   ... and 2 more
```

**Implementation** (lines 1076-1130):
- Traverses nested structure to find matching types
- Shows up to 5 fields with correct type
- Helps agents understand they need to access nested fields

## Test Coverage

### Primary Tests

**`test_template_validator_enhanced_errors.py`** (275 lines):
- Tests input description integration
- Tests path access errors
- Tests multiple missing inputs
- Verifies error message content

**`test_ir_schema_output_suggestions.py`** (85 lines):
- Tests wrong field name corrections
- Tests wrapping examples for type errors
- Ensures no false positives

**`test_node_wrapper_type_validation.py`**:
- Tests type mismatch error messages
- Verifies "Available fields with correct type" suggestions

## Security Features

**Sanitization** (lines 221-244):
- Removes control characters (newlines, carriage returns)
- Prevents terminal escape sequences
- Prevents log injection attacks
- Limits length to 100 chars with truncation

```python
def _sanitize_for_display(value: str, max_length: int = 100) -> str:
    """Sanitize for safe display in error messages."""
    # Remove non-printable and control chars
    sanitized = "".join(
        c for c in value
        if c.isprintable() and c not in ("\n", "\r", "\t", "\x0b", "\x0c")
    )

    # Truncate if too long
    if len(sanitized) > max_length:
        sanitized = sanitized[:max_length] + "..."

    return sanitized
```

## Integration Points

### Where Errors Are Generated

1. **Pre-execution validation** - `TemplateValidator.validate_workflow_templates()`
   - Called by compiler before execution
   - Returns list of error strings
   - Blocks execution if errors found

2. **Runtime validation** - `TemplateAwareNodeWrapper`
   - Validates during actual execution
   - Can be strict (fatal) or permissive (warnings)
   - Triggers repair if errors detected

3. **Repair service** - Uses validation errors to fix workflows
   - Parses error messages for context
   - Extracts suggestions for repair prompts
   - Re-validates after repair

### How Agents See Errors

**During workflow generation** (planner):
```json
{
  "validation_errors": [
    "Node 'fetch' does not output 'msg'\n\nAvailable outputs:\n  ‚úì ${fetch.result.messages}...",
    "Type mismatch in node 'process'..."
  ]
}
```

**During execution** (repair):
```json
{
  "error_type": "validation_error",
  "error_details": "Template validation failed with 2 errors",
  "validation_errors": ["...same format..."],
  "status": "failed",
  "repair_attempted": true
}
```

**In MCP tools** (structured output):
```json
{
  "success": false,
  "error": "Workflow validation failed",
  "validation_errors": ["..."],
  "suggestions": [
    "Change ${fetch.msg} to ${fetch.result.messages}"
  ]
}
```

## Suggestion Utilities (Shared)

**`src/pflow/core/suggestion_utils.py`** - Reusable suggestion logic:

```python
def find_similar_items(
    query: str,
    items: list[str],
    *,
    max_results: int = 5,
    method: Literal["substring", "fuzzy"] = "substring",
    cutoff: float = 0.4,
    sort_by_length: bool = False,
) -> list[str]:
    """Find items similar to query.

    Supports:
    - substring: Fast case-insensitive matching (default)
    - fuzzy: difflib.get_close_matches for typo tolerance
    """
```

**Used by**:
- CLI (`mcp.py`, `registry_cli.py`)
- Runtime (`compiler.py` - for MCP node suggestions)
- Formatters (`registry_run_formatter.py`)
- MCP server (`resolver.py`, `workflow_service.py`)

**Format function**:
```python
def format_did_you_mean(
    query: str,
    suggestions: list[str],
    *,
    item_type: str = "item",
    fallback_items: list[str] | None = None,
    max_fallback: int = 10,
) -> str:
    """Format as user-friendly message."""
```

## Key Design Decisions

### Why Substring Matching Instead of Fuzzy?

**Decision**: Use simple substring matching for template validation

**Reasoning**:
1. **Performance** - No external dependencies (difflib is stdlib but slower)
2. **Predictability** - Clear rules, easier to debug
3. **Common patterns** - Most errors are partial matches (`"msg"` ‚Üí `"messages"`)
4. **False positives** - Fuzzy matching can suggest unrelated fields

**Trade-off**: May miss some typo scenarios (`"mesages"` ‚Üí `"messages"`)

### Why Show Structure Instead of Just Keys?

**Decision**: Display full nested paths with types, not just keys

**Example**:
```
‚úÖ Shows: ${node.result.messages[0].text} (string)
‚ùå Not: text (string)
```

**Reasoning**:
1. **Context** - Agents need to know WHERE the field lives
2. **Namespacing** - With node IDs, full path is required
3. **Arrays** - Need to show `[0]` syntax for array access
4. **Types** - Type info helps agents choose correct field

### Why Limit to 20 Fields?

**Decision**: Cap displayed fields at 20 with "... and N more" message

**Reasoning**:
1. **Terminal space** - 20 fields fits ~25 lines with formatting
2. **Cognitive load** - More than 20 is overwhelming
3. **Relevance** - If top 20 don't help, more won't either
4. **Performance** - Generating/displaying large lists is expensive

**Escape hatch**: Full structure in registry.json if needed

### Why 3 Suggestions Maximum?

**Decision**: Show top 3 "Did you mean?" suggestions

**Reasoning**:
1. **Cognitive science** - Humans can compare ~3 options effectively
2. **Screen space** - 3 fits without scrolling
3. **Quality** - If top 3 don't help, rest won't either
4. **Agent behavior** - LLMs work better with focused options

## Performance Characteristics

**Template Validation**:
- O(n) where n = number of templates in workflow
- Structure flattening: O(d √ó f) where d=depth, f=fields
- Substring matching: O(p √ó k) where p=paths, k=key length

**Typical numbers**:
- 10-node workflow: ~30 templates
- MCP node output: ~20 fields (flattened from 5-10 nested)
- Validation time: <50ms for typical workflow

**Optimization**: Results cached during compilation, not re-computed

## Common Patterns Agents Encounter

### Pattern 1: Missing Nested Access

**Agent mistake**:
```json
{"prompt": "${fetch.result}"}  // result is dict, needs nested field
```

**Error shows**:
```
Type mismatch: template ${fetch.result} has type 'dict' but parameter expects 'str'

üí° Available fields with correct type:
   - ${fetch.result.messages[0].text}
   - ${fetch.result.messages[0].user}
```

**Agent fixes**:
```json
{"prompt": "${fetch.result.messages[0].text}"}
```

### Pattern 2: Typo in Field Name

**Agent mistake**:
```json
{"text": "${node.msg}"}  // Should be "message" or "messages"
```

**Error shows**:
```
Node 'node' does not output 'msg'

Did you mean: ${node.messages}?
Common fix: Change ${node.msg} to ${node.messages}
```

**Agent fixes**:
```json
{"text": "${node.messages}"}
```

### Pattern 3: Wrong Output Key

**Agent mistake**:
```json
{"data": "${node.output}"}  // Node outputs "result", not "output"
```

**Error shows**:
```
Available outputs from 'node':
  ‚úì ${node.result} (dict)
  ‚úì ${node.result.data} (string)
```

**Agent fixes**:
```json
{"data": "${node.result.data}"}
```

## Future Enhancements (Not in MVP)

### Fuzzy Matching Option
```python
# Could add fuzzy option to suggestion_utils
find_similar_items("mesages", items, method="fuzzy")  # ‚Üí ["messages"]
```

### Field Usage Frequency
```python
# Track which fields are commonly accessed
popular_fields = ["result.messages[0].text", ...]  # Prioritize in suggestions
```

### Cross-Node Analysis
```python
# Suggest fields from other nodes of same type
"Other slack nodes typically use: ${node.result.messages}"
```

### Visual Structure Diagram
```
result (dict)
‚îú‚îÄ‚îÄ messages (array)
‚îÇ   ‚îî‚îÄ‚îÄ [0] (dict)
‚îÇ       ‚îú‚îÄ‚îÄ text (string)
‚îÇ       ‚îî‚îÄ‚îÄ user (string)
‚îî‚îÄ‚îÄ ok (boolean)
```

## Related Task Implementation

**Task 71: Implement Node Interface Registry (Node IR)**
- Moved interface parsing to scan-time
- Eliminated false validation failures
- Created Enhanced Interface Format
- Implemented structure flattening
- Added "Did you mean?" suggestions
- Built multi-section error messages

**Files Changed**:
- `src/pflow/runtime/template_validator.py` - Enhanced error messages
- `src/pflow/registry/metadata_extractor.py` - Structure extraction
- `tests/test_runtime/test_template_validator_enhanced_errors.py` - New tests

## Conclusion

pflow's template validation error system is specifically designed for **AI agent self-correction**:

1. **Complete Context** - Shows all available fields, not just "field not found"
2. **Actionable Suggestions** - Provides concrete examples of correct syntax
3. **Type Awareness** - Helps agents understand data structure requirements
4. **Security** - Sanitizes all user-controlled values in error messages

This system enables agents to:
- Understand what went wrong (which field, which node)
- See what options are available (all outputs with types)
- Get specific fix suggestions (did you mean X?)
- Learn correct template syntax (common fix example)

The investment in rich error messages **directly translates to faster workflow repair** and fewer iteration cycles for agents.
