{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Create package setup and CLI entry point",
        "description": "Set up the pflow package structure with proper entry point configuration",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "Create pyproject.toml with proper package configuration and CLI entry point for 'pflow' command. Ensure package installs correctly with 'pip install -e .' and the pflow command becomes available. Set up initial src/pflow/ directory structure. Configure package metadata (name, version, dependencies). This task ensures the CLI framework in Task 2 has a proper foundation. Reference: standard Python packaging practices.",
        "testStrategy": "Verify package installs correctly. Test that 'pflow' command is available after installation. Validate package structure follows Python standards.",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure CLI entry point in pyproject.toml",
            "description": "Add [project.scripts] section to pyproject.toml defining 'pflow = \"pflow.cli:main\"' as the console script entry point. This enables the 'pflow' command to be available system-wide after installation.",
            "dependencies": [],
            "details": "Edit pyproject.toml to add the [project.scripts] section with the pflow entry point. This follows standard Python packaging practices for creating command-line tools. The entry point syntax 'pflow.cli:main' tells setuptools to look for a 'main' function in the pflow.cli module when the 'pflow' command is invoked.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create CLI module structure with basic click application",
            "description": "Create the src/pflow/cli/ directory structure with __init__.py and main.py files. Implement a basic click application in main.py with a simple test command to verify the framework is working.",
            "dependencies": [
              1
            ],
            "details": "Create src/pflow/cli/__init__.py (can be empty initially) and src/pflow/cli/main.py containing a basic click application. The main.py should define a main() function decorated with @click.command() or @click.group() and include at least one simple command (e.g., 'version' or 'hello') to test the CLI framework. This provides the actual CLI code that the pyproject.toml entry point references.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Verify package installation and CLI availability",
            "description": "Install the package in development mode using 'pip install -e .' and verify that the 'pflow' command becomes available in the terminal. Add a basic test to ensure the CLI framework is properly initialized.",
            "dependencies": [
              2
            ],
            "details": "Run 'pip install -e .' (or 'uv pip install -e .') in the project root to install the package in editable/development mode. Verify the 'pflow' command is available by running it in the terminal and checking it executes without errors. Create a basic test in tests/test_cli.py using click.testing.CliRunner to ensure the CLI framework initializes correctly and the test command works as expected. This confirms the entire setup chain from pyproject.toml entry point to CLI execution is functioning properly.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Set up basic CLI for argument collection",
        "description": "Create the minimal CLI entry point with basic flag parsing",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "Create src/pflow/cli.py using click. The primary goal is to accept and collect all command-line arguments (e.g., 'node1 --param=val >> node2') as a raw list or string. This raw input will be passed to the planner (in Phase 3) or a simple dispatcher. **This task does NOT involve parsing the '>>' operator or interpreting node syntax.** It only provides the entry point and collects the user's raw input.",
        "testStrategy": "Create tests/test_cli_core.py. Use click.testing.CliRunner to test basic command invocation (e.g., pflow --version). Verify the --help output is complete. Test that a simple dummy command receives its arguments correctly. Test error handling for both unknown arguments and invalid option formats to ensure the CLI is robust.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create CLI main entry point with click framework",
            "description": "Set up src/pflow/cli.py with @click.group() decorator for the main 'pflow' command group and add a placeholder 'run' subcommand following the command structure specified in docs/reference/cli-reference.md#basic-syntax",
            "status": "pending",
            "dependencies": [],
            "details": "Create the main CLI file at src/pflow/cli.py, implement the @click.group() decorator for 'pflow' command, add a basic 'run' subcommand with @click.command(), ensure the command structure follows the pattern 'pflow [command] [options]' as specified in the CLI reference",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Collect all command arguments, including '>>', into a raw list",
            "description": "Parse the custom '>>' flow operator to split node sequences and collect all --key=value flags into a list without categorization, following the operator semantics in docs/reference/cli-reference.md#the--operator",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Implement logic to capture all arguments passed to the `pflow` command, including the `>>` operator and any node parameters, into a raw list or string for later processing by the planner. This step does not interpret the arguments' meaning.",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create directory structure placeholders",
            "description": "Create empty directory structure with src/pflow/core/ and src/pflow/nodes/ as placeholders for future runtime engine and node implementations",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Create src/pflow/core/ directory with __init__.py file for future runtime engine components, create src/pflow/nodes/ directory with __init__.py file for future node implementations, add brief docstring comments indicating the purpose of each directory",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Execute a Hardcoded 'Hello World' Workflow",
        "description": "Create and execute a simple, hardcoded 'read-file >> write-file' workflow from a local JSON file to validate the core end-to-end execution pipeline.",
        "status": "pending",
        "dependencies": [
          1,
          2,
          4,
          5,
          6,
          11
        ],
        "priority": "high",
        "details": "This task integrates the core components built in Phase 1. It involves: 1. Creating a sample 'hello_workflow.json' file that conforms to the IR schema (Task 7). 2. Creating minimal 'read-file' and 'write-file' nodes (subset of Task 11). 3. Adding a basic 'pflow run <file.json>' command to the CLI (Task 2). 4. Calling the IR-to-Flow Converter (Task 4) to take the JSON and produce a pocketflow.Flow object. 5. Initialize a clean shared store dictionary and run the flow with it.",
        "testStrategy": "Create a tests/test_e2e_workflow.py. Define a simple workflow IR file for 'read-file >> write-file'. Use CliRunner to invoke 'pflow run --file <ir_file>'. Create mock nodes that write to the shared store. Verify the flow executes correctly and the final shared store contains expected values. Test with temporary files to ensure file I/O works as expected.",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize shared store dictionary before workflow execution",
            "description": "Create initialization logic to set up a clean shared store dictionary at the start of each workflow execution, ensuring proper state isolation between runs",
            "status": "pending",
            "dependencies": [],
            "details": "Implement shared store initialization in the execution engine that: 1) Creates a new empty dictionary for each workflow run, 2) Optionally pre-populates with stdin content if piped input is detected, 3) Ensures no state leakage between consecutive workflow executions, 4) Follows the shared store pattern defined in pocketflow/__init__.py and docs/core-concepts/shared-store.md",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Validate shared store after execution",
            "description": "Implement post-execution validation to verify shared store contains expected keys and perform proper cleanup to prevent memory leaks",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Create validation logic that: 1) Checks for required output keys based on the executed workflow, 2) Logs warnings for missing expected keys or unused data, 3) Clears the shared store after validation to free memory, 4) Optionally exports final shared store state for debugging when verbose mode is enabled, 5) Ensures compatibility with the execution reference patterns in docs/reference/execution-reference.md",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement IR-to-PocketFlow Object Converter",
        "description": "Build the compiler that converts JSON IR to pocketflow.Flow objects with template resolution",
        "status": "pending",
        "dependencies": [
          6,
          5
        ],
        "priority": "high",
        "details": "Create src/pflow/runtime/compiler.py with a function 'compile_ir_to_flow(ir_json)'. As per the integration guide, this function is NOT a complex compiler. It should: 1. Parse the input IR JSON. 2. Look up node classes from the registry (Task 5) based on their 'type'. 3. Instantiate the pocketflow.Node objects. 4. Connect the nodes using the '>>' operator based on the 'edges' array in the IR. 5. Return a fully formed, runnable pocketflow.Flow object. It should NOT implement any execution logic itself.",
        "testStrategy": "Test IR parsing and validation, node instantiation from registry, flow construction with >> operator, edge case handling for invalid IR. Verify the converter produces valid pocketflow.Flow objects.",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement node discovery via filesystem scanning",
        "description": "Scan Python files to find pocketflow.Node subclasses and extract metadata",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "Create src/pflow/registry/scanner.py with scan_for_nodes(directories) function. Use ast module or importlib to find all classes inheriting from pocketflow.Node (BaseNode). Extract basic metadata: node name, module path, docstring. Store results in simple dict or JSON file for fast access. No complex indexing needed - just a list of available nodes. Focus on src/pflow/nodes/ directory. This is not a package registry - just local filesystem scanning. Reference docs: registry.md",
        "testStrategy": "Test node discovery with mock node files, metadata extraction accuracy, and registry performance. Write tests for various node structures."
      },
      {
        "id": 6,
        "title": "Define JSON IR schema",
        "description": "Create the JSON schema for workflow intermediate representation",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "Create src/pflow/core/ir_schema.py with Pydantic models or JSON Schema definitions. Define minimal IR structure: nodes[] with id, type, params; edges[] with from, to, action (default 'default'); start_node id; optional mappings{} for NodeAwareSharedStore proxy. Keep it simple - just enough to represent a workflow graph. Use standard JSON Schema for validation. Don't overengineer - we can extend later. Example: {'nodes': [{'id': 'n1', 'type': 'read-file', 'params': {'file_path': 'input.txt'}}], 'edges': [], 'start_node': 'n1'}. Reference docs: schemas.md, planner.md#10.1",
        "testStrategy": "Test schema validation with valid and invalid IR examples, verify all required fields, test edge cases. Write schema validation tests."
      },
      {
        "id": 7,
        "title": "Extract node metadata from docstrings",
        "description": "Parse node docstrings to understand inputs, outputs, and parameters",
        "status": "pending",
        "dependencies": [
          5
        ],
        "priority": "medium",
        "details": "Create src/pflow/registry/metadata_extractor.py with extract_metadata(node_class) function. Parse docstring to find: 1) Node description, 2) Interface section with inputs/outputs, 3) Which parameters go to shared store vs node.set_params(). Use simple regex or docstring_parser library. Output format: {'inputs': ['file_path'], 'outputs': ['content'], 'params': ['encoding']}. This metadata helps the planner understand how to connect nodes. Keep extraction logic simple - don't over-parse. Reference docs: metadata-extraction.md, planner.md#5.1",
        "testStrategy": "Test extraction from various docstring formats, parameter classification accuracy. Write tests with different docstring styles."
      },
      {
        "id": 8,
        "title": "Build comprehensive shell pipe integration",
        "description": "Implement full Unix pipe support for stdin/stdout handling and shell integration",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "high",
        "details": "Create src/pflow/core/shell_integration.py with comprehensive Unix pipe support. Implement stdin detection and reading when data is piped to pflow. Support streaming for large data (not just reading entire stdin at once). Handle stdout output for chaining pflow with other Unix tools. Implement proper exit code propagation and signal handling (Ctrl+C). When stdin is detected, populate shared['stdin'] automatically. Support both batch mode (fail fast) and interactive mode (prompt for missing data). Look at Simon Willison's 'llm' CLI source code for excellent pipe integration patterns. This enables both: 'cat data.txt | pflow process' AND 'pflow node1 >> node2'. Reference docs: shell-pipes.md, architecture.md#5.1.1",
        "testStrategy": "Create tests/test_shell_integration.py. Use pytest fixtures to mock sys.stdin. Test stdin detection (distinguishing between piped data and interactive terminal input). Test reading various content sizes from stdin (empty, small, large). Verify that node output is correctly written to stdout. Include integration tests with mocked Unix tools like echo, cat, and grep to ensure compatibility. Test exit code propagation on both success and error cases.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create shell_integration.py with stdin detection and content reading",
            "description": "Implement the core shell integration module in src/pflow/core/shell_integration.py with functions to detect if stdin is piped using sys.stdin.isatty() and read piped content into shared['stdin']",
            "status": "pending",
            "dependencies": [],
            "details": "Create the shell_integration module following docs/features/shell-pipes.md#internal-implementation. Key functions: detect_stdin() to check if stdin is piped, read_stdin() to read content and place in shared['stdin']. Handle text encoding (UTF-8) and empty stdin cases. Ensure the module can be imported by other pflow components.",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Add streaming support for large inputs with buffered reading",
            "description": "Implement buffered reading using 8KB chunks to handle large piped inputs without loading entire content into memory at once",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Extend shell_integration.py with stream_stdin() function that reads in 8KB chunks as specified in docs/features/shell-pipes.md#streaming-support. Support both full read mode (for small inputs) and streaming mode (for large inputs). Add configuration option to set chunk size. Ensure chunks are properly concatenated when needed.",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement exit code handling for shell scripting compatibility",
            "description": "Add proper exit code propagation where 0 indicates success and non-zero indicates errors, enabling pflow to work correctly in shell scripts and pipelines",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Following docs/features/shell-pipes.md#exit-code-propagation, implement exit code handling in shell_integration.py. Create exit_with_code() function that properly exits with given code. Define standard exit codes (0=success, 1=general error, 2=usage error, etc). Ensure all error paths in stdin handling use appropriate exit codes.",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add SIGINT signal handling for graceful interruption",
            "description": "Implement signal handling using Python's signal module to allow users to interrupt pflow execution with Ctrl+C gracefully",
            "status": "pending",
            "dependencies": [
              1,
              3
            ],
            "details": "As per docs/features/shell-pipes.md#signal-handling, add signal handler for SIGINT in shell_integration.py. Create setup_signal_handlers() function that registers handler. Handler should clean up resources, print interruption message to stderr, and exit with code 130 (standard for SIGINT). Ensure handler works during stdin reading.",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement shared store collision detection and proxy mapping",
        "description": "Create validation for collision detection and NodeAwareSharedStore proxy for transparent key mapping",
        "status": "pending",
        "dependencies": [
          3,
          7
        ],
        "priority": "high",
        "details": "Merge validation and proxy implementation into src/pflow/core/proxy.py. Implement: 1) get_reserved_keys() returning ['stdin'], 2) detect_collisions(node_interfaces) to find key conflicts between nodes in a flow, 3) NodeAwareSharedStore class for transparent mapping around collisions and reserved keys. The proxy uses collision detection internally to know what needs mapping. Zero overhead when no collisions exist. Support input_mappings and output_mappings. Fail fast on missing required inputs. Include basic key existence validation: when a node requires an input key, verify it exists in the shared store before execution. This prevents silent failures and provides clear error messages. This minimal validation is essential for the proxy pattern to function correctly in MVP. Note: Natural pattern validation is NOT needed - nodes define their own conventions. Only validate for reserved keys and collisions. Reference docs: shared-store.md#2, cli-runtime.md#3",
        "testStrategy": "Test collision detection with various node combinations, proxy mapping scenarios, reserved key handling, and performance overhead. Verify fail-fast behavior and zero-overhead direct access. Test key existence validation to ensure missing keys produce clear error messages before node execution.",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Create registry CLI commands",
        "description": "Implement CLI commands for registry operations: list, describe, and search nodes",
        "status": "pending",
        "dependencies": [
          2,
          7
        ],
        "priority": "medium",
        "details": "Create src/pflow/cli/registry.py. Implement 'pflow registry list' showing individual platform nodes, 'pflow registry describe <node>' for detailed info with rich formatting for node-specific parameters. Part of enhanced registry infrastructure for fast lookups by node ID and capabilities. Reference docs: registry.md",
        "testStrategy": "Test CLI output formatting, search functionality, and error handling. Write integration tests for registry commands."
      },
      {
        "id": 11,
        "title": "Implement read-file and write-file nodes",
        "description": "Create basic file I/O nodes for reading and writing files",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "details": "Create file nodes in src/pflow/nodes/file/, all inheriting from pocketflow.BaseNode: read_file.py, write_file.py, copy_file.py, move_file.py, and delete_file.py. Simple interfaces: read-file uses shared['file_path'] \u2192 shared['content'], write-file uses shared['content'] + shared['file_path'], copy/move use shared['source_path'] + shared['dest_path'], delete uses shared['file_path']. Include safety checks for all destructive operations. Fail fast on missing required inputs. Natural interface pattern for file operations. Reference docs: simple-nodes.md",
        "testStrategy": "Test file operations, permission handling, and safety validations. Test fail-fast behavior. Write comprehensive unit tests."
      },
      {
        "id": 12,
        "title": "Implement general LLM node",
        "description": "Create the general-purpose LLM node for all text processing tasks",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "high",
        "details": "Create src/pflow/nodes/llm.py inheriting from pocketflow.BaseNode. Simple interface for general text processing. Natural interface: shared['prompt'] \u2192 shared['response']. Support multiple providers (Claude API, OpenAI). Fail fast on missing prompt with clear error message asking user for input. Smart exception to prevent prompt node proliferation. Reference docs: core-node-packages/llm-nodes.md, architecture.md#3.4",
        "testStrategy": "Mock LLM API responses, test different providers and parameter configurations. Test retry logic and error handling."
      },
      {
        "id": 13,
        "title": "Implement github-get-issue node",
        "description": "Create the first simple GitHub node for retrieving issue details",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "high",
        "details": "Create src/pflow/nodes/github/github_get_issue.py inheriting from pocketflow.BaseNode. Implement simple single-purpose node that reads issue_number and repo from shared store or params (check shared first, then params). Use PyGithub or requests for API calls. Natural interface: shared['repo'], shared['issue_number'] \u2192 shared['issue_data'], shared['issue_title']. Authentication via environment variables (GITHUB_TOKEN). Error handling for API failures and rate limits with clear error messages. Fail fast on missing required inputs. Part of Phase 3 Simple Platform Nodes. Reference docs: simple-nodes.md",
        "testStrategy": "Mock GitHub API responses, test parameter handling and error cases. Test fail-fast behavior on missing inputs. Write comprehensive unit tests."
      },
      {
        "id": 14,
        "title": "Implement git-commit node",
        "description": "Create simple git node for committing changes",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "medium",
        "details": "Create src/pflow/nodes/git/git_commit.py inheriting from pocketflow.BaseNode. Simple node reading shared['message'] and optional shared['files']. Execute git commands with safety checks and confirmation prompts in interactive mode. Natural interface: shared['message'] \u2192 shared['commit_hash']. Fail fast on missing commit message. Each node focused on single git operation. Reference docs: simple-nodes.md",
        "testStrategy": "Mock git commands, test safety measures and error handling. Test both interactive and batch modes."
      },
      {
        "id": 15,
        "title": "Implement LLM API client",
        "description": "Create simple client for calling LLM APIs during planning",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "Create src/pflow/planning/llm_client.py with simple functions: call_llm(prompt, model='claude-3-sonnet') that makes API calls to Claude or OpenAI. IMPLEMENTATION NOTE: Consider using Simon Willison's 'llm' package for simplified implementation and model flexibility. If using custom implementation: Use httpx or requests for API calls. Basic retry with exponential backoff (max 3 retries). Read API keys from environment variables. Return just the text response. Keep it simple - this is not a framework, just utility functions for the planner. Reference docs: planner.md#6.2",
        "testStrategy": "Mock LLM API calls, test retry logic and error handling. Test with different model providers."
      },
      {
        "id": 16,
        "title": "Create planning context builder",
        "description": "Format node metadata for LLM-based workflow planning",
        "status": "pending",
        "dependencies": [
          7
        ],
        "priority": "high",
        "details": "Create src/pflow/planning/context_builder.py with build_context(registry_metadata) function. Format node information into a structured text that LLMs can understand: node names, descriptions, inputs/outputs, parameter types. Keep format simple and readable - just markdown tables or structured text. Include which parameters go to shared store vs node.set_params(). This provides the LLM with available 'tools' for building workflows. Output example: 'Available nodes:\n- read-file: Reads file from disk\n  Inputs: file_path (from shared store)\n  Outputs: content (to shared store)'. Reference docs: planner.md#6.1",
        "testStrategy": "Test context generation, metadata optimization, and LLM readability. Verify output format is parseable."
      },
      {
        "id": 17,
        "title": "Implement LLM-based Workflow Generation Engine",
        "description": "Create the core engine that transforms natural language into template-driven workflows",
        "status": "pending",
        "dependencies": [
          15,
          16,
          18,
          19
        ],
        "priority": "high",
        "details": "Create src/pflow/planning/workflow_compiler.py with compile_request(user_input, node_context) function. In the MVP, this engine receives the entire raw input string from the CLI (e.g., \"pflow read-file --path=... >> llm\"). It uses the LLM to interpret this string as a domain-specific language, recognizing node names and conventions through pattern matching, not direct parsing. The LLM: 1) Interprets the natural language input, 2) Recognizes node names and parameter conventions, 3) Generates workflows with template variables like $issue_data, and 4) Fills in missing parameters. The planner uses the template resolver utility (Task 20) to validate and prepare templates. Reference docs: planner.md#6.1, workflow-analysis.md",
        "testStrategy": "Test workflow generation accuracy for natural language inputs. Verify LLM correctly interprets CLI-like syntax. Test template variable generation and validation. Verify IR structure is valid."
      },
      {
        "id": 18,
        "title": "Create prompt templates for planning",
        "description": "Design effective prompts for LLM-based workflow generation",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "Create src/pflow/planning/prompts.py with prompt templates as simple string constants or functions. Include: 1) Workflow generation prompt with node context and examples, 2) Error recovery prompt for failed attempts, 3) Template variable extraction prompt. Emphasize template-driven prompt generation with $variable syntax. Use f-strings or jinja2 for template composition. Keep prompts focused and include concrete examples. Test prompts manually to ensure they generate valid JSON IR. Example structure: WORKFLOW_PROMPT = '''Given these nodes: {node_context}\nGenerate a workflow for: {user_request}\nOutput JSON IR format with template variables like $issue_data...'''. Reference docs: planner.md#6.1",
        "testStrategy": "Test prompt generation for various scenarios, template composition, and token optimization. Verify IR generation success rate."
      },
      {
        "id": 19,
        "title": "Create Planner's Template Resolver Utility",
        "description": "Implement a simple, regex-based string substitution function for planner-internal use.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "Create a utility function 'resolve_template(template_str, available_vars)' in 'src/pflow/planning/utils.py'. As per the integration guide, this is a simple function for the planner (Task 18) to validate that template variables in its generated prompts can be resolved at runtime. This is NOT a runtime templating engine.",
        "testStrategy": "Test template resolution with various syntax forms ($var, ${var}), missing variable detection raises proper errors, escaping works correctly ($$var), resolution with empty/None values. Verify integration with planner workflow generation."
      },
      {
        "id": 20,
        "title": "Implement Planner Output Approval and Storage",
        "description": "Implement user verification and workflow persistence",
        "status": "pending",
        "dependencies": [
          17
        ],
        "priority": "medium",
        "details": "Create src/pflow/planning/approval.py and src/pflow/core/workflow_storage.py. Show generated CLI workflow for approval with clear presentation of individual node syntax. Allow parameter modifications before execution. Save approved workflows with meaningful names to ~/.pflow/workflows/ in JSON format. Implement storage system with name-based retrieval, listing capabilities, and pattern matching for intelligent reuse of similar workflows. Support loading workflows by name for execution. Parameter extraction for similar requests to enable 'Plan Once, Run Forever' optimization. Target \u226590% user approval rate. Reference docs: planner.md#11",
        "testStrategy": "Test approval flow, workflow saving/loading, and modification handling. Test pattern matching for workflow reuse."
      },
      {
        "id": 21,
        "title": "Implement workflow lockfile system",
        "description": "Create lockfile generation for deterministic workflow execution",
        "status": "deferred",
        "dependencies": [
          20
        ],
        "priority": "medium",
        "details": "Create src/pflow/core/lockfile.py to generate and manage workflow lockfiles. After workflow validation, generate lockfile containing: workflow IR hash, node versions from registry, execution timestamp, pflow version. Store lockfiles alongside workflows in ~/.pflow/workflows/<name>.lock. Lockfiles ensure deterministic execution across environments and time. Include functions: generate_lockfile(workflow, registry), validate_lockfile(lockfile, registry), check_compatibility(lockfile). This is critical for 'Plan Once, Run Forever' - workflows must execute identically months later. Reference docs: PRD#5.6, architecture.md",
        "testStrategy": "Test lockfile generation, validation, version compatibility checks. Test deterministic execution with lockfiles."
      },
      {
        "id": 22,
        "title": "Implement named workflow execution",
        "description": "Enable execution of saved workflows by name with parameters",
        "status": "pending",
        "dependencies": [
          20,
          21
        ],
        "priority": "high",
        "details": "Extend CLI to support executing saved workflows by name: 'pflow fix-issue --issue=1234'. Create src/pflow/cli/execute.py with execute_named_workflow(name, params) function. Load workflow from ~/.pflow/workflows/<name>.json, validate against lockfile, apply runtime parameters to template variables, execute via IR compiler. This is the core user-facing command that delivers the 'Plan Once, Run Forever' value. Support parameter override and validation. Include helpful error messages for missing workflows or parameters. Reference docs: mvp-scope.md, cli-reference.md",
        "testStrategy": "Test workflow loading, parameter application, execution flow. Test error cases and parameter validation."
      },
      {
        "id": 23,
        "title": "Implement execution tracing system",
        "description": "Build comprehensive execution visibility for debugging and optimization",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "high",
        "details": "Create src/pflow/runtime/tracing.py with execution tracing that helps users understand, debug, and optimize execution flow. Capture and display: inputs/outputs for each node, shared state diffs per step, LLM tokens used per node and total, execution time per node, cache hits/misses. Format output clearly with proper error context: '[1] read-file (0.02s) Input: {file_path: 'data.txt'} Output: {content: '...'} Shared Store \u0394: +content'. Support different verbosity levels via --trace flag. Include cost estimation for LLM nodes ($0.0012 per call). Persist execution traces to ~/.pflow/traces/<run-id>.json for post-execution analysis. Implement 'pflow trace <run-id>' command to retrieve and display saved traces. Critical for understanding workflow behavior and optimizing performance. Reference docs: architecture.md (observability), runtime.md",
        "testStrategy": "Test trace output formatting, performance overhead, and debugging effectiveness. Verify error context is clear."
      },
      {
        "id": 24,
        "title": "Build caching system",
        "description": "Implement node-level caching for flow_safe nodes",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "medium",
        "details": "Create src/pflow/runtime/cache.py as optional performance optimization. Simple disk-based cache using pickle or json. Cache key = hash(node_type + params + inputs). Store in ~/.pflow/cache/. Only cache nodes marked as @flow_safe (deterministic). Start with just LLM nodes to save API costs. Can be disabled via --no-cache flag. This is not critical for MVP - implement only if performance becomes an issue. Reference docs: runtime.md#caching-strategy",
        "testStrategy": "Test cache hits/misses, key computation, and storage operations. Test cache invalidation."
      },
      {
        "id": 25,
        "title": "Implement claude-code super node",
        "description": "Create the comprehensive Claude Code node for AI-assisted development with project context",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "high",
        "details": "Create src/pflow/nodes/claude_code.py inheriting from pocketflow.BaseNode. Single powerful node with instruction-based interface for AI-assisted development with full project context and file system access. Complex prompt generation from planner templates with structured instructions. Integration with headless Claude Code CLI for workflow automation. Natural interface: shared['prompt'] \u2192 shared['code_report'] (comprehensive development report). Model selection and parameter handling. Fail fast on missing required inputs (prompt) with clear error message. Part of two-tier AI approach. Reference docs: core-node-packages/claude-nodes.md",
        "testStrategy": "Mock Claude Code CLI responses, test template processing and output parsing. Test error handling and timeout scenarios."
      },
      {
        "id": 26,
        "title": "Create additional GitHub nodes",
        "description": "Implement remaining GitHub platform nodes: create-pr, list-prs, add-comment, merge-pr",
        "status": "pending",
        "dependencies": [
          13
        ],
        "priority": "low",
        "details": "Create GitHub nodes in src/pflow/nodes/github/, all inheriting from pocketflow.BaseNode: github_create_pr.py, github_list_prs.py, github_add_comment.py, and github_merge_pr.py. Each with focused single purpose and natural interfaces: shared['pr_title'], shared['pr_body'], shared['repo']. github_merge_pr includes safety checks and conflict handling with shared['pr_number'], shared['merge_method']. Reference docs: simple-nodes.md",
        "testStrategy": "Mock API responses for each operation, test error handling. Write unit tests for each node."
      },
      {
        "id": 27,
        "title": "Implement additional git nodes",
        "description": "Create remaining git platform nodes: push, create-branch, merge, and status",
        "status": "pending",
        "dependencies": [
          14
        ],
        "priority": "low",
        "details": "Create git nodes in src/pflow/nodes/git/, all inheriting from pocketflow.BaseNode: git_push.py, git_create_branch.py, git_merge.py, and git_status.py. Each node follows simple single-purpose pattern: git-push reads shared['branch'] and executes push, git-create-branch reads shared['branch_name'] and creates branch, git-merge reads shared['source_branch'] and shared['target_branch'], git-status writes shared['git_status']. Include safety checks and confirmation prompts for destructive operations. Reference docs: simple-nodes.md",
        "testStrategy": "Mock git commands for each operation, test safety measures and edge cases. Write unit tests."
      },
      {
        "id": 28,
        "title": "Build CI and shell nodes",
        "description": "Create comprehensive set of CI and shell execution nodes",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "low",
        "details": "Create CI and shell nodes, all inheriting from pocketflow.BaseNode. CI nodes in src/pflow/nodes/ci/: ci_run_tests.py, ci_get_status.py, ci_trigger_build.py, ci_get_logs.py. Shell nodes in src/pflow/nodes/shell/: shell_exec.py, shell_pipe.py, shell_background.py. CI nodes use natural interface: shared['test_command'] \u2192 shared['test_results'], support multiple CI systems (GitHub Actions, Jenkins, local). Shell nodes use shared['command'] \u2192 shared['output'] with shell_background also writing shared['pid']. Auto-detect test frameworks, handle exit codes, timeout handling, and security considerations. Reference docs: simple-nodes.md",
        "testStrategy": "Mock test framework responses, test command execution and timeout handling. Write security-focused tests."
      },
      {
        "id": 29,
        "title": "Create comprehensive test suite",
        "description": "Build integration tests for all components not covered by unit tests",
        "status": "pending",
        "dependencies": [
          2,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          17,
          20,
          21,
          22,
          4,
          5,
          6,
          7,
          23,
          24
        ],
        "priority": "high",
        "details": "Create tests/ structure for integration and performance tests (unit tests already created with each task). Integration tests for end-to-end workflows. Performance benchmark suite in tests/benchmarks/ measuring planning latency (\u2264800ms target), execution speed (\u22642s overhead target), token usage optimization. Error recovery test suite in tests/error_recovery/ for invalid input handling, external service failures, partial execution scenarios. Mock external services. Test natural language \u2192 workflow generation with \u226595% success rate target.",
        "testStrategy": "Achieve >90% code coverage including integration scenarios. Test all critical paths and error scenarios."
      },
      {
        "id": 30,
        "title": "Polish CLI experience and documentation",
        "description": "Enhance user experience with better error messages, help text, and initial documentation",
        "status": "pending",
        "dependencies": [
          29
        ],
        "priority": "low",
        "details": "Improve error messages with actionable suggestions and clear next steps. Add comprehensive --help for all commands. Create initial README with quickstart guide showing primary workflow example (GitHub issue resolution). Update relevant documentation as features are implemented. Support developer workflow scenarios including slash command comparison (10x efficiency gain target). Reference docs: architecture.md#11.3",
        "testStrategy": "User acceptance testing, documentation review, error message clarity testing."
      },
      {
        "id": 31,
        "title": "Create MVP validation test suite",
        "description": "End-to-end validation that MVP meets all acceptance criteria",
        "status": "pending",
        "dependencies": [
          29
        ],
        "priority": "high",
        "details": "Create tests/mvp_validation/ with end-to-end scenarios proving real value. Test core workflow: 'pflow \"fix github issue 123\"' generates appropriate workflow, executes successfully, creates PR with fix. Measure against MVP criteria: 10x faster than manual LLM interaction, natural language to workflow works, template resolution functions correctly, all platform nodes integrate properly. Include performance benchmarks showing planning \u2264800ms, execution overhead \u22642s. Document which v2.0 features are intentionally excluded. This is about proving the MVP delivers its core value proposition. Reference docs: mvp-scope.md, architecture.md#11",
        "testStrategy": "Test real-world scenarios, measure performance targets, validate user experience metrics."
      },
      {
        "id": 32,
        "title": "[DEFER TO v2.0] Execution configuration handling",
        "description": "Support node-level retry configuration in runtime",
        "status": "deferred",
        "dependencies": [
          6,
          4
        ],
        "priority": "low",
        "details": "[DEFERRED TO v2.0 - Advanced error handling] Extend runtime to handle execution configuration from IR. Support node-level settings: max_retries (default 3), retry_wait (exponential backoff), timeout. MVP uses simple hardcoded retry logic. This advanced configuration is for v2.0 when robust error handling is needed. Reference docs: schemas.md#execution-config, runtime.md",
        "testStrategy": "Test retry logic, timeout handling, configuration parsing. Mock failing operations to test retries."
      },
      {
        "id": 44,
        "title": "[DEFER TO v2.0] Trace persistence and retrieval",
        "description": "Save and retrieve execution traces for debugging",
        "status": "deferred",
        "dependencies": [
          23
        ],
        "priority": "low",
        "details": "[DEFERRED TO v2.0 - Nice to have] Extend tracing system to persist traces. Save detailed execution traces to ~/.pflow/traces/<run-id>.json. Implement 'pflow trace <run-id>' CLI command. MVP only needs real-time trace display during execution. Persistence and retrieval is a v2.0 enhancement for production debugging. Reference docs: runtime.md#tracing",
        "testStrategy": "Test trace saving, retrieval, listing. Test large trace handling and cleanup."
      },
      {
        "id": 45,
        "title": "[DEFER TO v2.0] Node version tracking",
        "description": "Track node versions for lockfile generation",
        "status": "deferred",
        "dependencies": [
          5,
          7
        ],
        "priority": "low",
        "details": "[DEFERRED TO v2.0 - Over-engineering for MVP] Extend registry to track node versions. Add version extraction to metadata. MVP can use simpler approach with hardcoded versions or git commit hash. Full version tracking system is v2.0 when marketplace and version compatibility become important. Reference docs: registry.md#versioning, schemas.md",
        "testStrategy": "Test version extraction, registry storage, version comparison logic."
      },
      {
        "id": 46,
        "title": "[DEFER TO v2.0] Implement interface compatibility system",
        "description": "Create the compatibility analysis for shared store key matching and type validation between nodes",
        "status": "deferred",
        "dependencies": [
          9,
          7
        ],
        "priority": "low",
        "details": "[DEFERRED TO v2.0 - Not needed for MVP] Advanced compatibility checking is for marketplace scenarios. MVP nodes have compatible interfaces by design. Basic validation is sufficient for MVP. When implemented in v2.0: Create compatibility analysis for heterogeneous node sources. Reference docs show this as post-MVP feature for complex marketplace integration.",
        "testStrategy": "Test interface matching scenarios, type validation edge cases, and proxy mapping generation"
      },
      {
        "id": 47,
        "title": "[DEFER TO v2.0] Build success metrics instrumentation",
        "description": "Implement comprehensive metrics tracking for planning success, execution reliability, and performance",
        "status": "deferred",
        "dependencies": [
          17,
          4
        ],
        "priority": "low",
        "details": "[DEFERRED TO v2.0 - Premature optimization] Focus on building working MVP first before instrumenting performance. Basic logging is sufficient for MVP. When implemented: Add lightweight metrics collection for key performance indicators. Can use simple timing decorators and counters initially. Reference docs: architecture.md#11 shows metrics as success criteria, not MVP feature.",
        "testStrategy": "Test metrics collection accuracy, performance overhead, and aggregation logic"
      },
      {
        "id": 48,
        "title": "[DEFER TO v2.0] Implement direct CLI parsing without LLM",
        "description": "Parse CLI pipe syntax directly for minor performance optimization",
        "status": "deferred",
        "dependencies": [
          17,
          49
        ],
        "priority": "low",
        "details": "[DEFERRED TO v2.0 - Minor optimization only] Create direct parser for CLI pipe syntax that bypasses LLM for fully-specified commands. Parse 'node1 --param=value >> node2' syntax directly into IR. Even with direct parsing, LLM still needed for: missing parameters, template variables, data flow connections, shared store mappings. This is a minor optimization since users rarely specify everything. Benefits: slightly faster for complete commands, more predictable for simple flows, reduced token usage. The high-value feature is autocomplete (task #39), not direct parsing. Reference docs: planner.md#3.2 (future state)",
        "testStrategy": "Test parsing accuracy, measure actual performance improvement, ensure LLM fallback works"
      },
      {
        "id": 49,
        "title": "[DEFER TO v2.0] Implement CLI autocomplete for node discovery",
        "description": "Add shell autocomplete for node names and parameters to enhance CLI usability",
        "status": "deferred",
        "dependencies": [
          2,
          5,
          10
        ],
        "priority": "high",
        "details": "Create src/pflow/cli/autocomplete.py with shell completion support. Implement autocomplete for: node names (read-file, llm, github-get-issue), common parameters (--path, --prompt, --issue), pipe operator (>>). Use click's shell completion features. Generate completion scripts for bash/zsh/fish. When user types 'pflow read-f[TAB]', complete to 'read-file'. When typing 'pflow read-file --[TAB]', show available parameters from node metadata. This provides immediate value by helping users discover available nodes without documentation. Works even though CLI syntax is processed by LLM in MVP. Makes the tool feel responsive and professional. Reference docs: autocomplete.md (concepts), cli-runtime.md",
        "testStrategy": "Test completion generation, shell integration, and user experience across shells. Write integration tests."
      },
      {
        "id": 50,
        "title": "[DEFER TO v2.0] Support nested proxy mappings",
        "description": "Implement complex key mapping patterns for advanced node compatibility",
        "status": "deferred",
        "dependencies": [
          9
        ],
        "priority": "low",
        "details": "[DEFERRED TO v2.0 - Not needed for MVP] Support for nested mappings like 'data.content' -> 'file_data.text'. MVP uses simple key-to-key mappings only. This feature enables more complex node compatibility patterns but adds significant complexity. When implemented: extend NodeAwareSharedStore to handle dot-notation paths and nested object transformations.",
        "testStrategy": "Test nested mapping scenarios, performance impact, and edge cases with deep nesting."
      },
      {
        "id": 51,
        "title": "[DEFER TO v2.0] Build context-aware CLI parameter resolution",
        "description": "Implement the CLI resolution system that routes flags to shared store or node parameters based on context",
        "status": "deferred",
        "dependencies": [
          2,
          8
        ],
        "priority": "medium",
        "details": "In v2.0, this will add smart CLI flag validation and categorization for better UX. The system will use node metadata to categorize flags as data (shared store) vs behavior (node params) vs execution config. This enables better error messages like '--temperature is not valid for github-get-issue node' and powers the autocomplete feature (task 39). In MVP, all input is treated as natural language by the planner, so this optimization is not needed. Create src/pflow/core/cli_resolver.py with functions that validate and categorize CLI flags based on node metadata. Reference docs: architecture.md#5.1.1, cli-runtime.md#7",
        "testStrategy": "Test flag categorization with various node types and parameter combinations. Verify error message generation for invalid flags. Test integration with autocomplete system. Write unit tests for validation logic.",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-18T20:41:12.050Z",
      "updated": "2025-06-27T18:56:33.614Z",
      "description": "Reorganized task list based on the phased refinement plan in scratchpads/task-refinement-plan.md. Tasks are now ordered logically, dependencies are corrected, and descriptions are clarified to align with project architecture."
    }
  }
}
