# Before/After Examples: Workflow Export

This document shows concrete examples of workflow IR (input) and the corresponding generated Python code (output).

## Example 1: Simple Shell Command

### BEFORE (Workflow IR)

```json
{
  "ir_version": "0.1.0",
  "nodes": [
    {
      "id": "list-files",
      "type": "shell",
      "purpose": "List all Python files",
      "params": {
        "command": "ls -la *.py"
      }
    }
  ]
}
```

### AFTER (Generated Python Code)

```python
#!/usr/bin/env python3
"""
Generated by pflow v0.1.0
Source: simple-shell.json
"""

import subprocess
from typing import Any

# ============================================================================
# Shared Store
# ============================================================================

shared: dict[str, Any] = {}

# ============================================================================
# Node Functions
# ============================================================================

def list_files():
    """Node: list-files - List all Python files"""
    try:
        # Execute shell command
        result = subprocess.run(
            ["ls", "-la", "*.py"],
            capture_output=True,
            text=True,
            shell=False
        )

        # Store results
        if result.returncode == 0:
            shared["list-files"] = {
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": 0
            }
            return "default"
        else:
            shared["list-files"] = {
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode,
                "error": f"Command failed with exit code {result.returncode}"
            }
            return "error"

    except Exception as e:
        shared["list-files"] = {
            "error": str(e),
            "returncode": -1
        }
        return "error"

# ============================================================================
# Main Execution
# ============================================================================

def main():
    """Execute workflow"""
    # Execute node: list-files
    action = list_files()

    # Print output
    print(shared["list-files"]["stdout"])

    return shared

if __name__ == "__main__":
    result = main()
```

---

## Example 2: File Pipeline with Template Variables

### BEFORE (Workflow IR)

```json
{
  "ir_version": "0.1.0",
  "inputs": {
    "input_file": {
      "type": "string",
      "required": true,
      "description": "File to read"
    },
    "output_file": {
      "type": "string",
      "default": "output.txt",
      "description": "File to write"
    }
  },
  "nodes": [
    {
      "id": "reader",
      "type": "read-file",
      "purpose": "Read input file",
      "params": {
        "file_path": "${input_file}"
      }
    },
    {
      "id": "writer",
      "type": "write-file",
      "purpose": "Write to output file",
      "params": {
        "file_path": "${output_file}",
        "content": "${reader.content}"
      }
    }
  ],
  "edges": [
    {"from": "reader", "to": "writer"}
  ],
  "outputs": {
    "final_path": {
      "description": "Path where file was written",
      "source": "${writer.file_path}"
    }
  }
}
```

### AFTER (Generated Python Code)

```python
#!/usr/bin/env python3
"""
Generated by pflow v0.1.0
Source: file-pipeline.json

USAGE:
    python file-pipeline.py --input_file input.txt
    python file-pipeline.py --input_file input.txt --output_file result.txt
"""

import sys
import argparse
from pathlib import Path
from typing import Any

# ============================================================================
# Shared Store
# ============================================================================

shared: dict[str, Any] = {}

# ============================================================================
# Node Functions
# ============================================================================

def reader():
    """Node: reader - Read input file"""
    try:
        # Get parameters
        file_path = shared.get("input_file")
        if not file_path:
            raise ValueError("Parameter 'file_path' is required but not provided")

        # Expand path
        file_path = Path(file_path).expanduser().resolve()

        # Read file
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Store results
        shared["reader"] = {
            "content": content,
            "file_path": str(file_path),
            "encoding": "utf-8"
        }
        return "default"

    except Exception as e:
        shared["reader"] = {
            "error": str(e)
        }
        return "error"

def writer():
    """Node: writer - Write to output file"""
    try:
        # Get parameters
        file_path = shared.get("output_file", "output.txt")
        content = shared["reader"]["content"]

        # Expand path
        file_path = Path(file_path).expanduser().resolve()

        # Ensure parent directory exists
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # Write file atomically (using temp file)
        temp_path = file_path.with_suffix(file_path.suffix + ".tmp")
        with open(temp_path, 'w', encoding='utf-8') as f:
            f.write(content)

        # Atomic rename
        temp_path.replace(file_path)

        # Store results
        shared["writer"] = {
            "file_path": str(file_path),
            "bytes_written": len(content.encode('utf-8'))
        }
        return "default"

    except Exception as e:
        shared["writer"] = {
            "error": str(e)
        }
        return "error"

# ============================================================================
# Main Execution
# ============================================================================

def main(input_file: str, output_file: str = "output.txt"):
    """Execute workflow

    Args:
        input_file: File to read
        output_file: File to write (default: output.txt)

    Returns:
        dict: Workflow outputs
    """
    # Set initial parameters
    shared["input_file"] = input_file
    shared["output_file"] = output_file

    # Execute node: reader
    action_reader = reader()
    if action_reader == "error":
        print(f"Error in reader: {shared['reader'].get('error')}", file=sys.stderr)
        sys.exit(1)

    # Execute node: writer
    action_writer = writer()
    if action_writer == "error":
        print(f"Error in writer: {shared['writer'].get('error')}", file=sys.stderr)
        sys.exit(1)

    # Return declared outputs
    return {
        "final_path": shared["writer"]["file_path"]
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="file-pipeline workflow")
    parser.add_argument("--input_file", required=True, help="File to read")
    parser.add_argument("--output_file", default="output.txt", help="File to write")

    args = parser.parse_args()
    result = main(input_file=args.input_file, output_file=args.output_file)

    print(f"✓ Workflow completed successfully")
    print(f"  Output written to: {result['final_path']}")
```

---

## Example 3: Error Handling with Action Routing

### BEFORE (Workflow IR)

```json
{
  "ir_version": "0.1.0",
  "nodes": [
    {
      "id": "fetch-data",
      "type": "shell",
      "purpose": "Fetch data from API",
      "params": {
        "command": "curl -f https://api.example.com/data"
      }
    },
    {
      "id": "process-data",
      "type": "shell",
      "purpose": "Process the fetched data",
      "params": {
        "command": "jq '.items | length'",
        "stdin": "${fetch-data.stdout}"
      }
    },
    {
      "id": "handle-error",
      "type": "write-file",
      "purpose": "Log error to file",
      "params": {
        "file_path": "error.log",
        "content": "Error: ${fetch-data.stderr}",
        "append": true
      }
    }
  ],
  "edges": [
    {"from": "fetch-data", "to": "process-data", "action": "default"},
    {"from": "fetch-data", "to": "handle-error", "action": "error"}
  ]
}
```

### AFTER (Generated Python Code)

```python
#!/usr/bin/env python3
"""
Generated by pflow v0.1.0
Source: error-handling.json
"""

import subprocess
from pathlib import Path
from typing import Any

# ============================================================================
# Shared Store
# ============================================================================

shared: dict[str, Any] = {}

# ============================================================================
# Node Functions
# ============================================================================

def fetch_data():
    """Node: fetch-data - Fetch data from API"""
    try:
        result = subprocess.run(
            ["curl", "-f", "https://api.example.com/data"],
            capture_output=True,
            text=True
        )

        if result.returncode == 0:
            shared["fetch-data"] = {
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": 0
            }
            return "default"
        else:
            shared["fetch-data"] = {
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode,
                "error": f"Command failed with exit code {result.returncode}"
            }
            return "error"

    except Exception as e:
        shared["fetch-data"] = {
            "error": str(e),
            "returncode": -1,
            "stderr": str(e),
            "stdout": ""
        }
        return "error"

def process_data():
    """Node: process-data - Process the fetched data"""
    try:
        # Get stdin from previous node
        stdin_data = shared["fetch-data"]["stdout"]

        result = subprocess.run(
            ["jq", ".items | length"],
            input=stdin_data,
            capture_output=True,
            text=True
        )

        if result.returncode == 0:
            shared["process-data"] = {
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": 0
            }
            return "default"
        else:
            shared["process-data"] = {
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode,
                "error": f"Command failed with exit code {result.returncode}"
            }
            return "error"

    except Exception as e:
        shared["process-data"] = {
            "error": str(e),
            "returncode": -1
        }
        return "error"

def handle_error():
    """Node: handle-error - Log error to file"""
    try:
        # Get parameters
        file_path = Path("error.log").expanduser().resolve()
        content = f"Error: {shared['fetch-data']['stderr']}"

        # Ensure parent directory exists
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # Append to file
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(content + '\n')

        shared["handle-error"] = {
            "file_path": str(file_path),
            "bytes_written": len(content.encode('utf-8'))
        }
        return "default"

    except Exception as e:
        shared["handle-error"] = {
            "error": str(e)
        }
        return "error"

# ============================================================================
# Main Execution
# ============================================================================

def main():
    """Execute workflow with error handling"""

    # Execute node: fetch-data
    action_fetch = fetch_data()

    # Conditional routing based on action
    if action_fetch == "error":
        # Error path: handle-error
        action_error = handle_error()
        if action_error == "error":
            print(f"Error handling failed: {shared['handle-error'].get('error')}")
            return shared
        print(f"Error logged to: {shared['handle-error']['file_path']}")
        return shared
    else:
        # Success path: process-data
        action_process = process_data()
        if action_process == "error":
            print(f"Processing failed: {shared['process-data'].get('error')}")
            return shared
        print(f"Items count: {shared['process-data']['stdout'].strip()}")
        return shared

if __name__ == "__main__":
    result = main()
```

---

## Example 4: Workflow with LLM Node (Phase 2)

### BEFORE (Workflow IR)

```json
{
  "ir_version": "0.1.0",
  "inputs": {
    "prompt": {
      "type": "string",
      "required": true,
      "description": "Prompt for the LLM"
    }
  },
  "nodes": [
    {
      "id": "generate",
      "type": "llm",
      "purpose": "Generate response using LLM",
      "params": {
        "prompt": "${prompt}",
        "model": "claude-3-5-sonnet-20241022",
        "max_tokens": 1000
      }
    },
    {
      "id": "save",
      "type": "write-file",
      "purpose": "Save response to file",
      "params": {
        "file_path": "response.txt",
        "content": "${generate.response}"
      }
    }
  ],
  "edges": [
    {"from": "generate", "to": "save"}
  ]
}
```

### AFTER (Generated Python Code)

```python
#!/usr/bin/env python3
"""
Generated by pflow v0.1.0
Source: llm-workflow.json

REQUIRED CREDENTIALS:
  - ANTHROPIC_API_KEY: Anthropic API authentication

Setup (choose one):
  # Option 1: Environment variable (recommended)
  export ANTHROPIC_API_KEY="sk-ant-..."

  # Option 2: pflow settings
  pflow settings set ANTHROPIC_API_KEY "sk-ant-..."

Dependencies:
  pip install llm

USAGE:
    python llm-workflow.py --prompt "Write a haiku about Python"
"""

import sys
import os
import json
import argparse
from pathlib import Path
from typing import Any

# ============================================================================
# Credential Management
# ============================================================================

def get_credential(key: str) -> str:
    """Load credential from environment or pflow settings."""
    # Check environment variable
    value = os.environ.get(key)
    if value:
        return value

    # Check pflow settings
    settings_path = Path.home() / ".pflow" / "settings.json"
    if settings_path.exists():
        with open(settings_path) as f:
            settings = json.load(f)
            value = settings.get("env", {}).get(key)
            if value:
                return value

    raise ValueError(
        f"Credential '{key}' not found.\n"
        f"Set it using: export {key}=<value>\n"
        f"Or: pflow settings set {key} <value>"
    )

# ============================================================================
# Shared Store
# ============================================================================

shared: dict[str, Any] = {}

# ============================================================================
# Node Functions
# ============================================================================

def generate():
    """Node: generate - Generate response using LLM"""
    try:
        import llm

        # Get parameters
        prompt = shared.get("prompt")
        model_name = "claude-3-5-sonnet-20241022"
        max_tokens = 1000

        # Get model
        model = llm.get_model(model_name)

        # Generate response
        response = model.prompt(prompt, max_tokens=max_tokens)
        response_text = response.text()

        # Store results
        shared["generate"] = {
            "response": response_text,
            "model": model_name,
            "prompt": prompt
        }
        return "default"

    except Exception as e:
        shared["generate"] = {
            "error": str(e)
        }
        return "error"

def save():
    """Node: save - Save response to file"""
    try:
        file_path = Path("response.txt").expanduser().resolve()
        content = shared["generate"]["response"]

        file_path.parent.mkdir(parents=True, exist_ok=True)

        temp_path = file_path.with_suffix(file_path.suffix + ".tmp")
        with open(temp_path, 'w', encoding='utf-8') as f:
            f.write(content)

        temp_path.replace(file_path)

        shared["save"] = {
            "file_path": str(file_path),
            "bytes_written": len(content.encode('utf-8'))
        }
        return "default"

    except Exception as e:
        shared["save"] = {
            "error": str(e)
        }
        return "error"

# ============================================================================
# Main Execution
# ============================================================================

def main(prompt: str):
    """Execute workflow

    Args:
        prompt: Prompt for the LLM

    Returns:
        dict: Shared store with results
    """
    # Validate credentials
    try:
        api_key = get_credential("ANTHROPIC_API_KEY")
    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

    # Set initial parameters
    shared["prompt"] = prompt

    # Execute node: generate
    action_generate = generate()
    if action_generate == "error":
        print(f"Error in generate: {shared['generate'].get('error')}", file=sys.stderr)
        sys.exit(1)

    # Execute node: save
    action_save = save()
    if action_save == "error":
        print(f"Error in save: {shared['save'].get('error')}", file=sys.stderr)
        sys.exit(1)

    print(f"✓ Workflow completed successfully")
    print(f"  Response saved to: {shared['save']['file_path']}")

    return shared

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="llm-workflow")
    parser.add_argument("--prompt", required=True, help="Prompt for the LLM")

    args = parser.parse_args()
    result = main(prompt=args.prompt)
```

---

## Summary of Generated Code Characteristics

### Common Features Across All Examples:

1. **Shebang and docstring** with source info and usage
2. **Credential management** (when needed)
3. **Type hints** (`dict[str, Any]`)
4. **Shared store** dictionary for inter-node communication
5. **Node functions** with try/except and return actions
6. **Main function** with flow control
7. **CLI argument parsing** for inputs
8. **Error messages** to stderr
9. **Success messages** to stdout

### Code Quality:

- ✅ Readable with clear section separators
- ✅ Documented with docstrings
- ✅ Follows Python conventions (PEP 8)
- ✅ Type-safe with type hints
- ✅ Error handling throughout
- ✅ Executable without modifications
- ✅ No pflow dependencies

### File Size Estimates:

- Simple shell: ~80 lines
- File pipeline: ~150 lines
- Error handling: ~180 lines
- LLM workflow: ~200 lines

All well under the 500-line target for typical workflows.
