---
title: "Nodes overview"
description: "Building blocks for pflow workflows"
icon: "boxes"
---

<Note>
  **For reference, not memorization.** Your AI agent knows which nodes to use and how to configure them. This reference is for understanding what's possible.
</Note>

Nodes are the building blocks of pflow workflows. Each node performs a single operation - reading a file, calling an API, running a shell command - and passes data to the next node through the shared store.

## Core nodes

pflow includes these built-in nodes:

<Columns cols={2}>
  <Card title="File operations" icon="file" href="/reference/nodes/file">
    Read, write, copy, move, and delete files
  </Card>
  <Card title="LLM" icon="sparkles" href="/reference/nodes/llm">
    Call AI models with prompts and images
  </Card>
  <Card title="HTTP" icon="globe" href="/reference/nodes/http">
    Make API requests to web services
  </Card>
  <Card title="Shell" icon="terminal" href="/reference/nodes/shell">
    Execute shell commands
  </Card>
  <Card title="Code" icon="code" href="/reference/nodes/code">
    Execute Python for data transformation
  </Card>
  <Card title="Claude Code" icon="square-asterisk" href="/reference/nodes/claude-code">
    AI-assisted development tasks
  </Card>
  <Card title="MCP tools" icon="plug" href="/reference/nodes/mcp">
    Use tools from MCP servers
  </Card>
</Columns>

## How nodes work

Every node follows the same pattern:

1. **Read inputs** from node parameters
2. **Execute** its operation
3. **Write outputs** to the shared store
4. **Return an action** that determines the next node

Because every node declares its inputs and outputs, pflow can validate the whole workflow before anything runs — catching bad template references, missing fields, and type mismatches at build time.

### Parameters vs shared store

Nodes receive data through parameters. Parameters can be static values or template variables that pull from the shared store:

| Type | When to use | Example |
|------|-------------|---------|
| **Static parameters** | Fixed values set when building the workflow | `"model": "gpt-4"` |
| **Template variables** | Dynamic values from previous nodes or workflow inputs | `"prompt": "${summarize.response}"` |

Template variables like `${node_id.key}` are resolved at runtime from the shared store and injected into node parameters. You can access nested fields and array elements directly: `${api.response.items[0].name}`.

Nodes write their outputs to the shared store, making them available for template variables in subsequent nodes. See [Template variables](/reference/template-variables) for complete syntax and examples.

### Automatic JSON parsing

When a node outputs a JSON string and the next node expects an object, pflow automatically parses it. This means shell commands that output JSON work directly with other nodes — no extra conversion steps needed.

### Output keys

Each node writes specific keys to the shared store. For example:

- `read-file` writes `content`, `file_path`, `content_is_binary`
- `llm` writes `response`, `llm_usage`
- `http` writes `response`, `status_code`, `response_headers`
- `code` writes `result`, `stdout`, `stderr`

Check each node's documentation for its complete interface.

## Discovering nodes

Your agent uses these commands to find the right nodes:

```bash
# List all available nodes
pflow registry list

# Search by capability
pflow registry discover "read a JSON file"

# Get full interface details
pflow registry describe read-file
```

## Extending with MCP

Beyond core nodes, you can add capabilities from MCP servers. When you run `pflow mcp sync`, each MCP tool becomes a pflow node:

```bash
# Add a GitHub MCP server
pflow mcp add github.mcp.json

# Sync to discover tools
pflow mcp sync github

# New nodes appear
pflow mcp tools github
# mcp-github-create_issue
# mcp-github-list_repos
# ...
```

See [MCP tools](/reference/nodes/mcp) for details on how this works.

## Node categories

| Category | Nodes | Purpose |
|----------|-------|---------|
| **File** | read-file, write-file, copy-file, move-file, delete-file | Local filesystem operations |
| **LLM** | llm | AI model calls via any provider |
| **HTTP** | http | Web API requests |
| **Shell** | shell | System command execution |
| **Code** | code | Python data transformation |
| **Claude** | claude-code | AI-assisted development |
| **MCP** | mcp-{server}-{tool} | External tool integration |

## Disabling nodes

You can disable any node (including core nodes) using the settings filter:

```bash
# Disable shell commands entirely
pflow settings deny shell

# Disable specific file operations
pflow settings deny delete-file

# Re-enable a node
pflow settings remove shell
```

Disabled nodes won't appear in `pflow registry list` and can't be used in workflows. See [settings commands](/reference/cli/settings) for details.
