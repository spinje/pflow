# v0.7.0 Release Context

Generated: 2026-02-04
This file contains implementation context for AI agents and release verification.

---

## Changelog

## v0.7.0 (2026-02-04)

- Removed `--description` and `--generate-metadata` flags from `workflow save` command [#80](https://github.com/spinje/pflow/pull/80)
- Removed legacy `${stdin}` shared store pattern in favor of explicit input routing [#73](https://github.com/spinje/pflow/pull/73)
- Replaced JSON workflow format with a new Markdown-based format (`.pflow.md`) that treats workflows as executable documentation [#80](https://github.com/spinje/pflow/pull/80) ([Task 107](.taskmaster/tasks/task_107/task-review.md))
- Added Python code node (`"type": "code"`) for in-process data transformation with native object inputs and AST-based type validation [#75](https://github.com/spinje/pflow/pull/75) ([Task 104](.taskmaster/tasks/task_104/task-review.md))
- Added automatic stdin routing via `"stdin": true` input property to support Unix-style workflow chaining [#73](https://github.com/spinje/pflow/pull/73) ([Task 115](.taskmaster/tasks/task_115/task-review.md))
- Added `disallowed_tools` parameter to `claude-code` node to block specific tools via allowlist patterns [#78](https://github.com/spinje/pflow/pull/78)
- Fixed pre-execution validation logic to ensure `--validate-only` catches unknown node types without tracebacks [#67](https://github.com/spinje/pflow/pull/67)
- Fixed template validation error when using nested dot-notation variables inside array brackets
- Improved validation to detect and reject JSON strings containing embedded template variables [#69](https://github.com/spinje/pflow/pull/69)

---

## Skipped Changes (Verification)

Review these to ensure nothing was incorrectly classified as internal:

- Upgrade pydantic 2.11→2.12, mcp 1.17→1.26, pytest 8→9

Enables Python 3.14 CI support (pydantic-core now ships 3.14 wheels).
- Add Python 3.14 to CI test matrix and classifiers
- more context for task 107
- docs: add comments for non-obvious test design decisions
- fix: restore minimal registry and serial marker in stdin hang test
- fix: clean up flaky stdin tests and remove stale mocks
- refactor: move pocketflow to src/pflow/pocketflow for PyPI release
- research for task 104
- docs: update CLAUDE.md files for Task 115 stdin routing changes
- fix: improve Claude Code review to focus on latest commits

- Add track_progress: false to prevent comment history bias
- Add event-aware review scope instructions so synchronize events
  focus on new commits rather than re-reviewing the full PR

Addresses known issues with Claude repeating feedback on already-fixed
code when new commits are pushed to a PR.
- add task for Automatic Stdin Routing for Unix-First Piping
- docs: add overview.md and consolidate architecture documentation
- research for task 87
- docs: architecture documentation cleanup and consolidation
- docs: add task browser script and improve CLAUDE.md documentation
- docs: sandbox architecture decisions and new task specifications
- docs: task 104 research files
- minor edits for braindump claude code command
- cleanup
- cleanup
- docs: add Task 112 for pre-execution type validation of literal parameters
- minor change to simplle example workflow
- task: add Task 111 batch limit for workflow iteration

Add task definition and specification for --batch-limit CLI parameter
that limits batch node iterations during development. Defaults to 3
for local files, enabling fast iteration without processing full datasets.
- open questions for task 49 and 107

---

## Documentation Changes

### docs/guides/using-pflow.mdx

*Commits: 4cffcb5, f34f5ed*

- Changed workflow file format from JSON to executable Markdown (`.pflow.md`) for better readability
- Updated CLI usage examples to include the `-p` flag for piping data:
  ```bash
  cat data.csv | pflow -p --output-format json process-csv > output.json
  ```
- Added a section on chaining workflows using Unix pipes:
  ```bash
  pflow -p step1 | pflow -p step2 | pflow step3
  ```
- Updated terminology to reflect that workflows are no longer authored as JSON files

### docs/how-it-works/batch-processing.mdx

*Commits: 4cffcb5*

- Converted all JSON-based workflow configuration examples into a more readable Markdown format using headers and property lists
- Updated task examples (HTTP, LLM, shell, and file processing) to follow a standardized "Steps" structure:
  ```markdown
  ### list_issues
  Fetch issues from the GitHub API.
  - type: http
  - url: https://api.github.com/repos/owner/repo/issues
  ```
- Refactored documentation for parallel processing, error handling (fail_fast vs continue), and retries to use the new Markdown-based syntax
- Redesigned shell command examples to use explicit code blocks for command strings:
  ```shell command
  tr '\n' ',' | jq -Rc 'split(",") | map(select(length > 0))'
  ```

### docs/how-it-works/template-variables.mdx

*Commits: 4cffcb5, f34f5ed*

- Migrated all workflow configuration examples from JSON format to Markdown (`.pflow.md`)
- Updated shell node examples to show YAML-based `stdin` configuration:
  ```yaml stdin
  config: ${settings}
  data: ${results}
  ```
- Added documentation for the `stdin: true` property on inputs to enable piped data routing
- Updated batch processing examples to use the new Markdown-based node syntax

### docs/index.mdx

*Commits: 4cffcb5*

- Added "Workflows are documentation" section explaining how `.pflow.md` files function as both executable pipelines and human-readable documentation

### docs/reference/cli/index.mdx

*Commits: 4cffcb5, f34f5ed*

- Updated CLI usage examples to use `.pflow.md` file extensions instead of `.json`
- Clarified that piped data is only routed to inputs explicitly marked with `stdin: true`:
  ```markdown
  ### data
  - type: string
  - stdin: true
  ```
- Added documentation for the `-p` flag to support workflow chaining:
  ```bash
  pflow -p step1 | pflow -p step2 | pflow step3
  ```
- Added note that explicit CLI parameters override piped stdin data

### docs/reference/cli/mcp.mdx

*Commits: 4cffcb5*

- Converted MCP (Model Context Protocol) node examples from JSON to Markdown format:
  ```markdown
  ### create-issue
  - type: mcp-github-create-issue
  - repo: owner/repo
  - title: Bug report
  ```

### docs/reference/cli/workflow.mdx

*Commits: 4cffcb5*

- Migrated workflow file format from JSON (`.json`) to Markdown (`.pflow.md`)
- Simplified the `pflow workflow save` command by removing the `--description` and `--generate-metadata` flags
- Updated workflow internal structure to use Markdown headers for Inputs, Steps, and Outputs instead of JSON keys:
  ```markdown
  # My Workflow
  ## Inputs
  ### input_name
  - type: string
  ## Steps
  ### read-data
  - type: read-file
  ```
- Removed the requirement for explicit edge declarations; execution is now determined by step order
- Changed the programmatic name assignment to be derived from the filename (e.g., `my-workflow.pflow.md` becomes `my-workflow`)

### docs/reference/experimental.mdx

*Commits: 4cffcb5*

- Updated experimental shell node examples to the new Markdown syntax, using fenced code blocks for commands:
  ```markdown
  ### commit-changes
  - type: shell
  ```shell command
  git add . && git commit -m '${commit_message}'
  ```

### docs/reference/nodes/claude-code.mdx

*Commits: 4cffcb5*

- Converted all `claude-code` node documentation and examples from JSON to Markdown format
- Standardized the use of YAML blocks within Markdown for complex node parameters like `output_schema` and `sandbox` configurations:
  ```yaml output_schema
  risk_level:
    type: str
    description: high, medium, or low
  ```
- Updated example for sandboxed execution to use the new list-based YAML syntax for `excludedCommands`:
  ```yaml sandbox
  enabled: true
  autoAllowBashIfSandboxed: true
  excludedCommands:
    - docker
  ```

### docs/reference/nodes/file.mdx

*Commits: 4cffcb5*

- Changed node configuration examples for file reading, LLM processing, and binary file copying from JSON to a descriptive Markdown format:
  ```markdown
  ### read

  Read the input file.

  - type: read-file
  - file_path: input.txt
  ```

### docs/reference/nodes/http.mdx

*Commits: 4cffcb5*

- Updated all HTTP node examples (GET, POST, authenticated requests, and binary downloads) from JSON blocks to a Markdown list format:
  ```markdown
  ### search

  Search the API with query parameters.

  - type: http
  - url: https://api.example.com/search
  - params:
      q: pflow
      page: 1
      limit: 10
  ```

### docs/reference/nodes/llm.mdx

*Commits: 4cffcb5*

- Converted multiple LLM node configuration examples from raw JSON blocks to a more readable Markdown list format including headers and descriptions
- Updated examples to cover specific use cases like OpenRouter models, local models, image analysis, and entity extraction:
  ````markdown
  ### summarize

  Summarize content using OpenRouter.

  - type: llm
  - model: openrouter/anthropic/claude-sonnet-4-5
  - prompt: Summarize this
  ````

### docs/reference/nodes/mcp.mdx

*Commits: 4cffcb5*

- Transitioned MCP tool workflow examples (GitHub and Slack) from JSON arrays to a descriptive Markdown format
- Added a clarification regarding how tool return values (e.g., issue URLs) are mapped to the shared store:
  ```markdown
  ### notify

  Send a Slack notification with the issue link.

  - type: mcp-slack-send_message
  - channel: "#alerts"
  - text: "Created issue: ${create_issue.issue_url}"
  ```

### docs/reference/nodes/shell.mdx

*Commits: 4cffcb5*

- Replaced JSON-based node configuration examples with a new Markdown format using H3 headers and property lists
- Updated multiple examples (process, list, fetch, extract, deploy, build, cleanup) to the new syntax:
  ````markdown
  ### process
  Process the API response with jq.
  - type: shell
  - stdin: ${api.response}
  ```shell command
  jq -r '.data.name'
  ```
  ````
- Refined text descriptions to refer to "the workflow" generally instead of "the workflow JSON" when discussing inspection and security controls

---

## Draft Entries with Context

### [1/7] Replaced JSON workflow format with a new Markdown-based format (`.pflow.md`) [#80](https://github.com/spinje/pflow/pull/80)

*Commit: `8ef6128` Merge pull request #80 from spinje/feat/markdown-workflow-format

feat: Replace JSON with markdown workflow format (.pflow.md)*
Task: [107](.taskmaster/tasks/task_107/task-review.md)
PR: #80 — feat: Replace JSON with markdown workflow format (.pflow.md) (https://github.com/spinje/pflow/pull/80)
Files: .taskmaster/tasks/task_107/implementation/implementation-plan.md, .taskmaster/tasks/task_107/implementation/kickstart-prompt.md, .taskmaster/tasks/task_107/implementation/progress-log-0b-parser-core.md, .taskmaster/tasks/task_107/implementation/progress-log-arch-fork-a.md, .taskmaster/tasks/task_107/implementation/progress-log-arch-fork-b.md ... and 262 more files

**PR Description**

## Summary

Replace JSON as the workflow file format with `.pflow.md` markdown. A custom 779-line state machine parser produces the same IR dict that `json.load()` previously produced — all downstream validation, compilation, and execution is unchanged. Save preserves original markdown content (no IR-to-markdown serialization). The planner and repair systems are gated (not removed) since their prompts assume JSON format.

264 files changed, +20,529 / -7,997 lines.

## Task

107

## Changes

**Parser & format:**
- Custom line-by-line state machine parser (`src/pflow/core/markdown_parser.py`, 779 lines, 73 tests)
- Three heading levels: `#` workflow, `##` sections (Inputs/Steps/Outputs), `###` entities
- `- key: value` YAML params, fenced code blocks with `language param_name` tags
- Edge generation from document order (linear chains)
- Frontmatter for system metadata on saved workflows
- Test utility `ir_to_markdown()` for converting IR dicts to `.pflow.md` in tests

**Integration (all 7 entry points):**
- CLI file loading, path detection, error display, save command (removed `--description` flag)
- WorkflowManager complete rewrite: `.json` → `.pflow.md`, metadata wrapper → flat frontmatter
- Save service signature: `save(name, markdown_content, metadata)` instead of `save(name, ir, description)`
- MCP resolver: markdown content detection (newline=content, `.pflow.md`=path, single-line=name)
- MCP save tool: restructured 6-function-deep save chain, removed `description`/`generate_metadata` params
- Runtime nested workflow loading via markdown parser
- Error messages: markdown-native with line numbers and suggestions

**Validation:**
- Removed JSON anti-pattern validation layer
- Added unknown param warnings (layer 8) comparing against registry interface metadata
- Added `workflow` type to compiler-special-types allowlist
- Fixed `output_mapping` template registration for nested workflows

**Gating (planner/repair):**
- 6 production entry points gated with `if` guards + comments
- ~516 tests skipped via `pytest.mark.skip`
- All code preserved for future re-enablement after prompt rewrite

**Examples & tests:**
- 30+ JSON workflow files converted to `.pflow.md`
- 8 new invalid `.pflow.md` example files
- 25+ test files migrated from JSON to markdown (200+ occurrences)
- Agent instructions (`cli-agent-instructions.md`) fully rewritten for markdown format

**Documentation:**
- 12 CLAUDE.md files updated across codebase
- All user-facing docs (`docs/`) updated with `.pflow.md` examples
- Architecture docs audited and updated
- Agent-friendly `.pflow.md` usage snippets in `registry describe`, `mcp info`, `workflow list`

## Explanation

LLMs are the primary users of pflow. JSON workflows had significant friction: prompts as single-line escaped strings, no syntax highlighting, documentation separate from workflow, ~20-40% more tokens than needed. Markdown is what LLMs already know — the format looks like documentation because documentation IS the workflow.

The key architectural insight is that markdown is just a different way to produce the same IR dict. The parser is the only new component; everything downstream (validation, compilation, execution) operates on dicts and is unchanged.

Save preserves original markdown content rather than serializing IR back to markdown. This means author formatting, comments, and prose survive save/load cycles. Frontmatter is additive (prepended on save, stripped on load).

The planner and repair systems are gated rather than removed because their LLM prompts assume JSON format. Re-enabling requires prompt rewrites (future task). The gating pattern uses simple `if` guards at entry points with clear comments marking each gate.

## Created Docs

- `.taskmaster/tasks/task_107/task-107.md` — task specification
- `.taskmaster/tasks/task_107/task-review.md` — implementation review
- `.taskmaster/tasks/task_107/implementation/progress-log.md` — detailed progress log (22 entries)
- `.taskmaster/tasks/task_107/starting-context/format-specification.md` — complete format design (27 decisions)
- `.taskmaster/tasks/task_107/implementation/implementation-plan.md` — phased implementation plan

User-facing docs updated: `docs/index.mdx` (new "Workflows are documentation" section), `docs/guides/using-pflow.mdx`, and ~10 other docs files with JSON→markdown migration.

## Testing

Run `make test` to verify all tests pass.

Current state: **3609 passed, 516 skipped, 0 failed**. All 516 skips are legitimate (planner/repair gating + pre-existing LLM API key skips). `make check` passes (ruff, ruff-format, mypy, deptry clean).

**Task 107 Review**

# Task 107 Review: Implement Markdown Workflow Format

## Metadata

- **Branch**: `feat/markdown-workflow-format`
- **Commits**: 20 on branch
- **Scope**: 265 files changed, +20,230 / -8,077 lines
- **Test state**: 3609 passed, 516 skipped, 0 failed

## Executive Summary

Replaced JSON as the workflow file format with `.pflow.md` markdown. A custom 779-line state machine parser (`markdown_parser.py`) produces the same IR dict that `json.load()` previously produced — all downstream validation, compilation, and execution is unchanged. Save preserves original markdown content (no IR-to-markdown serialization). The planner and repair systems are gated (not removed) since their prompts assume JSON format. This was the largest single task in pflow history, touching every integration point in the system.

## Implementation Overview

### What Was Built

1. **Markdown parser** (`src/pflow/core/markdown_parser.py`, 779 lines) — line-by-line state machine that parses `.pflow.md` files into IR dicts. Handles frontmatter, H1/H2/H3 heading hierarchy, `- key: value` YAML params with continuation/nesting, fenced code blocks (including nested fences), prose extraction, edge generation from document order, and param routing by section type.

2. **Test utility** (`tests/shared/markdown_utils.py`, 207 lines) — `ir_to_markdown()` converts any IR dict to valid `.pflow.md` content. `write_workflow_file()` writes it to disk. Test-only, not production. This was the single most important tool for migrating ~200 JSON-writing test occurrences.

3. **WorkflowManager rewrite** — complete storage format change from `.json` to `.pflow.md` with YAML frontmatter for system metadata. Flat metadata structure (no `rich_metadata` wrapper).

4. **Integration at all 7 entry points** — CLI file loading, CLI save command, WorkflowManager, save service, MCP resolver, MCP save tool chain, runtime nested workflow loading.

5. **Example conversions** — 30+ JSON workflow files converted to `.pflow.md`, 8 new invalid example files, 4 old invalid JSON files deleted.

6. **Test migration** — 25+ test files updated from JSON to markdown (200+ individual occurrences).

7. **Planner/repair gating** — 6 production entry points gated with `if` guards, ~10 test files skipped via `pytest.mark.skip`.

8. **Unknown param warnings** — new validation layer (layer 8 in `WorkflowValidator`) comparing node params against registry interface metadata.

9. **Agent instructions** — `cli-agent-instructions.md` fully rewritten for `.pflow.md` format.

10. **Agent-friendly command output** — `.pflow.md` usage snippets added to `registry describe`, `mcp info`, and other discovery commands.

### Deviations from Original Spec

| Area | Spec said | What happened |
|------|-----------|---------------|
| Parser size | ~300-400 lines | 779 lines (more validation, better errors) |
| `ir_to_markdown` | ~100-150 lines | 207 lines (more code block types handled) |
| Inline batch routing | Only from `yaml batch` code blocks | Also from inline `- batch:` params (discovered during Phase 5 agent instruction review) |
| `rich_metadata` flattening | 6 callers | Also `test_executor_service.py` (13 refs), MCP save tests, workflow save service tests |
| MCP save flow | Update `resolve_workflow()` | Built separate save-specific detection — `resolve_workflow()` discards original content, can't be reused for save |
| Phase 3 approach | All fork-session | Hit context window limit after ~12 forks; switched to test-writer-fixer subagents |
| Nested workflow validation | Not in scope | Found and fixed 2 pre-existing bugs (workflow type allowlist, output_mapping template registration) |

## Files Modified/Created

### Core Changes (new files)

- `src/pflow/core/markdown_parser.py` — the parser. `parse_markdown(content) -> MarkdownParseResult`. State machine with `MarkdownParseError` exception type. **This is the single most important new file in the task.**
- `tests/shared/markdown_utils.py` — `ir_to_markdown()` and `write_workflow_file()`. Test infrastructure only.
- `tests/test_core/test_markdown_parser.py` (1896 lines) — 73 tests across 15 categories.
- `examples/invalid/*.pflow.md` — 8 new invalid examples for parse error testing.

### Core Changes (major modifications)

- `src/pflow/core/workflow_manager.py` — complete rewrite. `.json` -> `.pflow.md`, metadata wrapper -> frontmatter, `save(name, ir, description)` -> `save(name, markdown_content, metadata)`, `load()` returns flat metadata, `update_metadata()` does frontmatter read-modify-write.
- `src/pflow/cli/main.py` — ~15 integration points: path detection, file loading, error display, registry lookup, save flow, gating, extension error UX, trace filename derivation.
- `src/pflow/cli/commands/workflow.py` — removed `--description` flag, markdown parsing for save, gated `--generate-metadata`.
- `src/pflow/core/workflow_save_service.py` — `save_workflow_with_options(name, markdown_content, *, force, metadata)`.
- `src/pflow/mcp_server/services/execution_service.py` — save flow restructured (6 functions deep, G8). Separate content detection for save path.
- `src/pflow/mcp_server/utils/resolver.py` — markdown content detection (newline = content, `.pflow.md` = path, single-line = name).
- `src/pflow/core/workflow_validator.py` — removed JSON anti-pattern layer (7), added unknown param warning layer (8).
- `src/pflow/cli/resources/cli-agent-instructions.md` — full rewrite for markdown format.

### Test Files (critical ones)

- `tests/test_core/test_markdown_parser.py` — 73 tests. The most important test file. Covers complete workflow parsing, section handling, entity parsing, YAML params (flat, nested, non-contiguous, block scalars), code blocks (nested fences), param routing, edge generation, frontmatter, validation errors, IR equivalence.
- `tests/test_core/test_ir_examples.py` — validates all example `.pflow.md` files parse correctly.
- `tests/test_docs/test_example_validation.py` — validates all examples in `examples/` directory.
- `tests/test_core/test_workflow_validator.py` — includes 2 new tests for nested workflow validation fixes.

## Integration Points & Dependencies

### Critical Integration Point: The Parser

Everything flows through `parse_markdown()`. It's called from:

| Caller | What it uses |
|--------|-------------|
| `cli/main.py:_try_load_workflow_from_file()` | `.ir` + normalize_ir() |
| `workflow_manager.py:load()` | `.ir`, `.title`, `.description`, `.metadata` |
| `workflow_manager.py:load_ir()` | `.ir` |
| `workflow_save_service.py:_load_from_file()` | `.ir` |
| `mcp resolver:resolve_workflow()` | `.ir` |
| `mcp execution_service.py:save_workflow()` | `.ir` (for validation/display) + original content (for save) |
| `runtime/workflow_executor.py:_load_workflow_file()` | `.ir` |

### The Save Pipeline (most complex integration)

The save pipeline has a dual data flow that future agents must understand:

```
Content (original markdown) ──→ WorkflowManager.save() ──→ disk (with frontmatter)
         ↓
   parse_markdown()
         ↓
      IR dict ──→ validation ──→ format_save_success() (display)
```

**Parse once, use twice.** The IR is for validation and display. The original content string is for saving. These are separate data flows that must not be confused.

### Shared Store / IR Contract

The parser produces the exact same IR dict shape as JSON. Top-level fields: `nodes`, `edges`, `inputs`, `outputs`. `normalize_ir()` adds `ir_version`. No new IR fields were added. The contract is unchanged.

### Frontmatter Metadata Structure (flat)

Previously: `{"ir": {...}, "description": "...", "rich_metadata": {"execution_count": 8, ...}}`

Now (YAML frontmatter):
```yaml
---
created_at: "..."
updated_at: "..."
version: "1.0.0"
execution_count: 8
last_execution_timestamp: "..."
last_execution_success: true
last_execution_params: {version: "1.0.0"}
search_keywords: [changelog, git]
capabilities: [generate reports]
---
```

All fields are top-level. No `rich_metadata` nesting. Every caller that accessed `metadata["rich_metadata"]` was updated.

## Architectural Decisions & Tradeoffs

### Key Decisions

**1. Custom parser, no markdown library.** The format is a DSL using markdown syntax, not a markdown document. Libraries parse `- key: value` as list items, interfering with YAML parsing. Line numbers are free with line-by-line scanning. ~779 lines of focused code vs. fighting a library's AST.

**2. Content preservation on save.** No IR-to-markdown serialization. Save stores the original markdown string with frontmatter prepended. This means author formatting, comments, and prose survive save/load cycles. The `ir_to_markdown()` utility exists only in tests.

**3. Gating vs. removing planner/repair.** Code preserved, entry points guarded with `if` statements and comments. Pattern: `# GATED: [System] disabled pending markdown format migration (Task 107).` Tests skipped via `pytestmark = pytest.mark.skip(...)`. All code is intact for future re-enablement.

**4. Flat metadata.** The `rich_metadata` wrapper was unnecessary indirection. Flattening required updating ~10 callers but simplified everything. Future code accesses `metadata["execution_count"]` directly.

**5. `- batch:` inline routing.** Initially only `yaml batch` code blocks routed to top-level `node["batch"]`. After writing agent instructions that showed inline `- batch:` for simple cases, discovered the parser needed a fix. Added `batch` pop from `all_params` in `_build_node_dict()`, same pattern as `type` extraction.

### Technical Debt Incurred

1. **Planner/repair gating** — 516 skipped tests. These systems need prompt rewrites for markdown format before re-enabling. The gating is clean (entry-point guards + test skips) but it's a lot of dormant code.

2. **MCP agent instructions** — `pflow://instructions` and `pflow://instructions/sandbox` resources still contain JSON examples. Any agent using the MCP server's built-in instructions will produce invalid workflows. Noted for a future task.

3. **`architecture/guides/json-workflows.md`** — added deprecation notice but didn't delete. Still referenced from some places.

## Unexpected Discoveries

### Gotchas That Actually Bit Us

**G1: `Path.stem` double extension.** `Path("my-workflow.pflow.md").stem` returns `"my-workflow.pflow"`, not `"my-workflow"`. Required `if name.endswith(".pflow"): name = name[:-6]` everywhere. 8 locations in WorkflowManager alone.

**G5: CLI has TWO `.json` checks in `_setup_workflow_execution()`.** One for source file path storage, one for workflow name stripping. Both needed updating. Easy to miss the second one.

**G8: MCP save chain is 6 functions deep.** `workflow_save()` → `ExecutionService.save_workflow()` → detection → `parse_markdown()` → `save_workflow_with_options()` → `WorkflowManager.save()`. Every function passed `ir_dict` + `description` separately. All signatures changed.

**normalize_ir() bug.** `_try_load_workflow_from_registry()` never called `normalize_ir()`. In JSON mode, the metadata wrapper always included `ir_version`, so it was never triggered. With markdown, the parser correctly omits `ir_version`, exposing the missing call. Latent bug revealed by format change.

**Nested workflow validation bugs (pre-existing).** `WorkflowValidator` rejected `type: workflow` as unknown (handled by compiler, not registry). `TemplateValidator` didn't register `output_mapping` outputs. Both pre-existing, never tested.

**Batch workflows fail through MCP (pre-existing).** MCP path skips template validation, which has a side effect of registering batch context variables (`item`, `__index__`). Filed as GitHub issue #79.

### Edge Cases Worth Knowing

- **Frontmatter on authored files**: Parser handles both with and without frontmatter. `metadata` field is `None` for authored files.
- **Single-node workflows**: Edge generation produces `[]` (correct — no edges needed).
- **Non-contiguous YAML params**: Prose between `- key: value` lines works. Items collected independently, joined, parsed as single YAML sequence.
- **Nested fences**: 4+ backtick outer fence wrapping 3-backtick inner content. Parser tracks fence length.
- **YAML type coercion**: `yes`/`no`/`on`/`off` become booleans. Matches old JSON behavior. Agents should quote string values.

## Patterns Established

### The Parse-Once Pattern

When both IR and original content are needed (save flows):
```python
result = parse_markdown(content)
ir = result.ir
normalize_ir(ir)
# Use ir for validation/display
# Use content (original string) for saving
```

Never parse twice. Never serialize IR back to markdown.

### The Gating Pattern

```python
# GATED: Planner disabled pending markdown format migration (Task 107).
# Planner prompts assume JSON workflow format. Re-enable after prompt rewrite.
if some_condition:
    click.echo("Feature temporarily unavailable...")
    return
```

Tests: `pytestmark = pytest.mark.skip(reason="Gated pending markdown format migration (Task 107)")`

### The `ir_to_markdown()` Test Pattern

For any test that previously wrote JSON to disk:
```python
from tests.shared.markdown_utils import ir_to_markdown, write_workflow_file

# Instead of: json.dump(ir_dict, f)
write_workflow_file(ir_dict, tmp_path / "workflow.pflow.md")

# Or for inline content:
content = ir_to_markdown(ir_dict, title="Test Workflow")
```

### Extension Detection Pattern

```python
if path.endswith(".pflow.md"):    # markdown workflow
elif path.endswith(".json"):       # reject with migration message
elif path.endswith(".md"):         # reject with rename suggestion
```

## Breaking Changes

### API/Interface Changes

| Component | Old | New |
|-----------|-----|-----|
| `WorkflowManager.save()` | `save(name, ir_dict, description)` | `save(name, markdown_content, metadata=None)` |
| `save_workflow_with_options()` | `(name, workflow_ir, *, force, description, metadata)` | `(name, markdown_content, *, force, metadata)` |
| `ExecutionService.save_workflow()` | `(workflow, name, description, ...)` | `(workflow, name, force)` |
| MCP `workflow_save` tool | `description` + `generate_metadata` params | Both removed |
| CLI `workflow save` | `--description` required flag | Removed (extracted from H1 prose) |
| `WorkflowManager.load()` return | `{"ir": ..., "rich_metadata": {...}}` | Flat dict with all fields top-level |

### Behavioral Changes

- `.json` workflow files are rejected with "no longer supported" error
- `.md` files (non `.pflow.md`) are rejected with rename suggestion
- Planner invocation shows "temporarily unavailable" message
- `--auto-repair` flag is silently disabled
- `--generate-metadata` flag is silently disabled
- Trace filenames now include workflow name from filename stem (was always generic)

## Future Considerations

### Extension Points

- **Conditional branching (Task 38)**: The parser generates linear edges from document order. When branching is added, explicit edge syntax can be introduced (e.g., `- next: node-a, node-b` or a dedicated `## Flow` section). The parser's state machine can accommodate new syntax without restructuring.
- **Planner re-enablement**: Prompts in `src/pflow/planning/` need rewriting to produce `.pflow.md` instead of JSON. The gating guards mark exact re-enablement points.
- **IR-to-markdown serialization**: Currently test-only (`ir_to_markdown()`). If the planner or repair system needs to produce markdown, the test utility could be promoted to production — but it generates minimal output, not pretty output.

### What Would Break If Naively Modified

1. **Changing `parse_markdown()` return type** — 7 callers depend on `.ir`, `.metadata`, `.description`, `.source` fields.
2. **Adding fields to IR top-level** — `additionalProperties: False` in schema. Parser must not produce unexpected fields.
3. **Modifying `_build_node_dict()` param routing** — `type` and `batch` go to top-level, everything else to `params`. Getting this wrong silently breaks workflows.
4. **Touching `update_metadata()` frontmatter handling** — Must correctly split `---` boundaries, never modify the markdown body.

## AI Agent Guidance

### Quick Start for Related Tasks

1. **Read `src/pflow/core/markdown_parser.py`** first — understand `parse_markdown()`, `MarkdownParseResult`, `MarkdownParseError`, and the state machine.
2. **Read `tests/shared/markdown_utils.py`** — `ir_to_markdown()` is essential for writing tests that involve workflow files.
3. **Read `src/pflow/core/ir_schema.py`** — the IR schema is what the parser targets. `validate_ir()` and `normalize_ir()` are called on every load path.
4. **The IR dict is the universal interface.** Everything downstream of the parser operates on dicts. If you're adding a feature that doesn't touch file I/O, you don't need to know about markdown at all.

### Common Pitfalls

1. **Forgetting `normalize_ir()`** — the parser does NOT add `ir_version`. Every load path must call `normalize_ir()` before validation. Missing this causes `'ir_version' is a required property` errors.
2. **`Path.stem` on `.pflow.md`** — returns `"name.pflow"`, not `"name"`. Always strip the `.pflow` suffix.
3. **Testing with `json.dump()` instead of `write_workflow_file()`** — the CLI rejects `.json` files. All test workflows must be `.pflow.md`.
4. **Accessing `metadata["rich_metadata"]`** — this wrapper no longer exists. All metadata fields are top-level.
5. **MCP save vs. execute paths** — save needs original content preservation; execute only needs IR. The MCP save path does NOT use `resolve_workflow()` because that function discards content.

### Test-First Recommendations

When modifying parser behavior:
- Run `pytest tests/test_core/test_markdown_parser.py -x` first (73 tests, fast)
- Then `pytest tests/test_core/test_ir_examples.py -x` (validates all example files)
- Then `pytest tests/test_docs/test_example_validation.py -x` (validates examples/ directory)

When modifying save/load:
- Run `pytest tests/test_core/test_workflow_manager.py tests/test_execution/test_executor_service.py -x`
- Then `pytest tests/test_cli/test_workflow_save_cli.py tests/test_cli/test_workflow_save.py -x`

---

*Generated from implementation context of Task 107*

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [2/7] Added disallowed_tools parameter to claude-code node [#78](https://github.com/spinje/pflow/pull/78)

*Commit: `fd6c712` Merge pull request #78 from spinje/feat/disallowed-tools

feat: add disallowed_tools parameter to claude-code node (closes #77)*
PR: #78 — feat: add disallowed_tools parameter to claude-code node (closes #77) (https://github.com/spinje/pflow/pull/78)
Files: src/pflow/nodes/claude/claude_code.py, tests/test_nodes/test_claude/test_claude_code.py

**PR Description**

## Summary

Add `disallowed_tools` parameter to the claude-code node, exposing the SDK's built-in denylist. This is more practical than allowlists for autonomous workflows — block the 2-3 dangerous tools instead of enumerating every needed one.

## Changes

- Added `disallowed_tools: list` parameter to claude-code node interface
- Added `_validate_disallowed_tools()` validation (mirrors existing `_validate_tools`)
- Pass-through to `ClaudeAgentOptions` in `_build_claude_options()`
- 6 new tests covering default, patterns, type validation, and coexistence with `allowed_tools`

## Explanation

The implementation mirrors the existing `allowed_tools` pattern exactly — validate it's a list, pass through to SDK. The SDK handles pattern matching (e.g., `Bash(pflow:*)`, `Bash(make:*)`). This is especially useful for fork-session subagents where you need to prevent recursive pflow calls or concurrent make runs.

Manually verified end-to-end: a workflow with `"disallowed_tools": ["Bash(make:*)"]` correctly blocks `make test` with the SDK reporting "Permission to use Bash with command make test has been denied."

## Testing

Run `make test` to verify all 4065 tests pass.

```
uv run pytest tests/test_nodes/test_claude/test_claude_code.py -v -k "disallowed"
```

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [3/7] Added Python code node for in-process data transformation [#75](https://github.com/spinje/pflow/pull/75)

*Commit: `d871bf6` Merge pull request #75 from spinje/feat/python-code

feat: add Python code node for in-process data transformation*
Task: [104](.taskmaster/tasks/task_104/task-review.md)
PR: #75 — feat: add Python code node for in-process data transformation (https://github.com/spinje/pflow/pull/75)
Files: .taskmaster/tasks/task_104/implementation/implementation-plan.md, .taskmaster/tasks/task_104/implementation/progress-log.md, .taskmaster/tasks/task_104/research/braindump-sandbox-decision-reversal.md, .taskmaster/tasks/task_104/research/task-104-handover.md, .taskmaster/tasks/task_104/starting-context/braindump-comprehensive-design-session.md ... and 10 more files

**PR Description**

## Summary

Add a `code` node type that executes Python code in-process with native object inputs, solving the shell node's fundamental limitations for data transformation (single stdin, JSON serialization overhead, escaping complexity).

## Task

Task 104

## Changes

- **New node**: `src/pflow/nodes/python/python_code.py` — `PythonCodeNode` (336 lines) with `"type": "code"` in workflows
- **AST-based type validation**: Extracts type annotations from code via `ast.parse()`, validates input types before execution (outer type only: `list[dict]` checks `isinstance(value, list)`)
- **ThreadPoolExecutor timeout**: Cross-platform timeout with `shutdown(wait=False)` — discovered and fixed a critical bug where `with ThreadPoolExecutor` context manager blocks indefinitely on timeout
- **stdout/stderr capture**: Via `contextlib.redirect_stdout/stderr` in worker thread
- **31 behavioral tests**: Organized by user-facing concerns, plus one integration test through the full compiler/executor pipeline (registry discovery → template resolution → namespaced output)
- **Zero existing files modified**: Node auto-discovered by registry scanner

## Explanation

The shell node requires JSON serialization for data passing and only supports a single stdin input. The code node solves this by running Python code in-process where inputs are native objects — lists, dicts, ints are passed directly without serialization.

Type annotations are **required** (not optional) as a strategic decision for Task 107 (markdown workflows), where they enable Python IDE tooling (mypy, LSP) in markdown code blocks.

No sandboxing — explicit user decision documented in braindump. Python language-level sandboxing is fundamentally bypassable. Real utility (pandas, external libs) prioritized for local automation. Container sandboxing deferred to Task 87.

## Example Usage

```json
{
  "id": "transform",
  "type": "code",
  "params": {
    "inputs": {"data": "${fetch.result}", "count": 10},
    "code": "data: list\ncount: int\n\nresult: list = data[:count]"
  }
}
```

## Created Docs

- `.taskmaster/tasks/task_104/task-104.md`
- `.taskmaster/tasks/task_104/task-review.md`
- `.taskmaster/tasks/task_104/implementation/progress-log.md`
- `.taskmaster/tasks/task_104/implementation/implementation-plan.md`

## Testing

```bash
make test   # Full suite (31 new tests, 0.35s)
make check  # Lint + type check (ruff, mypy, deptry all clean)
```

**Task 104 Review**

# Task 104 Review: Python Code Node for Data Transformation

## Executive Summary

Added a `code` node type (`"type": "code"`) that executes Python code in-process with native object inputs, AST-extracted type annotations, stdout/stderr capture, and ThreadPoolExecutor timeout. Zero existing files modified — the node is fully self-contained and auto-discovered by the registry scanner. Required type annotations are a strategic decision for Task 107 (markdown workflow IDE support).

## Implementation Overview

### What Was Built

A single node class (`PythonCodeNode`) in 336 lines that:
- Accepts a `code` string param and an `inputs` dict param
- Extracts type annotations from code via `ast.parse()` (~6 lines of AST walking)
- Validates input types match annotations (outer type only: `list[dict]` checks `isinstance(value, list)`)
- Executes code via `exec()` in a ThreadPoolExecutor thread with configurable timeout
- Captures stdout/stderr via `contextlib.redirect_stdout/stderr`
- Writes `result`, `stdout`, `stderr` to shared store

### Deviations from Spec

1. **Phased implementation collapsed to one pass** — the node is small enough that stubs add overhead without value.
2. **ThreadPoolExecutor used WITHOUT context manager** — the spec's suggested `with ThreadPoolExecutor` pattern causes the `__exit__` to block on `shutdown(wait=True)`, defeating the timeout. See "Critical Bug" below.
3. **`TypeError` (not `ValueError`) for wrong `inputs` param type** — ruff TRY004 correctly identified this as a type error.

## Files Created

### Core Changes
- `src/pflow/nodes/python/__init__.py` — Exports `PythonCodeNode`
- `src/pflow/nodes/python/python_code.py` — Full node implementation (336 lines)

### Test Files
- `tests/test_nodes/test_python/__init__.py` — Package init
- `tests/test_nodes/test_python/test_python_code.py` — 31 tests (0.35s)

## Integration Points & Dependencies

### How the Code Node Plugs Into pflow

The node requires **zero changes to existing code**. It integrates through three automatic mechanisms:

1. **Registry Scanner** (`scanner.py:53-65`): Discovers `PythonCodeNode` via file scan of `src/pflow/nodes/python/`. The explicit `name = "code"` class attribute overrides the auto-derived `"python-code"`.

2. **Compiler** (`compiler.py:163-202`): Resolves `"type": "code"` by looking up the registry, importing `pflow.nodes.python.python_code.PythonCodeNode`, then wrapping it with `TemplateAwareNodeWrapper` → `NamespacedNodeWrapper` → `InstrumentedNodeWrapper`.

3. **Template Resolution** (`node_wrapper.py:797-952`): The wrapper resolves `${...}` inside the `inputs` dict **before** `prep()` runs. This is the critical seam — the code node receives native Python objects, not template strings.

### Shared Store Keys

| Key | Type | When |
|---|---|---|
| `result` | any | Success — value of the `result` variable after execution |
| `stdout` | str | Always — captured `print()` output |
| `stderr` | str | Always — captured stderr output |
| `error` | str | Error only — formatted error message |

With namespacing (always active in compiled workflows): `shared["node_id"]["result"]` etc.

## Architectural Decisions & Tradeoffs

### Key Decisions

**Required type annotations** — All inputs and `result` must have type annotations in the code string. This adds 2-3 lines of boilerplate per node but enables Python IDE tooling (mypy, LSP) in Task 107's markdown code blocks. The user explicitly decided this is strategic, not cosmetic.

**No sandboxing** — Unrestricted `__builtins__` and imports. Python language-level sandboxing is fundamentally bypassable via object traversal. Users need real libraries (pandas, etc.). Container sandboxing deferred to Task 87.

**Outer type validation only** — `list[dict]` validates `isinstance(value, list)`, does not inspect element types. Catches 90% of type errors with zero complexity. Deep validation deferred.

**`max_retries=1, wait=0`** — Code execution is deterministic. A NameError won't fix itself on retry. This differs from most other nodes (shell: `max_retries=1`, LLM: `max_retries=3`).

### Technical Debt

- **Zombie threads on timeout**: When code times out, the worker thread keeps running until process exit. `pool.shutdown(wait=False, cancel_futures=True)` prevents blocking but doesn't kill the thread. Acceptable for MVP; Task 87 (container sandboxing) is the real fix.
- **`requires` field is documentation-only**: No validation, no auto-install. ImportError bubbles naturally. Option to catch ImportError and cross-reference against `requires` is easy to add later.

## Unexpected Discoveries

### Critical Bug: ThreadPoolExecutor Context Manager Defeats Timeout

**This is the most important finding of this implementation.**

```python
# BROKEN — blocks for the full sleep duration, not 0.1s
with ThreadPoolExecutor(max_workers=1) as pool:
    future = pool.submit(lambda: time.sleep(5))
    future.result(timeout=0.1)  # TimeoutError raised, but...
# __exit__ calls shutdown(wait=True) → blocks until thread finishes → 5 seconds

# CORRECT — returns immediately on timeout
pool = ThreadPoolExecutor(max_workers=1)
future = pool.submit(lambda: time.sleep(5))
try:
    future.result(timeout=0.1)  # TimeoutError after 0.1s
finally:
    pool.shutdown(wait=False, cancel_futures=True)  # Non-blocking cleanup
```

**Impact**: Any future node that uses ThreadPoolExecutor for timeout MUST NOT use the context manager pattern.

### AST Annotations vs. exec() Annotations

Type annotations exist in two different worlds:
- **AST-time**: Annotations are strings — `ast.unparse(node.annotation)` returns `"DataFrame"` without evaluating it
- **exec()-time**: Annotations are evaluated — `data: DataFrame` triggers `NameError` if `pandas` isn't imported

This means a user writing `data: DataFrame` MUST `import pandas as pd` in their code, even though our type validator doesn't check the type (it's not in `_TYPE_MAP`). The validator skips unknown types gracefully, but `exec()` will crash.

### mypy Type Narrowing with `self.params.get()`

`self.params.get("key")` returns `Any`. When validating, `isinstance()` must be the **dominant** guard for mypy to narrow the type:

```python
# FAILS mypy — truthiness check before isinstance prevents narrowing
if not code or not isinstance(code, str) or not code.strip():
    raise ValueError(...)
return code  # mypy: "Returning Any from function declared to return str"

# PASSES mypy — isinstance is the first meaningful check
if not isinstance(code, str) or not code.strip():
    raise ValueError(...)
return code  # mypy: str ✓
```

## Patterns Established

### The `inputs` Dict Pattern

The code node introduced a new param pattern: a dict of template-resolved values that become execution-namespace variables. This is different from other nodes where params are flat key-value pairs.

```json
"params": {
    "inputs": {"data": "${upstream.result}", "limit": 10},
    "code": "data: list\nlimit: int\nresult: list = data[:limit]"
}
```

The `TemplateAwareNodeWrapper` resolves templates **inside nested dicts** automatically — no special handling needed. This was verified in the integration test.

### Explicit `name` Class Attribute for Type Override

```python
class PythonCodeNode(Node):
    name = "code"  # Workflow uses "type": "code", not "python-code"
```

The scanner checks `name` attribute first (`scanner.py:56`), then falls back to kebab-case from class name. Use this when the desired workflow type name doesn't match the class name's kebab-case form.

## Testing Implementation

### Test Philosophy Applied

Started with 34 tests (auto-generated), reviewed critically, ended with 31. Removed tests that tested implementation details (private helpers, config constants, Python builtins). Added tests that catch real user-facing issues (dict results for downstream access, None inputs, pass-by-reference semantics).

### Tests That Catch Real Issues

| Test | What It Prevents |
|---|---|
| `test_dict_result_with_structured_data` | Regression in the primary use case — `${node.result.field}` access |
| `test_code_node_in_compiled_workflow` | Breaks in registry discovery, template resolution, or namespacing |
| `test_none_input_fails_type_check` | Upstream node returning None silently passes through |
| `test_timeout_stops_long_running_code` | Timeout regression (the ThreadPoolExecutor bug) |
| `test_input_mutation_affects_original` | Documents pass-by-reference — if someone adds defensive copying, this test validates or catches the change |
| `test_result_type_mismatch_caught_in_post` | Type annotation contract enforcement |

### Tests That Are Nice-to-Have

`test_bool_passes_as_int`, `test_int_passes_as_float`, `test_unknown_type_annotation_skips_check` — these document Python semantics and `_TYPE_MAP` design decisions. They're more documentation than regression prevention.

## AI Agent Guidance

### Quick Start for Related Tasks

**Task 107 (Markdown Workflows)**: The code node's type annotations are your hook. `_extract_annotations()` returns a dict of `{var_name: type_string}` that you can use to generate `.pyi` stubs for IDE support. The annotations are AST strings, not evaluated types.

**Task 87 (Container Sandboxing)**: The timeout mechanism already works correctly. To add container isolation, you'd replace `ThreadPoolExecutor.submit(exec, ...)` with subprocess execution. The `exec_fallback` error formatting would need container-specific error types.

**Task 113 (TypeScript Code Node)**: Follow this exact file structure (`src/pflow/nodes/typescript/`) with an explicit `name = "typescript-code"` or similar. The pattern of `inputs` dict → namespace injection → single `result` output is reusable.

### Key Files to Read First

1. `src/pflow/nodes/python/python_code.py` — the implementation
2. `src/pflow/nodes/CLAUDE.md` — the node pattern rules
3. `src/pflow/runtime/node_wrapper.py:797-952` — template resolution flow (critical for understanding what the node receives)

### Common Pitfalls

1. **Never use `with ThreadPoolExecutor` for timeout** — `__exit__` blocks. Use manual `pool.shutdown(wait=False, cancel_futures=True)`.
2. **Never add try/except to `exec()`** — breaks PocketFlow retry mechanism. Handle errors in `exec_fallback()`.
3. **`isinstance` must be the dominant guard** when narrowing `Any` from `self.params.get()` for mypy.
4. **Type annotations in exec() are evaluated eagerly** — users must import custom types before annotating with them.
5. **Test timeout tests with very short durations** (0.01s sleep, 0.001s timeout) to avoid zombie thread slowdowns in the test suite.

---

*Generated from implementation context of Task 104*

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [4/7] Added automatic stdin routing for Unix-first piping [#73](https://github.com/spinje/pflow/pull/73)

*Commit: `0f0cfc2` Merge pull request #73 from spinje/feat/stdin-routing

feat: automatic stdin routing for Unix-first piping (fixes #72)*
Task: [115](.taskmaster/tasks/task_115/task-review.md)
PR: #73 — feat: automatic stdin routing for Unix-first piping (fixes #72) (https://github.com/spinje/pflow/pull/73)
Files: .taskmaster/tasks/task_115/implementation/progress-log.md, .taskmaster/tasks/task_115/starting-context/braindump-research-complete.md, .taskmaster/tasks/task_115/starting-context/implementation-prompt.md, .taskmaster/tasks/task_115/starting-context/research-findings.md, .taskmaster/tasks/task_115/starting-context/task-115-spec.md ... and 30 more files

**PR Description**

## Summary

Implements automatic stdin routing for Unix-style workflow chaining. Piped stdin now routes to workflow inputs marked with `"stdin": true`, enabling:

```bash
pflow -p workflow1.json | pflow workflow2.json
cat data.json | pflow transform.json | pflow analyze.json > report.md
```

## Task

Task 115

## Changes

### Core Implementation
- **IR Schema**: Added `stdin` boolean field to input declarations
- **Stdin Routing**: Routes piped stdin to `stdin: true` input before validation
- **FIFO Detection**: Distinguishes real pipes from sockets to enable proper blocking
- **Legacy Removal**: Removed `populate_shared_store()` and `${stdin}` pattern

### Files Modified
- `src/pflow/core/ir_schema.py` - Added stdin field to schema
- `src/pflow/core/shell_integration.py` - FIFO detection, removed legacy function
- `src/pflow/cli/main.py` - Stdin routing logic (5 new helper functions)
- `src/pflow/runtime/workflow_validator.py` - Multi-stdin validation
- `src/pflow/execution/executor_service.py` - Removed legacy call
- `src/pflow/planning/nodes.py` - Removed stdin checking from planner

### Tests Added
- `test_workflow_chaining_producer_to_consumer` - Real subprocess pipe test
- `test_three_stage_pipeline` - Multi-stage pipeline test
- `test_stdin_has_data_returns_true_for_fifo` - FIFO detection
- `test_stdin_has_data_uses_select_for_non_fifo_non_tty` - Socket fallback (Claude Code)
- `test_stdin_error_when_no_stdin_input_declared` - Error path
- `test_cli_param_overrides_stdin` - CLI override behavior

## Explanation

### Why FIFO Detection?

The original spec only covered stdin routing, but workflow chaining still failed. Root cause: when shell pipes two processes (`A | B`), they start simultaneously. Process B checks for stdin before Process A writes anything.

The fix distinguishes between:
- **FIFO pipes** (real `|` pipes): Return True immediately, block on read (like cat, grep, jq)
- **Sockets** (Claude Code environment): Use `select()` with timeout=0 to avoid hanging

### Why Remove `${stdin}`?

The `stdin: true` pattern is strictly more flexible:
- Same workflow works via piping OR CLI arguments
- CLI param can override stdin for debugging
- Explicit declaration is more agent-friendly than magic shared store keys

## Created Docs

- `.taskmaster/tasks/task_115/task-115.md` - Task specification
- `.taskmaster/tasks/task_115/task-review.md` - Implementation review
- `.taskmaster/tasks/task_115/implementation/progress-log.md` - Development log

## Testing

```bash
make test   # 4016 tests pass
make check  # All linting/type checks pass
```

Manual verification:
```bash
# Direct pipe chaining works
pflow -p producer.json | pflow -p consumer.json

# Three-stage pipeline works
pflow -p a.json | pflow -p b.json | pflow -p c.json

# Terminal mode doesn't hang
pflow consumer.json  # Shows required input error, doesn't hang
```

---

Fixes #72

**Task 115 Review**

# Task 115 Review: Automatic Stdin Routing for Unix-First Piping

## Metadata
- **Implementation Date**: 2026-01-22
- **Status**: Complete
- **Branch**: `feat/stdin-routing`

## Executive Summary

Implemented explicit stdin routing via `"stdin": true` input declarations, enabling Unix-style workflow chaining (`pflow -p workflow1.json | pflow workflow2.json`). The final implementation uses FIFO-only detection - stdin is read only from real shell pipes, avoiding complexity with `select()` which proved unreliable. All 4019 tests pass.

## Implementation Overview

### What Was Built

1. **IR Schema Extension**: Added `stdin` boolean field to workflow input declarations
2. **Stdin Routing Logic**: Routes piped stdin to the input marked `"stdin": true`
3. **FIFO Pipe Detection**: Distinguishes real pipes from sockets to enable proper blocking
4. **Legacy Removal**: Completely removed the `${stdin}` shared store pattern

### Deviations from Original Spec

**FIFO Detection (Not in Spec)**: The spec assumed stdin routing was the only change needed. In reality, workflow chaining failed because `stdin_has_data()` used `select()` with timeout=0, which returns immediately before the upstream process writes data. This required detecting FIFO pipes and blocking appropriately.

**Error Function Signature**: Spec suggested `_route_stdin_to_params()` return `(params, error)` tuple. Implemented with `NoReturn` function `_show_stdin_routing_error()` instead, matching existing CLI patterns (direct `click.echo()` + `ctx.exit(1)`).

### Implementation Approach

The key insight was that stdin routing must happen **inside** `_validate_and_prepare_workflow_params()`, between parameter parsing and input validation. If routing happens after, required inputs fail validation before stdin is considered.

For stdin detection, the final solution is FIFO-only:
- **FIFO pipes** (real `|` pipes): `stdin_has_data()` returns True, caller blocks on `sys.stdin.read()`
- **Everything else** (terminals, sockets, char devices, StringIO): Returns False, no read attempted

This is simpler than using `select()`, which proved unreliable (see Unexpected Discoveries).

## Files Modified/Created

### Core Changes

| File | Change | Impact |
|------|--------|--------|
| `src/pflow/core/ir_schema.py` | Added `stdin` boolean field to input schema | Schema now accepts `"stdin": true` |
| `src/pflow/core/shell_integration.py` | Added FIFO detection in `stdin_has_data()`, removed `populate_shared_store()` | Workflow chaining works, legacy pattern removed |
| `src/pflow/core/__init__.py` | Removed `populate_shared_store` export | API cleanup |
| `src/pflow/runtime/workflow_validator.py` | Added multi-stdin validation | Rejects workflows with multiple `stdin: true` inputs |
| `src/pflow/cli/main.py` | Added stdin routing helpers, modified `_validate_and_prepare_workflow_params()` | Stdin routes to correct input before validation |
| `src/pflow/execution/executor_service.py` | Removed `populate_shared_store()` call | Legacy pattern fully removed |
| `src/pflow/planning/nodes.py` | Removed stdin checking from ParameterDiscoveryNode | Planner no longer knows about stdin |

### Test Files

| File | Purpose | Criticality |
|------|---------|-------------|
| `tests/test_cli/test_dual_mode_stdin.py` | Workflow chaining, error paths, CLI override | **Critical** - tests real subprocess piping |
| `tests/test_core/test_stdin_no_hang.py` | FIFO detection, socket fallback | **Critical** - prevents Claude Code regression |
| `tests/test_shell_integration.py` | Removed `populate_shared_store` tests | Cleanup |

## Integration Points & Dependencies

### Incoming Dependencies

```
CLI (_handle_named_workflow)
  → _validate_and_prepare_workflow_params(stdin_data=...)
    → _route_stdin_to_params()
      → _find_stdin_input()
```

### Outgoing Dependencies

```
stdin_has_data()
  → read_stdin() / read_stdin_enhanced()
    → CLI main loop
```

### Shared Store Keys

**Removed**:
- `shared["stdin"]` - Legacy pattern, no longer used
- `shared["stdin_binary"]` - Legacy pattern, no longer used
- `shared["stdin_path"]` - Legacy pattern, no longer used

**No new keys added** - stdin routes directly to workflow input params.

## Architectural Decisions & Tradeoffs

### Key Decisions

| Decision | Reasoning | Alternative Considered |
|----------|-----------|----------------------|
| Explicit `stdin: true` declaration | Predictable, agent-friendly, no magic | Type-based auto-detection (rejected - too magical) |
| FIFO-only detection | Simple, reliable, matches Unix standard | `select()` check (rejected - lies on char devices) |
| CLI param overrides stdin | Debugging/testing flexibility | Stdin always wins (rejected - less useful) |
| Remove `${stdin}` entirely | `stdin: true` is strictly more flexible | Keep both patterns (rejected - confusing) |
| Empty string is valid content | Unix standard - empty pipe routes empty string | Treat as no input (rejected - breaks Unix semantics) |

### Technical Debt Incurred

**None significant**. The implementation is clean and follows existing patterns.

### Design Decisions - Rejected Alternatives

The following alternatives were considered during code review and explicitly rejected:

**1. Implicit `stdin: true` for Single-Input Workflows**

*Proposal*: When a workflow has exactly one input, automatically route stdin there without requiring explicit `stdin: true`.

*Rejected because*:
- **Explicit is predictable**: A workflow with `{"inputs": {"file_path": {"type": "string"}}}` wants a PATH, not piped file CONTENT. Implicit routing would cause unexpected behavior.
- **Consistency**: Multi-input workflows require explicit declaration; single-input being implicit creates inconsistency.
- **Low cost of explicit**: Adding `"stdin": true` is 5 characters - trivial burden for clear intent.
- **Spec explicitly excluded this**: "Does not implement type-based auto-detection of stdin target"

*Better alternative*: If users frequently forget `stdin: true`, consider a CLI warning: "Piped data ignored. Did you mean to add stdin: true to input 'data'?"

**2. Type Coercion for Stdin Input**

*Proposal*: When input declares `"type": "object"` and stdin contains valid JSON, auto-parse it.

*Rejected because*:
- **Adds magic behavior**: Parsing sometimes but not others is confusing
- **Explicit is simpler**: Workflows can use `jq` or template expressions for parsing
- **Error handling complexity**: What if JSON is malformed? Silent string fallback?
- **Current approach is predictable**: Stdin is always a string; workflow decides how to use it

**3. Using `select()` for Non-FIFO Detection**

*Proposal*: Use `select.select([sys.stdin], [], [], 0)` to check if data is available for non-FIFO stdin.

*Rejected because*:
- **`select()` lies on character devices**: In Claude Code, stdin is a char device where `select()` returns "ready" even with no data
- **Complexity**: Added fallback logic that was still unreliable
- **Unix tools don't do this**: cat, grep, jq just check `isatty()` and read

*Final approach*: FIFO-only detection. If stdin is a FIFO pipe, read it. Otherwise, don't. Simple and reliable.

These decisions prioritize predictability and explicitness over convenience, which aligns with pflow's design philosophy as an agent-friendly tool where behavior should be obvious from the workflow definition.

### Potential Future Work

- **Auto `-p` flag**: When stdout is piped, auto-enable print mode (separate task)
- **Binary stdin routing**: Currently only text is routed; binary could route to temp file path

## Testing Implementation

### Test Strategy Applied

1. **Unit tests**: FIFO detection, socket fallback behavior
2. **Integration tests**: Real subprocess piping with `shell=True`
3. **Behavior tests**: Error messages, CLI override, validation errors

### Critical Test Cases

| Test | What It Validates |
|------|-------------------|
| `test_workflow_chaining_producer_to_consumer` | **THE key test** - real subprocess pipe works |
| `test_three_stage_pipeline` | Multi-stage piping works |
| `test_stdin_has_data_returns_true_for_fifo` | FIFO detection returns True for real pipes |
| `test_stdin_has_data_non_fifo_returns_false` | Non-FIFO (char device, socket) returns False |
| `test_stringio_returns_false` | CliRunner compatibility - StringIO returns False |
| `test_stdin_error_when_no_stdin_input_declared` | Error message is agent-friendly |

## Unexpected Discoveries

### Gotchas Encountered

1. **Workflow chaining timing**: Shell starts both processes simultaneously. Process B checks stdin before Process A writes. This is why FIFO detection was essential.

2. **`select()` lies on character devices**: In Claude Code, stdin is a character device (S_ISCHR=True). `select()` returns "ready" even when no data exists, but `stdin.read()` hangs forever. This led to abandoning `select()` entirely in favor of FIFO-only detection.

3. **Claude Code stdin is NOT a socket**: Earlier assumption was wrong. It's a character device, which behaves differently. FIFO-only detection handles this correctly (char devices are not FIFOs → no read → no hang).

4. **SIGPIPE handling**: Already handled in main.py with `SIG_IGN`. Without this, large data piping would fail.

### Edge Cases Found

| Scenario | Behavior |
|----------|----------|
| Empty stdin (`echo -n "" \| pflow`) | Routes empty string (valid content per Unix standard) |
| Binary stdin | Not routed, falls back to normal required input behavior |
| Large stdin (>10MB) | Handled via temp file, not routed (text only) |
| Formatted JSON output | jq may fail parsing multi-line JSON - use `-c` for compact |
| CliRunner tests | StringIO has no fileno → `stdin_has_data()` returns False → must mock for stdin tests |

## Patterns Established

### Reusable Patterns

**FIFO-Only Stdin Detection** (the final, simplified approach):
```python
def stdin_has_data() -> bool:
    if sys.stdin.isatty():
        return False
    try:
        fd = sys.stdin.fileno()
    except:
        return False  # StringIO has no fileno
    try:
        mode = os.fstat(fd).st_mode
        return stat.S_ISFIFO(mode)  # Only True for real pipes
    except:
        return False
```

**Agent-Friendly Error Messages**:
```python
def _show_stdin_routing_error(ctx: click.Context) -> NoReturn:
    click.echo("❌ Piped input cannot be routed to workflow", err=True)
    click.echo('   This workflow has no input marked with "stdin": true.', err=True)
    # Show JSON example of the fix
    click.echo('     "inputs": {', err=True)
    click.echo('       "data": {"type": "string", "required": true, "stdin": true}', err=True)
    click.echo("     }", err=True)
    ctx.exit(1)
```

### Anti-Patterns to Avoid

1. **Don't use `select()` for stdin detection** - It lies on character devices (returns "ready" with no data). Use FIFO detection instead.

2. **Don't put stdin in shared store** - It bypasses input validation and loses CLI override capability.

3. **Don't auto-detect stdin target by type** - Explicit declaration is more predictable for agents.

4. **Don't assume non-TTY means pipe** - Claude Code stdin is non-TTY but not a pipe. Only FIFOs are real pipes.

## Breaking Changes

### API/Interface Changes

- **Removed**: `populate_shared_store()` function from public API
- **Removed**: `${stdin}` template variable support

### Behavioral Changes

- Piping to workflow without `stdin: true` now shows helpful error (previously silent failure)
- Workflows with `stdin: true` work identically via pipe OR CLI args

## Future Considerations

### Extension Points

1. **Auto `-p` flag** (Task mentioned in task-115.md): When stdout is piped, auto-enable print mode
2. **Binary stdin routing**: Route to temp file path for binary data inputs
3. **Multiple stdin sources**: Currently one input; could support named pipes

### Scalability Concerns

None. FIFO detection is O(1) syscall. No memory overhead.

## AI Agent Guidance

### Quick Start for Related Tasks

1. **Read first**: `src/pflow/core/shell_integration.py` - the FIFO detection logic
2. **Understand**: The distinction between FIFO (pipes) and sockets (Claude Code)
3. **Test with**: Real subprocess tests using `shell=True` for actual pipe behavior

### Common Pitfalls

1. **CliRunner doesn't simulate real pipes** - Use subprocess tests for real pipe behavior
2. **CliRunner tests need mocking** - Mock `stdin_has_data` to return True when testing stdin routing with CliRunner
3. **Don't forget SIGPIPE** - It's handled in main.py, must stay `SIG_IGN`
4. **Stdin routing is BEFORE validation** - Position matters in `_validate_and_prepare_workflow_params()`
5. **Don't use `select()` for stdin** - It lies on character devices; use FIFO detection

### Test-First Recommendations

When modifying stdin handling:
1. Run `test_stdin_no_hang_integration` first - ensures Claude Code doesn't hang
2. Run `test_workflow_chaining_producer_to_consumer` - ensures pipes work
3. Run full `tests/test_cli/test_dual_mode_stdin.py` suite

### Key Files to Understand

```
src/pflow/core/shell_integration.py:68-112  # stdin_has_data() with FIFO detection
src/pflow/cli/main.py:3073-3190              # Stdin routing helpers
src/pflow/cli/main.py:3257-3262              # Call site in _handle_named_workflow
```

---

*Generated from implementation context of Task 115*

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [5/7] Fixed detection of JSON strings with embedded templates during workflow validation [#69](https://github.com/spinje/pflow/pull/69)

*Commit: `40e1c8a` Merge pull request #69 from spinje/fix/string-params-parse

fix: detect JSON strings with embedded templates during validation (fixes #68)*
PR: #69 — fix: detect JSON strings with embedded templates during validation (fixes #68) (https://github.com/spinje/pflow/pull/69)
Files: src/pflow/core/workflow_validator.py, tests/test_core/test_json_string_template_validation.py

**PR Description**

## Summary

Adds validation to detect when users pass JSON strings with embedded template variables to `str`-typed parameters - an anti-pattern that silently passes validation but fails at runtime when template values contain newlines, quotes, or backslashes.

## Changes

- Added `_check_json_string_with_template()` - detects the anti-pattern in individual parameters
- Added `_build_param_type_map()` - extracts param types from node interface metadata
- Added `_validate_json_string_templates()` - orchestrates validation across all workflow nodes
- Integrated as step 6 in `WorkflowValidator.validate()`
- Added 30 comprehensive tests

## Explanation

When users see a parameter typed as `str` that expects JSON, they often write:
```json
"body_schema": "{\"content\": \"${var}\"}"
```

This fails at runtime when `${var}` resolves to text with newlines (literal `\n` bytes inside JSON string = invalid). The correct pattern is object syntax:
```json
"body_schema": {"content": "${var}"}
```

The validation detects this by checking for strings that:
1. Have `str` expected type
2. Start with `{"` or `["` (unambiguous JSON start)
3. Contain `${` template syntax

Error message example:
```
Node 'format' parameter 'prompt' will fail if ${generate.response} contains newlines or quotes.

Replace JSON string with object syntax:
  ✗  "prompt": "{\"content\": \"${generate.response}\"}"
  ✓  "prompt": {"content": "${generate.response}"}

Objects are auto-serialized with proper JSON escaping.
```

## Testing

Run `make test` to verify all tests pass.

```
 src/pflow/core/workflow_validator.py               | 171 +++++++
 tests/test_core/test_json_string_template_validation.py | 504 +++++++++++++++++++++
 2 files changed, 675 insertions(+)
```

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [6/7] Unified pre-execution validation with the --validate-only flag [#67](https://github.com/spinje/pflow/pull/67)

*Commit: `243e4dc` Merge pull request #67 from spinje/fix/unify-pre-execution-validation

fix: unify pre-execution validation with --validate-only (fixes #66)*
PR: #67 — fix: unify pre-execution validation with --validate-only (fixes #66) (https://github.com/spinje/pflow/pull/67)
Files: src/pflow/cli/main.py, tests/test_cli/test_auto_repair_flag.py, tests/test_cli/test_validation_before_execution.py, tests/test_cli/test_workflow_output_handling.py

**PR Description**

## Summary

Unifies validation behavior between normal workflow execution and `--validate-only` mode. Previously, normal execution used weaker schema-only validation while `--validate-only` used full `WorkflowValidator`. This caused unknown node types to show ugly tracebacks instead of clean error messages.

## Changes

- Add `_validate_before_execution()` function using full `WorkflowValidator` with real execution params
- Remove `_validate_workflow_structure()` and `_validate_and_handle_workflow_errors()` (dead code)
- Remove unused `validate_ir` import from `main.py`
- Add validation call after `_prepare_execution_environment()` but before `execute_workflow()`
- Update test fixtures to patch `WorkflowValidator.validate` instead of removed `validate_ir`
- Add new test file `test_validation_before_execution.py` with 5 tests

## Explanation

The CLI had two different validation paths:

| Mode | Before | After |
|------|--------|-------|
| Normal execution | `validate_ir()` (schema only) | `WorkflowValidator.validate()` (full) |
| `--validate-only` | `WorkflowValidator.validate()` (full) | `WorkflowValidator.validate()` (full) |
| `--auto-repair` | `WorkflowValidator.validate()` (full) | `WorkflowValidator.validate()` (full) |

The key insight was that validation should happen AFTER `_prepare_execution_environment()` creates `enhanced_params`, so we can validate with real params instead of dummy placeholders. This provides better template validation than `--validate-only` mode.

**Before (ugly traceback):**
```
ERROR: Node instantiation failed
Traceback (most recent call last):
  ...
pflow.runtime.compiler.CompilationError: compiler: Node type 'nonexistent-node' not found
...
❌ Planning failed: compiler: Node type 'nonexistent-node' not found
```

**After (clean error):**
```
✗ Static validation failed:
  • Unknown node type: 'nonexistent-node'
```

## Testing

- All 3985 tests pass
- `make check` passes (linting, type checking)
- New test file verifies:
  - Unknown nodes are caught before any node executes
  - Clean error messages (no tracebacks)
  - JSON format works correctly
  - Valid workflows still execute normally
  - Validation is consistent between `--validate-only` and normal execution

Run `make test` to verify all tests pass.

---
🤖 Implemented by Claude

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [7/7] Fixed template validation error when using nested templates containing dots inside array brackets (e.g., `${item.field}`)

*Commit: `e87bcab` fix: nested template validation for ${item.field} in array brackets

The validator used template.split(".") which incorrectly split dots inside
nested templates like ${item.draft_index}, causing validation to fail with:
"Node 'drafts' does not output 'results[${item'"

Added _split_template_path() helper that preserves dots inside ${...} blocks.

Fixes follow-up bug from #65 (dynamic batch indexing).*
Files: src/pflow/runtime/template_validator.py, tests/test_runtime/test_nested_templates.py, tests/test_runtime/test_template_validator.py