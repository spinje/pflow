# v1.0.0 Release Context

Generated: 2026-01-04
This file contains implementation context for AI agents and release verification.

---

## Changelog

## v1.0.0 (2026-01-04)

- Removed parameter fallback pattern from all nodes to ensure explicit data wiring and resolve namespace collisions [#28](https://github.com/spinje/pflow/pull/28)
- Removed the `context` parameter from the Claude Code node [#10](https://github.com/spinje/pflow/pull/10)
- Migrated the Claude Code node to the Claude Agent SDK and renamed parameters `task` to `prompt` and `working_directory` to `cwd` [#10](https://github.com/spinje/pflow/pull/10)
- Changed template resolution to preserve original types (dict, list, int, etc.) for simple `${var}` templates, resolving double-serialization issues in nested objects [#32](https://github.com/spinje/pflow/pull/32)
- Added batch processing capability with support for sequential and parallel execution, isolated item contexts, and thread-safe retries [#11](https://github.com/spinje/pflow/pull/11)
- Added automatic JSON parsing when traversing paths in templates (e.g., `${node.stdout.field}`) or when using JSON strings in inline object templates [#37](https://github.com/spinje/pflow/pull/37), [#39](https://github.com/spinje/pflow/pull/39)
- Added real-time progress status display for batch node execution [#20](https://github.com/spinje/pflow/pull/20)
- Added `sandbox` configuration, `timeout`, and session `resume` parameters to the Claude Code node, and enabled all tools by default [#10](https://github.com/spinje/pflow/pull/10)
- Added a release context file and CLI summary to the `generate-changelog` workflow
- Added support for tag and branch references in `git-log` node `since` and `until` parameters
- Fixed a critical SIGPIPE issue that caused processes to terminate with exit 141 when a subprocess ignored large stdin data [#26](https://github.com/spinje/pflow/pull/26)
- Fixed shell node validation by moving dict/list checks to compile time and providing descriptive fix suggestions [#30](https://github.com/spinje/pflow/pull/30)
- Fixed tracking of LLM usage and costs to correctly accumulate data from inner executions within batch nodes [#22](https://github.com/spinje/pflow/pull/22)
- Fixed template variables to allow arrays and dictionaries to auto-serialize to JSON when used in string contexts like shell commands [#19](https://github.com/spinje/pflow/pull/19)
- Fixed batch template validation to correctly recognize dotted references like `${item.field}` and structured batch outputs
- Fixed planner behavior to gracefully handle missing file paths during requirements analysis

---

## Skipped Changes (Verification)

Review these to ensure nothing was incorrectly classified as internal:

- docs: improve agent instructions with consistent extraction/transformation guidance
- docs: update CLAUDE.md to enhance pflow usage instructions
- docs: streamline User Decisions section in CLAUDE.md
- docs: clarify that agents must read instruction parts in full
- docs: add Task 108 - Smart Trace Debug Output for Agent Iteration
- docs: add agent steering principles to CLAUDE.md
- docs: update create-pr.md to include task section
- docs: sync CLAUDE.md task list with actual state
- docs: add shell command development example to Task 106 handover
- docs: add --run-node as future enhancement for Task 106
- docs: add comprehensive format exploration for Task 107
- feat: add Task 107 - Markdown Workflow Format
- refactor: improve create-task and braindump command prompts
- feat: add Task 106 - Automatic Workflow Iteration Cache
- feat: add generate-changelog example workflow
- forgot to add task review to task 105
- docs: clarify batch results contain inner node outputs
- docs: add 'No Parallel Paths' to workflow limitations
- more simple real workflow
- fix: improve rm command safety checks in pre_tool_use.py
- docs: update performance metrics and vision cost breakdown in README
- docs: add LLM capture documentation and interleaved batch test
- chore: rename GitHub repo references from anthropics to spinje
- remove statusline from project
- cleanup
- fix: eliminate non-LLM skipped tests by fixing or removing them
- fix: add example IR validation test and fix schema compliance issues
- fix: implement skipped test_missing_variable_keeps_template test
- fix: update test to use existing echo node instead of non-existent test-node
- fix: resolve race condition in SettingsManager causing intermittent test failures
- test
- chore: add frontmatter to commands and simplify read_context logic
- add note to task 99

---

## Task Implementation Reviews

### Task 96

# Task 96 Review: Support Batch Processing in Workflows

## Metadata

- **Implementation Date**: 2024-12-23 to 2024-12-27
- **Branch**: `feat/batch-processing`
- **Total Tests Added**: 140 batch-related tests
- **Files Changed**: 12 files (6 core, 6 test)

## Executive Summary

Task 96 implemented both sequential and parallel batch processing for pflow workflows, enabling a single node to process multiple items from an array with isolated execution contexts. The implementation required significant architectural discoveries—most critically, changing from `BatchNode` to `Node` inheritance to avoid thread-safety issues, and establishing the deep copy pattern for thread isolation that Task 39 will reuse.

## Implementation Overview

### What Was Built

1. **Sequential Batch Processing (Phase 1)**: Process items one at a time with isolated `dict(shared)` context per item
2. **Parallel Batch Processing (Phase 2)**: Concurrent execution using `ThreadPoolExecutor` with deep-copied node chains per thread
3. **Batch Metadata**: Rich timing stats and execution mode info that auto-flows into workflow traces
4. **Batch Error Display**: CLI and MCP display enhancements showing "8/10 items succeeded, 2 failed" summaries

### Deviations from Original Spec

| Spec Said | Actually Built | Reason |
|-----------|---------------|--------|
| Inherit from `BatchNode` | Inherit from `Node` directly | `BatchNode` uses `self.cur_retry` which races in parallel |
| Use MRO trick for retry | Local `retry` variable | Thread-safe, clearer code |
| Phase 2 "later" | Implemented in same PR | Synergy with Task 39, context was fresh |

### Implementation Approach

The wrapper chain position was the critical architectural decision. Batch wraps OUTSIDE namespace:

```
Instrumented._run(shared)
  → PflowBatchNode._run(shared)           # Injects ${item} at ROOT level
    → NamespacedNodeWrapper._run(shared)  # Creates proxy
      → TemplateAwareNodeWrapper._run(proxy)  # Resolves ${item}
        → ActualNode._run(proxy)
```

If batch were inside namespace, `shared["item"] = x` would write to `shared["node_id"]["item"]`, and template resolution for `${item}` would fail.

## Files Modified/Created

### Core Changes

| File | Purpose |
|------|---------|
| `src/pflow/core/ir_schema.py` | Added `BATCH_CONFIG_SCHEMA` with 7 fields: `items`, `as`, `error_handling`, `parallel`, `max_concurrent`, `max_retries`, `retry_wait` |
| `src/pflow/runtime/batch_node.py` | **NEW** - 500+ line `PflowBatchNode` class with sequential and parallel execution |
| `src/pflow/runtime/compiler.py` | Integrated batch wrapper at line ~680, between Namespace and Instrumented |
| `src/pflow/execution/execution_state.py` | Added batch detection via `batch_metadata` key |
| `src/pflow/execution/formatters/success_formatter.py` | Added `_format_batch_node_line()` and `_format_batch_errors_section()` |
| `src/pflow/cli/main.py` | Updated `_format_node_status_line()` and added `_display_batch_errors()` |

### Test Files

| File | Tests | Critical Tests |
|------|-------|----------------|
| `tests/test_core/test_ir_schema.py` | 26 | `test_batch_config_items_not_template` - validates template pattern |
| `tests/test_runtime/test_batch_node.py` | 83 | `test_parallel_retry_resets_namespace` - regression test for namespace reset bug |
| `tests/test_runtime/test_compiler_batch.py` | 15 | `test_batch_node_in_wrapper_chain` - validates wrapper order |
| `tests/test_execution/formatters/test_success_formatter.py` | 23 | `test_batch_errors_capped_at_5` - prevents overwhelming output |

## Integration Points & Dependencies

### Incoming Dependencies

| Component | Depends On | Via |
|-----------|-----------|-----|
| Workflow Compiler | `PflowBatchNode` | `compile_ir_to_flow()` creates batch wrapper when `batch` config present |
| Trace System | Batch output | `batch_metadata` key captured in `shared_after` |
| CLI Display | Batch step fields | `is_batch`, `batch_total`, `batch_failed` in execution steps |
| MCP Display | Batch step fields | Same fields via `format_success_as_text()` |

### Outgoing Dependencies

| This Task | Depends On | Via |
|-----------|-----------|-----|
| Template Resolution | `TemplateResolver.resolve_value()` | Resolves `${node.items}` to actual array |
| Namespace Store | `NamespacedSharedStore.keys()` | Returns union of namespace + root keys (critical for `${item}` visibility) |
| Instrumented Wrapper | `InstrumentedNodeWrapper` | Wraps batch for metrics/tracing |

### Shared Store Keys

| Key Pattern | Purpose | Data Structure |
|-------------|---------|----------------|
| `shared[node_id].results` | Array of per-item results | `list[dict]` |
| `shared[node_id].batch_metadata` | Execution metadata | `{"parallel": bool, "timing": {...}, ...}` |
| `shared[node_id].errors` | Error details | `list[{"index": int, "item": Any, "error": str}]` or `None` |
| `shared[item_alias]` | Current item during iteration | `Any` (injected at root level) |

## Architectural Decisions & Tradeoffs

### Key Decisions

| Decision | Reasoning | Alternative Rejected |
|----------|-----------|---------------------|
| Inherit from `Node` not `BatchNode` | `BatchNode` uses `self.cur_retry` instance state that races in parallel | MRO trick was clever but obscure and not thread-safe |
| Deep copy node chain per thread | `TemplateAwareNodeWrapper` mutates `inner_node.params`—races without isolation | Threading lock would serialize execution, defeating parallelism |
| Shallow copy shared store | `__llm_calls__` list needs to accumulate across all items (GIL-protected) | Deep copy would isolate tracking, breaking metrics |
| Batch outside namespace wrapper | Item alias must be at root level for `${item}` to resolve | Inside would write to `shared["node_id"]["item"]` |
| No shared concurrency module | ~100 lines inline, Task 39 follows same pattern | Premature abstraction; can extract later if needed |

### Technical Debt Incurred

| Debt | Reason | Future Fix |
|------|--------|------------|
| `exec()` raises `NotImplementedError` | Inherited from Node but never called (we override `_exec`) | Document clearly, or restructure inheritance |
| `# noqa: C901` on `_collect_parallel_results` | Concurrent result collection is inherently complex | Accept complexity, document well |

## Testing Implementation

### Test Strategy Applied

1. **Unit tests first**: `PflowBatchNode` in isolation with mock inner nodes
2. **Integration tests**: Through compiler with real node types (ValueNode, ShellNode)
3. **Thread safety tests**: Verify isolation with concurrent execution
4. **Regression tests**: Specific tests for bugs found during review

### Critical Test Cases

| Test | What It Catches |
|------|-----------------|
| `test_parallel_retry_resets_namespace` | Namespace pollution between retries in parallel mode |
| `test_batch_with_template_resolution` | `set_params()` forwarding to inner node chain |
| `test_batch_node_in_wrapper_chain` | Correct wrapper order (Batch outside Namespace) |
| `test_parallel_preserves_order` | Result ordering despite completion order |
| `test_llm_calls_accumulated_correctly` | `__llm_calls__` shared via shallow copy |

## Unexpected Discoveries

### Gotchas Encountered

1. **`set_params()` doesn't forward by default**: `BaseNode.set_params()` only sets `self.params`. Batch must override to forward to inner node, or `TemplateAwareNodeWrapper` never sees templates.

2. **NamespacedSharedStore.keys() returns UNION**: The proxy returns both namespace keys AND root keys. This is WHY `${item}` works—template resolution sees root-level keys through the proxy.

3. **Shell node outputs strings, not arrays**: `${shell.stdout}` is a string like `'["a","b","c"]\n'`. For batch processing, use HTTP node (parses JSON) or other array-producing nodes.

4. **Dual display paths**: CLI has `_display_execution_summary()` in `main.py`, MCP uses `format_success_as_text()` in `success_formatter.py`. BOTH needed updating for batch display.

### Edge Cases Found

| Edge Case | Handling |
|-----------|----------|
| Empty items array | Returns empty results, no errors |
| Single item | Works correctly in both modes |
| `max_concurrent=1` | Effectively sequential (useful for debugging) |
| Item is `0` or `""` (falsy) | Use `if key in shared` not `shared.get(key) or ...` |
| Partial writes before failure | Namespace reset on each retry |

## Patterns Established

### Reusable Patterns

**1. Deep Copy for Thread Isolation**
```python
def process_item(idx, item):
    thread_node = copy.deepcopy(self.inner_node)  # Thread-local copy
    return thread_node._run(item_shared)
```
Task 39's `ParallelGroupNode` will use this exact pattern.

**2. Thread-Safe Retry with Local Variable**
```python
for retry in range(self.max_retries):  # Local, not self.cur_retry
    try:
        result = self._execute(item)
        return result
    except Exception as e:
        if retry < self.max_retries - 1:
            time.sleep(self.retry_wait)
            continue
        raise
```

**3. Shallow Copy for Shared Mutable Tracking**
```python
item_shared = dict(shared)  # Shallow: shares __llm_calls__ list
item_shared[self.node_id] = {}  # Fresh namespace per item
item_shared[self.item_alias] = item  # Inject at root
```

**4. Batch Detection via Metadata Key**
```python
if isinstance(node_output, dict) and "batch_metadata" in node_output:
    # It's a batch node
```

### Anti-Patterns to Avoid

| Anti-Pattern | Why It Fails |
|--------------|--------------|
| `shared.get("item") or shared.get("file")` | Fails when item is `0` or `""` (falsy) |
| Deep copy of shared store | Breaks `__llm_calls__` accumulation |
| Batch inside namespace wrapper | `${item}` won't resolve (wrong level) |
| `self.cur_retry` for parallel retry | Races between threads |

## Breaking Changes

### API/Interface Changes

| Change | Impact |
|--------|--------|
| New `batch` property on nodes | Additive, no breaking change |
| New output keys (`batch_metadata`, etc.) | Additive, existing code ignores |

### Behavioral Changes

| Change | Impact |
|--------|--------|
| Batch nodes show "8/10 items succeeded" | Improved UX, no breaking change |
| Trace files include batch timing | More data in traces, no breaking change |

## Future Considerations

### Extension Points

1. **Task 39 Integration**: `ParallelGroupNode` should use same deep copy pattern, same `max_concurrent` config
2. **Async Batch**: Replace `ThreadPoolExecutor` with `asyncio.gather` + `to_thread()` for async nodes
3. **Nested Batch**: Batch node containing another batch node (not tested, may work)

### Scalability Concerns

| Concern | Mitigation |
|---------|------------|
| Deep copy overhead for large node chains | Measured at ~100μs vs ~2000ms for LLM calls (0.05%) |
| Memory with many concurrent items | `max_concurrent` defaults to 10, caps at 100 |
| Error list size with many failures | Display capped at 5 errors |

## AI Agent Guidance

### Quick Start for Related Tasks

**For Task 39 (Parallel Nodes)**:
1. Read `batch_node.py` lines 420-480 (`_exec_parallel`)
2. Copy the deep copy pattern exactly
3. Your `ParallelGroupNode` is ~20 lines using established patterns

**For any wrapper chain work**:
1. Read `compiler.py` lines 650-700 to understand wrapper order
2. Batch is OUTSIDE namespace—this is critical and non-negotiable

**For template resolution issues**:
1. Check if `set_params()` forwards to inner node
2. Check if item alias is injected at ROOT level (not in namespace)
3. Verify `NamespacedSharedStore.keys()` behavior

### Common Pitfalls

| Pitfall | Solution |
|---------|----------|
| Template returns `None` for `${item}` | Add `set_params()` forwarding to inner node |
| Parallel tests pass locally, fail in CI | Use `dict(shared)` shallow copy, not deep copy |
| Retry counter shared between items | Use local `retry` variable, not `self.cur_retry` |
| Only CLI or only MCP shows batch info | Update BOTH display paths |

### Test-First Recommendations

When modifying batch processing:
1. Run `test_batch_node_in_wrapper_chain` first (validates architecture)
2. Run `test_parallel_retry_resets_namespace` (catches context pollution)
3. Run full batch suite: `pytest tests/test_runtime/test_batch_node.py -v`

When modifying display:
1. Run `test_batch_errors_capped_at_5` (validates truncation)
2. Run `pytest tests/test_execution/formatters/test_success_formatter.py -v`

---

## Appendix: Key Code Locations

| Concept | File | Line Range |
|---------|------|------------|
| Batch schema | `ir_schema.py` | 117-175 |
| Wrapper chain insertion | `compiler.py` | 671-689 |
| Sequential execution | `batch_node.py` | 170-200 |
| Parallel execution | `batch_node.py` | 420-480 |
| Thread-safe retry | `batch_node.py` | 200-260 |
| Batch detection | `execution_state.py` | 85-100 |
| CLI batch display | `main.py` | `_format_node_status_line()` |
| MCP batch display | `success_formatter.py` | `_format_batch_node_line()` |

---

*Generated from implementation context of Task 96*

### Task 102

# Task 102 Review: Remove Parameter Fallback Pattern

## Metadata
- **Implementation Date**: 2024-12-30
- **Branch**: `fix/namespace-collision`
- **Related Tasks**: Task 9 (Namespacing), Task 11 (First file nodes where pattern originated)

## Executive Summary

Removed the "shared store fallback" pattern from all 20 platform nodes (~60 parameters) that caused silent failures when node IDs or workflow inputs matched parameter names. The fix aligns pflow with PocketFlow's original design philosophy where params and shared store are separate channels, with templates serving as the explicit data wiring mechanism.

## Implementation Overview

### What Was Built

**Original spec proposed 4 options:**
1. Filter namespaces from visibility (heuristic-based)
2. Invert priority (params first, then shared)
3. Add collision detection (compile-time errors)
4. Explicit namespace prefix (breaking change)

**What was actually implemented: None of the above.**

Through deep investigation, we discovered the root cause was the fallback pattern itself. The solution was simpler and more correct:
- **Remove the fallback entirely** - nodes read only from `self.params`
- **Templates handle all wiring** - `${var}` resolves from shared store into params
- **Collision detection unnecessary** - with params-only, naming collisions simply don't matter

### Implementation Approach

1. **Investigation Phase**: Launched 6 parallel subagents to understand:
   - NamespacedSharedStore implementation
   - All nodes using fallback pattern
   - Template resolution flow
   - Workflow input handling
   - PocketFlow's original design

2. **Key Discovery**: The fallback pattern was introduced in Task 11 (file nodes) with NO documented rationale. It predates templates and conflicts with namespacing (Task 9).

3. **Execution**: Parallel subagents for mechanical changes, manual review for complex parts.

## Files Modified/Created

### Core Changes

**Node Implementations (20 files)**
| Path | Parameters Changed |
|------|-------------------|
| `src/pflow/nodes/file/read_file.py` | file_path, encoding |
| `src/pflow/nodes/file/write_file.py` | content, file_path, encoding, content_is_binary |
| `src/pflow/nodes/file/copy_file.py` | source_path, dest_path |
| `src/pflow/nodes/file/move_file.py` | source_path, dest_path |
| `src/pflow/nodes/file/delete_file.py` | file_path |
| `src/pflow/nodes/git/*.py` (6 files) | branch, remote, working_directory, message, files, etc. |
| `src/pflow/nodes/github/*.py` (4 files) | repo, issue_number, title, body, head, base, state, limit, etc. |
| `src/pflow/nodes/http/http.py` | url, method, body, headers, params, timeout, auth_token, api_key |
| `src/pflow/nodes/llm/llm.py` | prompt, system, images |
| `src/pflow/nodes/shell/shell.py` | stdin |
| `src/pflow/nodes/claude/claude_code.py` | prompt, output_schema |
| `src/pflow/nodes/test/echo.py` | message, count, data |

**Interface Docstrings (23 files)**
Changed `- Reads: shared["key"]` to `- Params: key` format to accurately reflect new behavior.

**Documentation (9 files)**
- `src/pflow/nodes/CLAUDE.md` - New "Parameter-Only Pattern" section
- `architecture/core-concepts/shared-store.md` - Updated precedence rules
- `.taskmaster/knowledge/decisions.md` - Added architectural decision record

### Test Files

**New File Created:**
- `tests/test_runtime/test_namespace_collision_regression.py` - 15 critical regression tests

**Files Updated (~17):**
- Tests that put data in `shared` expecting nodes to read it
- Tests explicitly verifying "shared-first" behavior
- Integration tests across file/git/github/http/llm/shell/claude nodes

## Integration Points & Dependencies

### Incoming Dependencies
- **CLI** → Nodes via `compile_ir_to_flow()` - Passes initial_params which become template context
- **Planner** → Nodes via IR generation - Must generate templates for data flow
- **Template System** → Nodes via `TemplateAwareNodeWrapper` - Resolves `${var}` into params

### Outgoing Dependencies
- **Nodes** → `self.params` only - No more shared store reads for declared parameters
- **Nodes** → Shared store for outputs only - Write via `shared["key"] = value`

### Critical Coupling
**Template resolution MUST happen before node execution.**

The wrapper chain order is critical:
```
TemplateAwareNodeWrapper (resolves ${var} into params)
  → NamespacedNodeWrapper (isolates writes)
    → InstrumentedNodeWrapper (metrics/tracing)
      → ActualNode (reads from self.params)
```

## Architectural Decisions & Tradeoffs

### Key Decisions

| Decision | Reasoning | Alternative Rejected |
|----------|-----------|---------------------|
| Remove fallback entirely | Aligns with PocketFlow philosophy, templates handle wiring | Filter namespaces (heuristic-based, band-aid) |
| Params-only pattern | Explicit > implicit, no magic based on naming | Invert priority (still implicit fallback) |
| No collision detection | Unnecessary with params-only design | Would add complexity for no benefit |
| Update Interface docstrings | `Reads: shared["x"]` was now misleading | Leave as-is (confusing for future agents) |

### Technical Debt Incurred

None. This task reduced technical debt by:
1. Removing undocumented pattern that conflicted with namespacing
2. Aligning with PocketFlow's design philosophy
3. Making data flow explicit through templates

## Testing Implementation

### Test Strategy Applied

1. **Regression tests first**: Created tests for exact bugs before implementation
2. **Behavioral tests**: Verified new semantics (static params win, falsy values preserved)
3. **Integration verification**: Full test suite must pass

### Critical Test Cases

| Test | What It Catches |
|------|-----------------|
| `test_node_named_images_does_not_collide_with_llm_images_param` | The exact LLM + namespace dict bug |
| `test_input_named_url_does_not_override_http_url_template` | The exact HTTP + raw input bug |
| `test_static_url_param_not_overridden_by_input` | Static params respected (new behavior) |
| `test_falsy_values_not_overridden_by_shared_store` | `0`, `False`, `""` preserved (behavioral change) |

## Unexpected Discoveries

### Gotchas Encountered

1. **Interface docstrings were misleading** - Discovered mid-implementation that `- Reads: shared["key"]` format was now wrong. Required updating 23 files.

2. **More tests than expected** - Initial grep found ~3 test files. Actual count was ~17 because many integration tests put data in `shared` expecting nodes to read it.

3. **Three pattern variants existed**:
   - `shared.get("x") or params.get("x")` (most common)
   - `shared.get("x") if "x" in shared else params.get("x")` (HTTP, LLM)
   - `if "x" in shared: ... elif "x" in params: ...` (write_file content)

### Edge Cases Found

1. **Falsy values**: Old `or` pattern fell through on `0`, `False`, `""`. New pattern preserves them.
2. **None values**: `self.params.get("x")` returns `None` if not set, which is correct behavior.
3. **Required params**: Nodes should use `self.params["x"]` (KeyError) or explicit check for required params.

## Patterns Established

### Reusable Patterns

**The Params-Only Pattern (MANDATORY for all nodes):**
```python
# ✅ CORRECT - Read from params only
url = self.params.get("url")
timeout = self.params.get("timeout", 30)  # With default

# ❌ WRONG - Never use fallback
url = shared.get("url") or self.params.get("url")
```

**Template Wiring in IR:**
```json
{
  "inputs": {"api_url": {"type": "string"}},
  "nodes": [{
    "type": "http",
    "params": {"url": "${api_url}"}  // Explicit wiring via template
  }]
}
```

**Interface Docstring Format:**
```python
"""
Interface:
- Params: url: str  # API endpoint (required)
- Params: timeout: int  # Request timeout in seconds (default: 30)
- Writes: shared["response"]: str  # HTTP response body
"""
```

### Anti-Patterns to Avoid

1. **Never add shared store fallback** - The bug will return
2. **Never assume naming = wiring** - Use explicit templates
3. **Never use `Reads: shared["x"]` in docstrings** - Use `Params: x` format

## Breaking Changes

### Behavioral Changes

| Before | After |
|--------|-------|
| `shared["url"]` overrides `params["url"]` | `params["url"]` always used |
| Falsy values (`0`, `False`, `""`) fall through | Falsy values preserved |
| Naming collision = silent bug | Naming collision = no effect |

### No API Changes
External interfaces unchanged. Internal node behavior fixed.

## Future Considerations

### Extension Points

When creating new nodes:
1. Use `self.params.get("x")` pattern exclusively
2. Use `- Params:` format in Interface docstrings
3. Write to shared store for outputs only

### Potential Improvements

1. **Linter rule**: Detect `shared.get(...) or self.params.get(...)` pattern
2. **Template validation**: Could warn if param name matches node ID (educational, not blocking)

## AI Agent Guidance

### Quick Start for Related Tasks

**When implementing a new node:**
1. Read `src/pflow/nodes/CLAUDE.md` first (updated with params-only pattern)
2. Use `src/pflow/nodes/http/http.py` as reference implementation
3. Never read from shared store for declared parameters

**When debugging "wrong value" issues:**
1. Check if node ID matches a parameter name → Not a problem anymore
2. Check if workflow input matches a parameter name → Not a problem anymore
3. Check template syntax → `${var}` must be correctly formed

**Key files to understand the system:**
- `src/pflow/runtime/node_wrapper.py` - Template resolution
- `src/pflow/runtime/namespaced_store.py` - Namespace isolation
- `tests/test_runtime/test_namespace_collision_regression.py` - Critical regression tests

### Common Pitfalls

1. **Don't copy old node code blindly** - It might have the old fallback pattern
2. **Don't assume `shared["param"]` works** - Use templates in IR
3. **Don't forget Interface docstrings** - They must say `Params:` not `Reads: shared[...]`

### Test-First Recommendations

When modifying any node:
1. Run `pytest tests/test_runtime/test_namespace_collision_regression.py` first
2. Run the specific node's test file
3. Run full suite before committing

---

*Generated from implementation context of Task 102*

### Task 103

# Task 103 Review: Preserve Inline Object Type in Template Resolution

## Metadata

- **Implementation Date**: 2026-01-01
- **Branch**: `feat/inline-object-type`
- **Pull Request**: #32

## Executive Summary

Fixed the double-serialization bug where `{"key": "${dict_var}"}` produced `{"key": "{\"nested\": \"value\"}"}` instead of `{"key": {"nested": "value"}}`. Renamed `resolve_string` to `resolve_template` with type-preserving behavior for simple templates (`${var}`), while maintaining string interpolation for complex templates (`Hello ${name}`). This fix benefits ALL nodes automatically through the wrapper chain.

## Implementation Overview

### What Was Built

1. **Type-preserving template resolution**: Simple templates (`${var}`) now return the original value type (dict, list, int, bool, None). Complex templates (`"Hello ${name}"`) still return strings.

2. **Helper methods**: Added `is_simple_template()` and `extract_simple_template_var()` to TemplateResolver for reuse.

3. **Shared pattern constant**: Extracted `_VAR_NAME_PATTERN` to ensure consistency between `TEMPLATE_PATTERN` and `SIMPLE_TEMPLATE_PATTERN`.

4. **Refactored node_wrapper**: Uses shared helper instead of duplicating regex.

### Deviation from Original Spec

The spec suggested fixing only `resolve_nested()`. We chose a more comprehensive approach:
- Renamed `resolve_string` → `resolve_template` for API clarity
- Fixed `workflow_executor.py` too (nested workflow param mapping had same bug)
- Added strict pattern validation (PR review feedback)

## Files Modified/Created

### Core Changes

| File | Change |
|------|--------|
| `src/pflow/runtime/template_resolver.py` | Added `_VAR_NAME_PATTERN`, `SIMPLE_TEMPLATE_PATTERN`, `is_simple_template()`, `extract_simple_template_var()`. Renamed `resolve_string` → `resolve_template` with type preservation. Updated `resolve_nested` to call new method. |
| `src/pflow/runtime/node_wrapper.py` | `_resolve_simple_template()` now uses `TemplateResolver.extract_simple_template_var()`. Changed call from `resolve_string` to `resolve_template`. |
| `src/pflow/runtime/workflow_executor.py` | Changed `resolve_string` → `resolve_template` for nested workflow param mapping. |

### Test Files

| File | Change | Priority |
|------|--------|----------|
| `tests/test_runtime/test_template_type_preservation.py` | **NEW** - 13 tests for type preservation behavior | Critical |
| `tests/test_integration/test_shell_stdin_type_preservation.py` | **NEW** - 4 integration tests for shell stdin | Critical |
| `tests/test_runtime/test_template_resolver.py` | Updated 22 calls to `resolve_template` | High |
| `tests/test_runtime/test_node_wrapper_nested_resolution.py` | Fixed 2 assertions expecting old stringified behavior | High |
| `tests/test_nodes/test_mcp/test_json_text_parsing.py` | Rewrote tests for new type-preserving behavior | Medium |
| `tests/test_runtime/test_template_resolver_arrays.py` | Updated 6 calls | Low |
| `tests/test_runtime/test_template_resolver_nested.py` | Updated 2 calls | Low |
| `tests/test_runtime/test_template_array_notation.py` | Updated 1 call | Low |

## Integration Points & Dependencies

### Incoming Dependencies

All nodes benefit automatically:
```
TemplateAwareNodeWrapper
    └── calls TemplateResolver.resolve_nested()
        └── calls TemplateResolver.resolve_template()
            └── Type preserved for simple templates
```

Specific nodes affected:
- **Shell node**: `stdin` can now be `{"a": "${data-a}", "b": "${data-b}"}` without double-encoding
- **HTTP node**: `body`, `headers`, `params` preserve types
- **MCP node**: Dynamic tool arguments preserve types
- **LLM node**: Structured params preserve types
- **All other nodes**: Same benefit through wrapper chain

### Outgoing Dependencies

```
resolve_template() depends on:
├── extract_simple_template_var() - Pattern detection
├── variable_exists() - Check if var is in context
├── resolve_value() - Get the actual value (any type)
└── _convert_to_string() - Only for complex templates
```

### Shared Store Keys

No new shared store keys. Template resolution reads from:
- `shared` store (runtime data)
- `initial_params` (planner parameters)

## Architectural Decisions & Tradeoffs

### Key Decisions

| Decision | Reasoning | Alternative Considered |
|----------|-----------|----------------------|
| Rename `resolve_string` → `resolve_template` | Name should reflect return type (`Any`, not `str`) | Keep name, change behavior (confusing API) |
| Delete `resolve_string` entirely | No backward compat needed per user | Keep as deprecated alias (adds noise) |
| Extract `_VAR_NAME_PATTERN` | Single source of truth for valid variable pattern | Duplicate regex in both patterns (drift risk) |
| Fix in `TemplateResolver`, not `node_wrapper` | Fixes all callers automatically, single location | Fix in each caller (scattered, error-prone) |

### Technical Debt Incurred

None significant. The implementation is cleaner than before:
- Eliminated duplicate regex in node_wrapper
- Consolidated detection logic in TemplateResolver
- Stricter validation after PR review

## Testing Implementation

### Test Strategy Applied

1. **Unit tests**: Direct testing of `resolve_template()` behavior
2. **Integration tests**: Shell node stdin with real workflow execution
3. **Edge case tests**: Pattern boundary (simple vs complex vs invalid)
4. **Regression tests**: Updated existing tests expecting old behavior

### Critical Test Cases

| Test | What It Validates |
|------|-------------------|
| `test_inline_object_preserves_dict_type` | THE primary bug fix - `{"key": "${dict}"}` preserves inner dict |
| `test_shell_stdin_inline_object_not_double_encoded` | End-to-end with real shell execution |
| `test_complex_template_in_stdin_still_stringifies` | `"Hello ${name}"` still returns string |
| `test_invalid_variable_names_not_simple` | `${123}`, `${ var }` correctly rejected |

## Unexpected Discoveries

### Gotchas Encountered

1. **Tests expecting old behavior**: Two tests in `test_node_wrapper_nested_resolution.py` explicitly tested the broken behavior with comments like "Template resolution converts numbers to strings". Had to update these to expect correct behavior.

2. **Original SIMPLE_TEMPLATE_PATTERN too permissive**: Initial pattern `r"^\$\{([^}]+)\}$"` matched invalid names like `${123}`, `${ var }`. Fixed by extracting shared pattern from TEMPLATE_PATTERN.

3. **workflow_executor.py also affected**: Nested workflow parameter mapping had the same bug. Would have been missed if we only fixed `resolve_nested()`.

### Edge Cases Found

| Edge Case | Behavior |
|-----------|----------|
| `${missing}` (unresolved) | Stays as `${missing}` in output |
| `${data.field}` (path) | Simple template - preserves type |
| `${items[0]}` (array index) | Simple template - preserves type |
| `${a}${b}` (adjacent) | Complex template - returns string |
| ` ${var}` (leading space) | Complex template - returns string |
| `$${var}` (escaped) | Not a template - literal `${var}` |

## Patterns Established

### Reusable Patterns

**Simple vs Complex Template Detection**:
```python
# Simple template pattern - preserves type
SIMPLE_TEMPLATE_PATTERN = re.compile(rf"^\$\{{({_VAR_NAME_PATTERN})\}}$")

# Usage:
if TemplateResolver.is_simple_template(value):
    return TemplateResolver.resolve_value(var_name, context)  # Type preserved
else:
    return TemplateResolver.resolve_string(value, context)  # String interpolation
```

**Shared Pattern Extraction**:
```python
# Extract reusable pattern for DRY
_VAR_NAME_PATTERN = r"[a-zA-Z_][\w-]*(?:(?:\[\d+\])?(?:\.[a-zA-Z_][\w-]*(?:\[\d+\])?)*)?"

# Use in multiple regex
TEMPLATE_PATTERN = re.compile(rf"(?<!\$)\$\{{({_VAR_NAME_PATTERN})\}}")
SIMPLE_TEMPLATE_PATTERN = re.compile(rf"^\$\{{({_VAR_NAME_PATTERN})\}}$")
```

### Anti-Patterns to Avoid

1. **Don't duplicate regex patterns**: If you need to detect simple templates elsewhere, use `TemplateResolver.is_simple_template()`, don't copy the regex.

2. **Don't call `_convert_to_string()` for simple templates**: The whole point is to preserve types. Only use it for complex templates.

3. **Don't assume `resolve_template()` returns string**: It returns `Any` now. Check `is_simple_template()` if you need to know the return type.

## Breaking Changes

### API/Interface Changes

| Change | Migration |
|--------|-----------|
| `resolve_string()` removed | Use `resolve_template()` instead |
| Return type of template resolution | Now `Any` instead of always `str` |

### Behavioral Changes

| Before | After |
|--------|-------|
| `resolve_nested({"key": "${dict}"})` → `{"key": "{\"nested\":\"value\"}"}` | `{"key": {"nested": "value"}}` |
| Simple templates in nested structures stringified | Type preserved |
| `int` in nested template became `"42"` | Stays `42` |
| `bool` in nested template became `"True"` | Stays `True` |

## Future Considerations

### Extension Points

- **Type coercion**: If a node expects string but receives dict from simple template, the JSON auto-parsing in node_wrapper handles this. Could be extended for other coercions.

- **Custom serialization**: If someone needs the old behavior (stringify simple templates), they could add a `force_string=True` parameter to `resolve_template()`.

### Scalability Concerns

None. Template resolution is O(n) with template count, unchanged from before.

## AI Agent Guidance

### Quick Start for Related Tasks

1. **Read first**:
   - `src/pflow/runtime/template_resolver.py:336-434` - The `resolve_template()` method
   - `tests/test_runtime/test_template_type_preservation.py` - All behavior documented in tests

2. **Key insight**: Simple template = entire string is `${var}` → type preserved. Anything else → string.

3. **Pattern to follow**: Use `TemplateResolver.is_simple_template()` and `extract_simple_template_var()` for any new detection logic.

### Common Pitfalls

1. **Don't assume string return**: `resolve_template("${data}", ctx)` might return a dict. Check before string operations.

2. **Pattern consistency**: If you modify `_VAR_NAME_PATTERN`, both `TEMPLATE_PATTERN` and `SIMPLE_TEMPLATE_PATTERN` change. Test both.

3. **Test the boundary**: When adding template features, test both `${var}` (simple) and `"prefix ${var}"` (complex) cases.

### Test-First Recommendations

When modifying template resolution:
1. Run `test_template_type_preservation.py` first - covers core behavior
2. Run `test_shell_stdin_type_preservation.py` - integration test
3. Run full `tests/test_runtime/test_template_*.py` - all template tests

```bash
uv run pytest tests/test_runtime/test_template_type_preservation.py tests/test_integration/test_shell_stdin_type_preservation.py -v
```

---

*Generated from implementation context of Task 103*

### Task 105

# Task 105 Review: Auto-Parse JSON Strings During Nested Template Access

## Metadata
- **Implementation Date**: 2026-01-01
- **Branch**: `feat/auto-parse-json`
- **Pull Request**: https://github.com/spinje/pflow/pull/37

## Executive Summary

Implemented automatic JSON parsing during template path traversal, enabling `${node.stdout.field}` when stdout contains a JSON string. This required coordinating two systems (compile-time validator + runtime resolver) and establishing a shared JSON utility that consolidated 7 duplicate implementations.

## Implementation Overview

### What Was Built

1. **Core Feature**: JSON auto-parsing in `template_resolver.py` during path traversal
2. **Shared Utility**: `json_utils.py` with `try_parse_json()` returning `(success, result)` tuple
3. **Validator Relaxation**: Allow nested access on `str` types with compile-time warning
4. **Error Message Improvements**: JSON-specific hints and better path-aware suggestions

### Key Deviations from Original Plan

1. **Phase 4 was not in original plan** - Discovered validator/resolver mismatch during testing. Had to relax validator to allow `str` nested access.
2. **Error message improvements added post-review** - Code review feedback led to JSON-specific runtime hints and improved suggestion logic.
3. **Removed low-value tests** - Deleted 6 tests, replaced with 1 high-value behavioral test. Tests should verify behavior, not implementation.

## Files Modified/Created

### Core Changes

| File | Change | Impact |
|------|--------|--------|
| `src/pflow/core/json_utils.py` | **NEW** - Shared JSON parsing utility | Eliminates 7 duplicates, single source of truth |
| `src/pflow/runtime/template_resolver.py` | Added `_try_parse_json_for_traversal()` and `_get_dict_value()` helpers | Core feature - parses JSON during path traversal |
| `src/pflow/runtime/template_validator.py` | Added `_check_type_allows_traversal()`, `_strip_array_indices()` | Allows `str` nested access with warning |
| `src/pflow/runtime/node_wrapper.py` | Added `_detect_json_parse_hints()`, improved `_generate_suggestions()` | Better error messages for JSON failures |
| `src/pflow/runtime/batch_node.py` | Use `try_parse_json()` | Consolidation |

### Test Files

| File | Purpose | Critical? |
|------|---------|-----------|
| `tests/test_core/test_json_utils.py` | Utility tests (16 tests) | YES - validates core parsing logic |
| `tests/test_runtime/test_template_resolver_json_parsing.py` | Feature tests (21 tests) | YES - especially `TestValidationConsistency` |
| `tests/test_integration/test_json_nested_access_e2e.py` | E2E workflow tests (9 tests) | YES - proves feature works end-to-end |
| `tests/test_runtime/test_template_validator_warnings.py` | Warning behavior tests (8 tests) | MEDIUM - validates compile-time warnings |

## Integration Points & Dependencies

### Critical Integration: Validator ↔ Resolver

```
Compile Time                          Runtime
     │                                    │
     ▼                                    ▼
┌─────────────────┐              ┌─────────────────┐
│ TemplateValidator│              │ TemplateResolver │
│                 │              │                 │
│ Sees: stdout:str│              │ Sees: stdout=   │
│ Decision: ALLOW │──────────────│ '{"field":"v"}' │
│ (with warning)  │  Must agree  │ Action: PARSE   │
└─────────────────┘              └─────────────────┘
```

**Lesson**: When adding runtime "magic", always check if compile-time validation needs updating. These systems MUST agree on what's valid.

### Shared Store Keys

No new shared store keys. Feature operates on existing node output keys.

### Outgoing Dependencies

- `json_utils.try_parse_json()` is now used by:
  - `template_resolver.py` (path traversal)
  - `node_wrapper.py` (target-side auto-parse)
  - `batch_node.py` (items parsing)

## Architectural Decisions & Tradeoffs

### Decision 1: Two-Value Return Pattern

**Choice**: `try_parse_json()` returns `(success: bool, result: Any)`

**Reasoning**: Distinguishes three cases:
- `(True, {"key": "value"})` - Parsed successfully to dict
- `(True, None)` - Parsed successfully to JSON null
- `(False, "original string")` - Parse failed

**Alternative considered**: Return parsed value or original. Rejected because can't distinguish "parsed to None" from "failed to parse".

### Decision 2: Parse Only During Traversal

**Choice**: `${node.stdout}` returns raw string, `${node.stdout.field}` triggers parsing

**Reasoning**: Backward compatible. Existing workflows expecting raw strings continue to work.

**Alternative considered**: Always parse if valid JSON. Rejected because changes behavior for existing workflows.

### Decision 3: Warning for `str`, Not `any`

**Choice**: Only `str` type generates compile-time warning for nested access

**Reasoning**:
- `dict`/`object`: Trusted structured data, no warning
- `any`: Node author explicitly declared "could be anything", no warning
- `str`: JSON auto-parsing is implicit/surprising, warn user

### Technical Debt Incurred

1. **No caching of parsed JSON** - Same string parsed multiple times if accessed with different paths. Acceptable for MVP (parsing is <1ms vs node execution 100-1000ms).
2. **Display layer doesn't deduplicate warnings** - Multiple templates on same output generate multiple warnings. Could group in future.

## Testing Implementation

### Test Strategy Applied

**Principle**: Test behavior, not implementation. Focus on:
1. Does the feature work? (E2E tests)
2. Do the edge cases behave correctly? (Unit tests)
3. Do the systems agree? (Validation consistency tests)

### Critical Test Cases

| Test | What It Validates | Why Critical |
|------|-------------------|--------------|
| `TestValidationConsistency.test_exists_agrees_with_resolve_for_valid_json` | `variable_exists()` and `resolve_value()` agree | Prevents validator/resolver mismatch |
| `TestJsonNestedAccessE2E.test_shell_json_output_nested_access` | Original feature request scenario | Proves feature works |
| `TestRecursiveJsonParsing.test_nested_json_string_is_parsed_at_each_level` | JSON-in-JSON works | Edge case users will hit |
| `test_distinguishes_parsed_none_from_parse_failure` | Two-value return correctness | API contract |

## Unexpected Discoveries

### Gotcha 1: Validator Blocks Valid Runtime Patterns

**Discovery**: E2E tests failed because validator saw `stdout: str` and rejected nested access.

**Solution**: Relaxed validator to allow `str` nested access with warning.

**Impact**: Created new coordination requirement between validator and resolver.

### Gotcha 2: Array Access on JSON Strings

**Discovery**: `${node.stdout[0].field}` requires special handling in validator.

**Solution**: Added `_strip_array_indices()` to extract base key before validation.

### Gotcha 3: Error Message Suggestions Were Wrong

**Discovery**: For `${mynode.stdout}` (typo), suggestion was `${my-node}` not `${my-node.stdout}`.

**Solution**: Updated `_generate_suggestions()` to preserve full path when suggesting corrections.

## Patterns Established

### Pattern 1: Two-Value Return for Parse Operations

```python
def try_parse_json(value: str) -> tuple[bool, Any]:
    """Return (success, result) to distinguish parse-to-None from failure."""
    try:
        return (True, json.loads(value))
    except (json.JSONDecodeError, ValueError):
        return (False, value)
```

**When to use**: Any parse operation where the result could legitimately be None/empty.

### Pattern 2: Helper Methods for Dual-Path Consistency

```python
# In template_resolver.py
@staticmethod
def _get_dict_value(value: Any, key: str) -> tuple[bool, Any]:
    """Shared logic for resolve_value() and _traverse_path_part()."""
```

**When to use**: When two code paths must behave identically. Extract shared logic.

### Pattern 3: Actionable Warning Messages

```python
reason=(
    f"Nested access on '{output_type}' requires valid JSON at runtime. "
    f"Non-JSON strings cause 'Unresolved variables' error."
)
```

**Structure**: [What's required] + [What happens if violated]

### Anti-Pattern: Testing Implementation Details

**Don't**: Test that a specific function is called with specific arguments.
**Do**: Test that the observable behavior is correct.

Example: Don't test "JSON parsing was attempted". Test "nested access resolves to expected value".

## Breaking Changes

None. Feature is purely additive:
- Previously failing patterns (`${node.stdout.field}` on JSON string) now work
- Previously working patterns unchanged

## AI Agent Guidance

### Quick Start for Related Tasks

1. **Read first**:
   - `src/pflow/runtime/template_resolver.py` - `_try_parse_json_for_traversal()`
   - `src/pflow/runtime/template_validator.py` - `_check_type_allows_traversal()`

2. **Key insight**: Validator and resolver must agree. If you change one, check the other.

3. **Test pattern**: Always include a test that verifies `variable_exists()` agrees with `resolve_value()`.

### Common Pitfalls

1. **Don't forget the validator** - If you add runtime behavior for a type, the validator might block it at compile time.

2. **Two-value return matters** - Don't change `try_parse_json()` to return parsed-or-original. The tuple is intentional.

3. **Preserve full path in errors** - When generating suggestions for `${node.output.field}`, suggest `${other-node.output.field}`, not just `${other-node}`.

### Test-First Recommendations

When modifying template resolution:
1. Run `tests/test_runtime/test_template_resolver_json_parsing.py` - Core feature
2. Run `tests/test_runtime/test_template_validator_warnings.py` - Warning behavior
3. Run `tests/test_integration/test_json_nested_access_e2e.py` - Full flow

---

*Generated from implementation context of Task 105*

---

## Documentation Changes

## docs/reference/nodes/claude-code.mdx
Commits: badf76a fix(claude-code): update default model and fix documentation URLs [skip review],8f84825 feat(claude-code): migrate to Claude Agent SDK and streamline parameters
Changes:
-### Task parameters
+### Prompt parameters
-| `task` | str | Yes | - | Task description (max 10,000 chars) |
-| `context` | str/dict | No | - | Additional context for the task |
+| `prompt` | str | Yes | - | The prompt to send to Claude (max 10,000 chars) |
-| `working_directory` | str | No | Current dir | Project root directory |
+| `cwd` | str | No | Current dir | Working directory for Claude |
-| `allowed_tools` | list | No | `["Read", "Write", "Edit", "Bash"]` | Permitted tools |
+| `allowed_tools` | list | No | All tools | Permitted tools |
+| `timeout` | int | No | `300` | Execution timeout in seconds (30-3600) |
+| `resume` | str | No | - | Session ID to resume a previous conversation |
+| `sandbox` | dict | No | - | Sandbox configuration for command isolation |
+
+### Sandbox configuration
+
+The `sandbox` parameter controls command execution isolation. See the [Claude Agent SDK sandbox documentation](https://platform.claude.com/docs/en/agent-sdk/python#sandbox-configuration) for full details.
+
+| Key | Type | Description |
+|-----|------|-------------|
+| `enabled` | bool | Enable sandbox mode |
+| `autoAllowBashIfSandboxed` | bool | Auto-allow bash when sandboxed |
+| `excludedCommands` | list | Commands that bypass sandbox (e.g., `["docker"]`) |
+| `allowUnsandboxedCommands` | bool | Allow model to request unsandboxed execution |
+| `network` | dict | Network settings (`allowLocalBinding`, `allowUnixSockets`) |
-| `llm_usage` | dict | Token usage metrics |
-  "duration_api_ms": 2800,
-  "num_turns": 2,
-    "task": "Review this code for security issues",
-    "context": "${code.content}",
+    "prompt": "Review this code for security issues: ${code.content}",
-        "task": "Write a Python function to calculate Fibonacci numbers",
+        "prompt": "Write a Python function to calculate Fibonacci numbers",
-        "task": "Review this authentication code",
-        "context": "${read.content}",
+        "prompt": "Review this authentication code: ${read.content}",
-### File modification
+### File modification with sandbox
-        "task": "Refactor the User class to use dataclasses",
-        "working_directory": "/path/to/project",
-        "allowed_tools": ["Read", "Write", "Edit"]
+        "prompt": "Refactor the User class to use dataclasses",
+        "cwd": "/path/to/project",
+        "allowed_tools": ["Read", "Write", "Edit"],
+        "sandbox": {
+          "enabled": true,
+          "autoAllowBashIfSandboxed": true,
+          "excludedCommands": ["docker"]
+        }
+| `Task` | Spawn subagents |
+| `Glob` | Find files by pattern |

## docs/roadmap.mdx
Commits: fc2dd3c documentation updates and cleanup [skip review]
Changes:
+- **Batch processing** — process arrays of items through a single node (sequential or parallel)
-- Parallel execution — run independent nodes concurrently
+- Task parallelism — run independent nodes concurrently (fan-out/fan-in)



---

## Draft Entries with Context

```json
[
  {
    "draft": "Added release context file and CLI summary to the generate-changelog workflow",
    "index": 2,
    "context": {
      "commit_message": "feat(generate-changelog): add release context file and CLI summary",
      "task_number": null,
      "files_changed": "examples/real-workflows/generate-changelog-simple/archive/workflow-simple.json, examples/real-workflows/generate-changelog-simple/prompt.md, examples/real-workflows/generate-changelog/README.md, examples/real-workflows/generate-changelog/prompt.md, examples/real-workflows/generate-changelog/workflow.json",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Fixed automatic parsing of JSON strings in inline object templates [#39](https://github.com/spinje/pflow/pull/39)",
    "index": 16,
    "context": {
      "commit_message": "Merge pull request #39 from spinje/feat/stdin-json-parse",
      "task_number": null,
      "files_changed": "src/pflow/cli/resources/cli-agent-instructions.md, src/pflow/runtime/template_resolver.py, tests/test_integration/test_inline_object_json_parsing_e2e.py, tests/test_runtime/test_template_resolver_inline_object_parsing.py",
      "pr_title": "fix: auto-parse JSON strings in inline object templates",
      "pr_body": "## Summary\n\nFixes #38 - When a simple template like `${node.stdout}` is used inside an inline object, the resolved JSON string is now automatically parsed to a dict/list/primitive.\n\n## Problem\n\nPreviously, combining JSON outputs in inline objects didn't work as expected:\n\n```json\n{\"stdin\": {\"data\": \"${shell.stdout}\"}}\n```\n\nWhere `shell.stdout` is `'{\"items\": [1,2,3]}\\n'` would result in:\n```python\n{\"stdin\": {\"data\": '{\"items\": [1,2,3]}\\n'}}  # String, not dict!\n```\n\nThis meant jq couldn't access `.data.items` because `.data` was a string.\n\n## Solution\n\nModified `resolve_nested()` in `template_resolver.py` to auto-parse JSON strings from simple templates. Now the same pattern produces:\n\n```python\n{\"stdin\": {\"data\": {\"items\": [1, 2, 3]}}}  # Parsed dict!\n```\n\nAll valid JSON types are parsed: objects, arrays, numbers, booleans, strings, and null.\n\n## Escape Hatch\n\nComplex templates remain as strings, providing control when raw JSON is needed:\n- `\"prefix ${var}\"` → stays string\n- `\"${var} \"` → stays string (trailing space)\n- `\"'${var}'\"` → stays string (with quotes)\n\n## Changes\n\n- Modified `resolve_nested()` to auto-parse JSON from simple templates\n- Added 37 unit tests covering all JSON types and edge cases\n- Added 6 E2E integration tests with shell → jq workflows\n\n```\n src/pflow/runtime/template_resolver.py                         |  48 ++-\n tests/test_integration/test_inline_object_json_parsing_e2e.py  | 153 ++++++++\n tests/test_runtime/test_template_resolver_inline_object_parsing.py | 263 +++++++++++++\n 4 files changed, 511 insertions(+), 48 deletions(-)\n```\n\n## Testing\n\n```bash\nmake test   # 3837 passed\nmake check  # All linting/type checks pass\n```",
      "pr_link": "https://github.com/spinje/pflow/pull/39",
      "pr_number": 39
    }
  },
  {
    "draft": "Fixed static validation to correctly recognize batch output structures",
    "index": 19,
    "context": {
      "commit_message": "fix: static validation now understands batch output structure",
      "task_number": null,
      "files_changed": "src/pflow/runtime/template_validator.py, tests/test_runtime/test_template_validator.py",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Added auto-parsing of JSON strings during nested template access [#37](https://github.com/spinje/pflow/pull/37)",
    "index": 20,
    "context": {
      "commit_message": "Merge pull request #37 from spinje/feat/auto-parse-json",
      "task_number": 105,
      "files_changed": ".taskmaster/tasks/task_105/implementation/implementation-plan.md, .taskmaster/tasks/task_105/implementation/progress-log.md, .taskmaster/tasks/task_105/research/feature-request-json-string-nested-access.md, .taskmaster/tasks/task_105/research/findings.md, .taskmaster/tasks/task_105/research/path-traversal-analysis.md ... and 20 more files",
      "pr_title": "feat: auto-parse JSON strings during nested template access",
      "pr_body": "## Summary\n\nEnable `${node.stdout.field}` to work when `stdout` contains a JSON string like `'{\"field\": \"value\"}'`. Previously, path traversal failed because the resolver treated JSON strings as opaque strings.\n\nCloses #36\n\n## Changes\n\n- Add shared `json_utils.py` with `try_parse_json()` utility (two-value return for null vs failure)\n- Update `template_resolver.py` to parse JSON during path traversal\n- Update `template_validator.py` to allow `str` nested access with warning\n- Consolidate 5 duplicate JSON parsing implementations across codebase\n- Add comprehensive tests (10 unit, 21 resolver, 9 E2E)\n\n## Key Design Decisions\n\n1. **Parse only when traversing deeper** - `${node.stdout}` returns raw string, `${node.stdout.field}` triggers parsing\n2. **Validator warns for `str` type** - `dict`/`any` allowed silently, `str` generates warning about JSON auto-parsing\n3. **Two-value return** - `(success, result)` distinguishes \"parsed to None\" from \"failed to parse\"\n\n## Before/After\n\n```\nBefore:  ${node.stdout.iso}  → ❌ Unresolved variable error\nAfter:   ${node.stdout.iso}  → ✅ \"2026-01-01\"\n```\n\n## Created Docs\n\n- `.taskmaster/tasks/task_105/task-105.md`\n- `.taskmaster/tasks/task_105/implementation/progress-log.md`\n- `.taskmaster/tasks/task_105/implementation/implementation-plan.md`\n\n## Testing\n\n```bash\nmake test   # 3661 tests pass\nmake check  # All linting/type checks pass\n```\n\nSpecific feature tests:\n```bash\npytest tests/test_core/test_json_utils.py tests/test_runtime/test_template_resolver_json_parsing.py tests/test_integration/test_json_nested_access_e2e.py -v\n```",
      "pr_link": "https://github.com/spinje/pflow/pull/37",
      "pr_number": 37
    }
  },
  {
    "draft": "git-log node now supports tag and branch references for since and until parameters",
    "index": 21,
    "context": {
      "commit_message": "fix: git-log node now supports tag/branch refs for since/until",
      "task_number": null,
      "files_changed": "src/pflow/nodes/git/log.py, tests/test_nodes/test_git/test_git_log.py",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Preserved inline object types during template resolution [#32](https://github.com/spinje/pflow/pull/32)",
    "index": 23,
    "context": {
      "commit_message": "Merge pull request #32 from spinje/feat/inline-object-type",
      "task_number": 103,
      "files_changed": ".taskmaster/tasks/task_103/implementation/implementation-plan.md, .taskmaster/tasks/task_103/implementation/progress-log.md, .taskmaster/tasks/task_103/task-review.md, architecture/reference/template-variables.md, src/pflow/runtime/CLAUDE.md ... and 11 more files",
      "pr_title": "feat: preserve inline object type in template resolution (fixes #31)",
      "pr_body": "## Summary\n\nFixes the double-serialization bug where templates in nested structures like `{\"key\": \"${dict_var}\"}` would produce `{\"key\": \"{\\\"nested\\\": \\\"value\\\"}\"}` instead of the correct `{\"key\": {\"nested\": \"value\"}}`.\n\nThis enables the intuitive pattern of passing multiple data sources to shell commands:\n\n```json\n{\n  \"stdin\": {\"config\": \"${config}\", \"data\": \"${data}\"},\n  \"command\": \"jq '.config + .data'\"\n}\n```\n\n## Changes\n\n- **New helper methods** in `TemplateResolver`:\n  - `is_simple_template()` - detects if string is exactly `${var}`\n  - `extract_simple_template_var()` - extracts variable name from simple template\n  \n- **Renamed `resolve_string()` → `resolve_template()`** with type preservation:\n  - Simple templates (`${var}`) preserve original type (dict, list, int, bool, None)\n  - Complex templates (`\"Hello ${name}\"`) return interpolated strings\n  \n- **Updated callers**:\n  - `workflow_executor.py` - nested workflow param mapping\n  - `node_wrapper.py` - refactored to use shared helper (DRY)\n\n## Test Coverage\n\n- `test_template_type_preservation.py` - Core type preservation behavior (12 tests)\n- `test_shell_stdin_type_preservation.py` - Integration tests with shell node (4 tests)\n- Updated existing tests to use `resolve_template()` (36 occurrences across 5 files)\n\n## Created Docs\n\n- `.taskmaster/tasks/task_103/implementation/implementation-plan.md`\n- `.taskmaster/tasks/task_103/implementation/progress-log.md`\n\n## File Stats\n\n```\n 13 files changed, 1620 insertions(+), 98 deletions(-)\n```\n\n## Testing\n\n```bash\nmake test  # 3236 passed, 9 skipped\n```\n\n---\nTask-ID: 103",
      "pr_link": "https://github.com/spinje/pflow/pull/32",
      "pr_number": 32
    }
  },
  {
    "draft": "Moved shell dict and list validation to compile time [#30](https://github.com/spinje/pflow/pull/30)",
    "index": 24,
    "context": {
      "commit_message": "Merge pull request #30 from spinje/fix/shell-dict-list-validation",
      "task_number": 103,
      "files_changed": ".taskmaster/knowledge/pitfalls.md, .taskmaster/tasks/task_103/starting-context/task-103-handover.md, .taskmaster/tasks/task_103/task-103.md, .taskmaster/tasks/task_104/starting-context/task-104-handover.md, .taskmaster/tasks/task_104/task-104.md ... and 6 more files",
      "pr_title": "fix: move shell dict/list validation to compile time",
      "pr_body": "## Summary\n\nFixes #29 - Shell node dict/list validation was broken (dead code + false positives).\n\nThe shell node's runtime checks for dict/list in commands had two problems:\n1. `_check_command_template_safety()` ran AFTER template resolution, so `extract_variables()` found nothing (dead code)\n2. `_warn_shell_unsafe_json()` regex `\\[.{0,500}?\\]` matched shell syntax like `[a-z]`, `[0-9]` (false positives)\n\nResult: dict/list in shell commands passed validation, then failed at runtime with cryptic shell syntax errors.\n\n## Changes\n\n- Add `_validate_shell_command_types()` in `template_validator.py` - runs at compile time BEFORE template resolution\n- Remove dead code from `shell.py` (`_check_command_template_safety`, `_warn_shell_unsafe_json`)\n- Add actionable error messages:\n  - Single template: suggests stdin parameter with example\n  - Multiple templates: suggests temp files workaround\n- Add 9 high-value tests covering workflow inputs, nested access, union types, and validation timing\n\n## Error Messages\n\n**Single template:**\n```\nShell node 'process': cannot use ${data} (type: object) in command parameter.\n\nPROBLEM: object data embedded in shell commands breaks parsing...\n\nFIX: Move data to stdin, keep command simple:\n  {\"stdin\": \"${data}\", \"command\": \"jq '.field'\"}\n```\n\n**Multiple templates:**\n```\nShell node 'process': multiple structured data templates in command: ${config} (object), ${data} (object)\n\nPROBLEM: Shell commands can only receive ONE data source via stdin.\n\nFIX OPTIONS:\n1. Use temp files - write each data source to a file, then read in shell\n2. Process each data source in separate shell nodes\n```\n\n## Future Enhancement (Task 103)\n\nThe ideal solution for multiple data sources would be inline object construction:\n```json\n{\"stdin\": {\"config\": \"${config}\", \"data\": \"${data}\"}, \"command\": \"jq '.config + .data'\"}\n```\n\nThis doesn't work yet due to double-serialization (templates inside dicts serialize to JSON strings instead of preserving type). **Task 103** tracks the fix: preserve type when resolving simple templates inside dict/list values.\n\n## Testing\n\n```bash\nmake test  # 3723 passed\nmake check # All checks pass\n```",
      "pr_link": "https://github.com/spinje/pflow/pull/30",
      "pr_number": 30
    }
  },
  {
    "draft": "Removed parameter fallback pattern from all nodes to resolve namespace collisions [#28](https://github.com/spinje/pflow/pull/28)",
    "index": 26,
    "context": {
      "commit_message": "Merge pull request #28 from spinje/fix/namespace-collision",
      "task_number": 102,
      "files_changed": ".taskmaster/knowledge/decisions.md, .taskmaster/knowledge/patterns.md, .taskmaster/tasks/task_102/implementation/implementation-plan.md, .taskmaster/tasks/task_102/implementation/progress-log.md, .taskmaster/tasks/task_102/task-102.md ... and 78 more files",
      "pr_title": "refactor: remove parameter fallback pattern from all nodes (closes #27)",
      "pr_body": "## Summary\n\nRemove the \"shared store fallback\" pattern from all nodes that caused silent failures when node IDs or workflow inputs matched parameter names. This is an architectural refactor that aligns pflow with PocketFlow's original design philosophy.\n\n## Problem\n\nThe fallback pattern `shared.get(\"x\") or params.get(\"x\")` caused two types of collisions:\n\n1. **Node ID Collision**: Node named \"images\" creates `shared[\"images\"] = {stdout: ...}`, LLM node finds this dict instead of template-resolved param\n2. **Workflow Input Collision**: Input named \"url\" creates `shared[\"url\"] = \"value\"`, HTTP node uses raw value instead of template transformation\n\n## Solution\n\n- Nodes now read only from `self.params`\n- Templates (`${var}`) handle all data wiring explicitly\n- No collision detection needed - naming collisions simply don't matter with params-only\n\n## Changes\n\n**Node implementations (20 files, ~60 parameters)**\n- All file, git, github, http, llm, shell, claude, and test nodes updated\n\n**Interface docstrings (23 files)**\n- Changed from `- Reads: shared[\"key\"]` to `- Params: key` format\n\n**Documentation (9 files)**\n- Updated CLAUDE.md files, architecture docs, and reference docs\n\n**Tests (~17 files, ~150 tests)**\n- Updated tests expecting shared-first behavior\n- Added 15 regression tests in `test_namespace_collision_regression.py`\n\n## Stats\n\n```\n83 files changed, 4611 insertions(+), 909 deletions(-)\n```\n\n## Task Documentation\n\n- `.taskmaster/tasks/task_102/task-102.md` - Task specification\n- `.taskmaster/tasks/task_102/task-review.md` - Implementation review\n- `.taskmaster/tasks/task_102/implementation/progress-log.md` - Full timeline\n\n## Testing\n\n```bash\nmake test   # 3646 passed, 119 skipped\nmake check  # All passing\n```\n\nCloses #27",
      "pr_link": "https://github.com/spinje/pflow/pull/28",
      "pr_number": 28
    }
  },
  {
    "draft": "Fixed issue where SIGPIPE would kill the process when a subprocess ignores stdin [#26](https://github.com/spinje/pflow/pull/26)",
    "index": 27,
    "context": {
      "commit_message": "Merge pull request #26 from spinje/fix/silent-workflow-failure",
      "task_number": null,
      "files_changed": ".taskmaster/knowledge/pitfalls.md, src/pflow/cli/main.py, tests/test_integration/test_sigpipe_regression.py, tests/test_nodes/test_shell/test_shell_sigpipe.py",
      "pr_title": "fix: prevent SIGPIPE from killing process when subprocess ignores stdin (fixes #25)",
      "pr_body": "## Summary\n\nFixes a critical bug where shell nodes would cause **silent exit 141** when the subprocess doesn't consume all its stdin data. This happened when:\n- Large stdin data (>16KB on macOS, >64KB on Linux) was passed to a shell command\n- The command didn't read stdin (e.g., `echo '[]'` in a conditional branch)\n- SIGPIPE was set to `SIG_DFL`, which immediately terminates Python\n\n## Root Cause\n\nThe signal handler `signal.signal(signal.SIGPIPE, signal.SIG_DFL)` caused Python to terminate with exit 141 when `subprocess.run()` tried to write to a pipe that was closed because the subprocess exited without consuming stdin.\n\n**Example trigger**: A workflow with conditional shell logic:\n```bash\ncase '${describe_images}' in\n  *[Ff]alse*) echo '[]' ;;    # Doesn't read stdin → SIGPIPE\n  *) grep | jq ;;              # Reads all stdin → works fine\nesac\n```\n\n## The Fix\n\nChange `SIG_DFL` to `SIG_IGN`:\n```python\nsignal.signal(signal.SIGPIPE, signal.SIG_IGN)\n```\n\nWith `SIG_IGN`, `subprocess.run()` handles broken pipes gracefully internally.\n\n## Changes\n\n| File | Changes |\n|------|---------|\n| `src/pflow/cli/main.py` | `SIG_DFL` → `SIG_IGN` with detailed comment |\n| `tests/test_nodes/test_shell/test_shell_sigpipe.py` | 9 unit tests |\n| `tests/test_integration/test_sigpipe_regression.py` | 7 integration tests |\n| `.taskmaster/knowledge/pitfalls.md` | Corrected SIGPIPE guidance |\n\n## Testing\n\n```bash\n# Run the new regression tests\nmake test\n\n# All 16 SIGPIPE tests pass\npytest tests/test_nodes/test_shell/test_shell_sigpipe.py tests/test_integration/test_sigpipe_regression.py -v\n```\n\n### Test Coverage\n\nThe tests verify:\n- ✅ Large stdin (20KB+) with commands that ignore stdin\n- ✅ Partial stdin consumption (`head -n 1`)\n- ✅ Conditional shell commands (the exact bug pattern)\n- ✅ Template resolution with boolean `False`\n- ✅ Meta-test verifying SIGPIPE is set to `SIG_IGN`\n\n### Verified the tests catch regression\n\n```bash\n# With SIG_DFL (the bug): Exit 141\n# With SIG_IGN (the fix): Exit 0\n```\n\nFixes #25",
      "pr_link": "https://github.com/spinje/pflow/pull/26",
      "pr_number": 26
    }
  },
  {
    "draft": "Fixed batch template validation to allow dotted references like `${item.field}`",
    "index": 28,
    "context": {
      "commit_message": "fix: batch template validation now allows dotted refs like ${item.field}",
      "task_number": null,
      "files_changed": "src/pflow/core/workflow_data_flow.py, tests/test_core/test_workflow_data_flow.py",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Fixed LLM usage tracking from batch node inner executions [#22](https://github.com/spinje/pflow/pull/22)",
    "index": 32,
    "context": {
      "commit_message": "Merge pull request #22 from spinje/fix/llm-costs-batch",
      "task_number": null,
      "files_changed": "src/pflow/runtime/batch_node.py, tests/test_runtime/test_batch_node.py",
      "pr_title": "fix: capture LLM usage from batch node inner executions",
      "pr_body": "## Summary\n\nFixes #21 - LLM costs were not being tracked for batch node execution because the `llm_usage` data written by inner LLM nodes to the per-item isolated context (`item_shared`) was discarded after each item's execution.\n\n## Root Cause\n\nThe wrapper chain applies `InstrumentedNodeWrapper` around `PflowBatchNode`, not the inner LLM node:\n```\nInstrumentedNodeWrapper (outermost)\n  └─ PflowBatchNode\n     └─ NamespacedNodeWrapper  \n        └─ TemplateAwareNodeWrapper\n           └─ LLMNode (innermost)\n```\n\nWhen batch executes each item, the inner LLM node writes `llm_usage` to `item_shared`, but this context is discarded after execution. The outer `InstrumentedNodeWrapper` never sees this data.\n\n## Changes\n\n- Added `_capture_item_llm_usage()` method to `PflowBatchNode` that captures `llm_usage` from `item_shared` after each inner node execution\n- Initializes `__llm_calls__` list in `prep()` before batch execution starts (critical for shallow copy to share the list reference)\n- Captures work for both sequential and parallel execution modes\n- Handles both root-level (`shared[\"llm_usage\"]`) and namespaced (`shared[node_id][\"llm_usage\"]`) locations\n- Adds `node_id` and `batch_item_index` to each captured LLM call for tracing\n\n```\n src/pflow/runtime/batch_node.py       |  46 ++++++\n tests/test_runtime/test_batch_node.py | 258 ++++++++++++++++++++++++++++++++++\n 2 files changed, 304 insertions(+)\n```\n\n## Testing\n\nAdded 7 new tests:\n- `test_batch_captures_inner_node_llm_usage_sequential` - Sequential capture\n- `test_batch_captures_inner_node_llm_usage_parallel` - Parallel capture with thread safety\n- `test_batch_captures_namespaced_llm_usage` - Namespaced location support\n- `test_batch_initializes_llm_calls_list` - List initialization before batch\n- `test_batch_no_llm_usage_no_crash` - Non-LLM nodes don't crash\n- `test_llm_calls_contain_all_required_fields_for_metrics` - All required fields present for cost calculation\n- Original `test_llm_calls_accumulated` still passes\n\nRun `make test` to verify all 3685 tests pass.",
      "pr_link": "https://github.com/spinje/pflow/pull/22",
      "pr_number": 22
    }
  },
  {
    "draft": "Display batch node progress as items complete [#20](https://github.com/spinje/pflow/pull/20)",
    "index": 34,
    "context": {
      "commit_message": "Merge pull request #20 from spinje/feat/batch-progress-display",
      "task_number": null,
      "files_changed": ".taskmaster/tasks/task_100/task-100.md, .taskmaster/tasks/task_101/task-101.md, CLAUDE.md, examples/real-workflows/README.md, examples/real-workflows/webpage-to-markdown.json ... and 5 more files",
      "pr_title": "feat: display batch node progress as items complete",
      "pr_body": "## Summary\n\nAdd real-time progress updates for batch node execution, showing item completion status as they happen instead of blocking silently until all items complete.\n\n**Before:**\n```\n  convert-sections... ✓ 24.9s\n```\n\n**After:**\n```\n  convert-sections... 1/8 ✓\n  convert-sections... 2/8 ✓     (updates in place via \\r)\n  convert-sections... 3/8 ✗     (shows failures)\n  ...\n  convert-sections... 8/8 ✓ 24.9s\n```\n\n## Changes\n\n- **output_controller.py**: Add `_handle_batch_progress()` method using carriage return for in-place line updates; extend `_handle_node_complete()` to handle batch nodes (only shows timing, not duplicate checkmark)\n- **batch_node.py**: Call progress callback after each item in `_exec_sequential()` and `_collect_parallel_results()`\n- **instrumented_wrapper.py**: Detect batch nodes via `batch_metadata` in output and pass `is_batch`, `batch_total`, `batch_success_count` to completion callback\n- **Tests**: Add `TestBatchProgressCallbacks` (6 tests) and `TestBatchProgressDisplay` (8 tests)\n\n```\n src/pflow/core/output_controller.py      |  71 ++++++++++--\n src/pflow/runtime/batch_node.py          |  57 +++++++++-\n src/pflow/runtime/instrumented_wrapper.py|  15 ++-\n tests/test_core/test_output_controller.py| 193 +++++++++++++++++++++++++++++++\n tests/test_runtime/test_batch_node.py    | 185 +++++++++++++++++++++++++++++\n 5 files changed, 508 insertions(+), 13 deletions(-)\n```\n\n## Testing\n\nAll 3677 tests pass:\n```bash\nmake test\n```\n\nFixes #17",
      "pr_link": "https://github.com/spinje/pflow/pull/20",
      "pr_number": 20
    }
  },
  {
    "draft": "Fixed template variables to allow arrays and dictionaries to serialize to JSON in string contexts [#19](https://github.com/spinje/pflow/pull/19)",
    "index": 35,
    "context": {
      "commit_message": "Merge pull request #19 from spinje/fix/template-variable-mismatch",
      "task_number": null,
      "files_changed": ".taskmaster/bugfix/bugfix-log.md, .taskmaster/knowledge/pitfalls.md, .taskmaster/tasks/task_96/implementation/progress-log.md, architecture/reference/template-variables.md, examples/legacy/fix_issue_workflow.json ... and 21 more files",
      "pr_title": "fix: allow arrays/dicts to serialize to JSON in string contexts",
      "pr_body": "## Summary\n\nFixes #18 - Arrays and dicts can now be embedded in shell command strings (and other string parameters) by auto-serializing to JSON.\n\n## Changes\n\n- **Type checker update**: Added `str`, `string` to compatible types for `dict`, `list`, `array`, `object`\n- **Shell-unsafe JSON warning**: New `_warn_shell_unsafe_json()` method warns when JSON contains characters that could break shell parsing (`'`, `` ` ``, `$(`)\n- **Documentation**: Updated `architecture/reference/template-variables.md` with new type matrix and \"Shell Command Limitations\" section\n- **Tests updated**: Modified tests that expected dict→str to fail, added new tests for list→str\n\n## Behavior\n\n| Template | Data | Result |\n|----------|------|--------|\n| `\"echo '${items}'\"` | `[\"a\", \"b\"]` | ✅ Works → `echo '[\"a\", \"b\"]'` |\n| `\"${items}\"` | `[\"a\", \"b\"]` | ❌ Blocked (simple template, runtime check preserved) |\n| `\"echo '${data}'\"` | `{\"msg\": \"it's\"}` | ⚠️ Warning about shell-unsafe chars |\n\n## Guardrails Preserved\n\n1. **Simple templates** (`${items}` alone) → runtime check still blocks dict/list→str\n2. **Shell-unsafe JSON** → warning emitted, suggests using `stdin`\n3. **Type mismatches for non-string targets** (dict→int) → still caught\n\n## File Changes\n\n```\n architecture/reference/template-variables.md       | 62 ++++++++++-\n src/pflow/nodes/shell/shell.py                     | 44 ++++++++\n src/pflow/runtime/template_validator.py            |  2 +-\n src/pflow/runtime/type_checker.py                  |  8 +-\n tests/test_runtime/test_template_validator_types.py| 73 ++++++++++---\n tests/test_runtime/test_type_checker.py            | 38 ++++---\n 6 files changed, 190 insertions(+), 37 deletions(-)\n```\n\n## Testing\n\nRun `make test` to verify all tests pass (3612 passed).\nRun `make check` to verify linting and type checks pass.",
      "pr_link": "https://github.com/spinje/pflow/pull/19",
      "pr_number": 19
    }
  },
  {
    "draft": "Added batch processing with sequential and parallel execution [#11](https://github.com/spinje/pflow/pull/11)",
    "index": 46,
    "context": {
      "commit_message": "Merge pull request #11 from spinje/feat/batch-processing",
      "task_number": 96,
      "files_changed": ".taskmaster/tasks/task_39/starting-context/task-39-handover-01.md, .taskmaster/tasks/task_39/starting-context/task-39-handover-from-task-96.md, .taskmaster/tasks/task_96/implementation/batch-error-display-fix.md, .taskmaster/tasks/task_96/implementation/batch-error-display-implementation-plan.md, .taskmaster/tasks/task_96/implementation/implementation-plan.md ... and 32 more files",
      "pr_title": "feat: add batch processing with sequential and parallel execution",
      "pr_body": "## Summary\n\nAdds batch processing capability to pflow workflows, enabling a single node to process multiple items from an array with isolated contexts per item. Supports both sequential and parallel execution modes.\n\n**Task 96: Support Batch Processing in Workflows**\n\n## Changes\n\n### Phase 1 - Sequential Batch Processing\n- Add `batch` configuration to IR schema (`items`, `as`, `error_handling`)\n- Implement `PflowBatchNode` with isolated context per item\n- Integrate batch wrapper in compiler between Namespace and Instrumented wrappers\n- Two-layer error detection (exceptions + result dict error key)\n\n### Phase 2 - Parallel Batch Processing\n- Add `parallel`, `max_concurrent`, `max_retries`, `retry_wait` to schema\n- Refactor to inherit from `Node` (not `BatchNode`) for thread safety\n- Implement `ThreadPoolExecutor` with deep copy per thread\n- Thread-safe retry with local variable (avoids `self.cur_retry` race condition)\n- Deep copy node chain per thread (avoids `TemplateAwareNodeWrapper` race condition)\n- Preserve result ordering despite async completion\n\n### Key Design Decisions\n- Batch wrapper **outside** namespace for root-level item injection\n- Shallow copy of shared store (shares `__llm_calls__` for LLM tracking)\n- Deep copy of node chain per thread for isolation\n- Same `_exec_single()` code path for both sequential and parallel modes\n\n## Example IR Syntax\n\n```json\n{\n  \"id\": \"summarize\",\n  \"type\": \"llm\",\n  \"batch\": {\n    \"items\": \"${list_files.files}\",\n    \"as\": \"file\",\n    \"parallel\": true,\n    \"max_concurrent\": 5,\n    \"max_retries\": 3,\n    \"error_handling\": \"continue\"\n  },\n  \"params\": {\"prompt\": \"Summarize: ${file}\"}\n}\n```\n\n## Files Changed\n\n| File | Purpose |\n|------|---------|\n| `src/pflow/core/ir_schema.py` | Add `BATCH_CONFIG_SCHEMA` with Phase 1+2 fields |\n| `src/pflow/runtime/batch_node.py` | Core `PflowBatchNode` implementation |\n| `src/pflow/runtime/compiler.py` | Integrate batch wrapper in compilation chain |\n| `examples/batch-test.json` | Sequential batch example |\n| `examples/batch-test-parallel.json` | Parallel batch example |\n\n## Documentation\n\n- `.taskmaster/tasks/task_96/task-96.md` - Task specification\n- `.taskmaster/tasks/task_96/implementation/progress-log.md` - Implementation progress\n- `.taskmaster/tasks/task_96/research/task-39-synergy-analysis.md` - Synergy with Task 39 (fan-out)\n\n## Testing\n\n- **98 batch-related tests** added (26 schema + 57 batch node + 15 compiler)\n- All 3562 project tests pass\n- `make check` passes (linting, type checking)\n\n```bash\nmake test  # Run all tests\nmake check # Run linting and type checks\n```\n\n## Task 39 Synergy\n\nThis implementation establishes patterns that Task 39 (Task Parallelism / Fan-out) will reuse:\n- Deep copy per thread pattern for isolation\n- ThreadPoolExecutor usage\n- Error handling modes (fail_fast/continue)\n\nTask 39's `ParallelGroupNode` will be ~20 lines using these established patterns.",
      "pr_link": "https://github.com/spinje/pflow/pull/11",
      "pr_number": 11
    }
  },
  {
    "draft": "Migrated Claude Code node to Claude Agent SDK and streamlined parameters [#10](https://github.com/spinje/pflow/pull/10)",
    "index": 47,
    "context": {
      "commit_message": "Merge pull request #10 from spinje/feat/claude-agent-sdk-migration",
      "task_number": null,
      "files_changed": ".taskmaster/tasks/task_99/starting-context/task-99-handover.md, .taskmaster/tasks/task_99/task-99.md, NOTICE, docs/reference/nodes/claude-code.mdx, pyproject.toml ... and 3 more files",
      "pr_title": "feat(claude-code): migrate to Claude Agent SDK and streamline parameters",
      "pr_body": "## Summary\n\nMigrates the ClaudeCodeNode from the deprecated `claude-code-sdk` to the new `claude-agent-sdk` v0.1.18, streamlines parameter naming to match the SDK, and adds sandbox support for command execution isolation.\n\n## Breaking changes\n\n| Before | After | Reason |\n|--------|-------|--------|\n| `task` | `prompt` | Matches SDK naming |\n| `working_directory` | `cwd` | Matches SDK naming |\n| `context` | *(removed)* | Users include context in prompt directly |\n\n## Changes\n\n- **SDK Migration**: Update from `claude-code-sdk>=0.0.25` to `claude-agent-sdk>=0.1.18`\n- **New sandbox parameter**: Add command isolation configuration with options:\n  - `enabled` - Enable sandbox mode\n  - `autoAllowBashIfSandboxed` - Auto-allow bash when sandboxed\n  - `excludedCommands` - Commands that bypass sandbox (e.g., `[\"docker\"]`)\n  - `allowUnsandboxedCommands` - Allow model to request unsandboxed execution\n  - `network` - Network settings (allowLocalBinding, allowUnixSockets)\n- **Parameter streamlining**: Rename parameters to match Claude Agent SDK naming conventions\n- **Documentation**: Complete rewrite of `docs/reference/nodes/claude-code.mdx` with:\n  - Updated parameter tables\n  - Sandbox configuration documentation\n  - Links to official Claude Agent SDK docs\n  - Expanded tools table (Glob, Grep, LS, WebFetch, WebSearch)\n\n## Files changed\n\n```\npyproject.toml                          - SDK dependency update\nNOTICE                                  - License attribution update\nsrc/pflow/nodes/claude/claude_code.py   - Node implementation\ntests/test_nodes/test_claude/...        - Updated tests (41 pass)\ndocs/reference/nodes/claude-code.mdx    - User documentation\nuv.lock                                 - Lockfile update\n```\n\n## Testing\n\n```bash\nmake test   # 41 tests pass\nmake check  # All linting/type checks pass\n```\n\nManually verified the node works with:\n- `prompt` parameter (basic execution)\n- `cwd` parameter (working directory)\n- `sandbox` parameter validation",
      "pr_link": "https://github.com/spinje/pflow/pull/10",
      "pr_number": 10
    }
  },
  {
    "draft": "Enabled all tools by default and added session and timeout parameters to the Claude Code node",
    "index": 48,
    "context": {
      "commit_message": "feat(claude-code): enable all tools by default and add session/timeout params",
      "task_number": null,
      "files_changed": ".taskmaster/tasks/task_99/starting-context/task-99-handover.md, .taskmaster/tasks/task_99/task-99.md, scratchpads/super-code-reviewer/research-findings.md, src/pflow/nodes/claude/claude_code.py, tests/test_nodes/test_claude/test_claude_code.py",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Fixed an issue where the planner did not handle missing file paths gracefully",
    "index": 49,
    "context": {
      "commit_message": "fix: planner now handles missing file paths gracefully",
      "task_number": null,
      "files_changed": "src/pflow/planning/prompts/requirements_analysis.md, src/pflow/planning/prompts/workflow_system_overview.md",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  }
]
```
