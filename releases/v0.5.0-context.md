# v0.5.0 Release Context

Generated: 2026-01-13
This file contains implementation context for AI agents and release verification.

---

## Changelog

## v0.5.0 (2026-01-13)

- Removed parameter fallback pattern from all nodes to prevent silent namespace collisions [#28](https://github.com/spinje/pflow/pull/28)
- Changed `claude-code` node to use Claude Agent SDK, renaming parameters to `prompt` and `cwd`, enabling all tools by default, and adding `session` and `timeout` support [#10](https://github.com/spinje/pflow/pull/10)
- Changed shell node to strip trailing newlines from `stdout` by default [#51](https://github.com/spinje/pflow/pull/51)
- Changed CLI agent instructions to increase the `max_concurrent` limit for batch processing
- Added batch processing capability with support for both sequential and parallel execution modes [#11](https://github.com/spinje/pflow/pull/11)
- Added real-time progress display for batch node execution, showing completion status as items finish [#20](https://github.com/spinje/pflow/pull/20)
- Added automatic JSON string parsing during nested template access (e.g., `${node.stdout.field}`) [#37](https://github.com/spinje/pflow/pull/37)
- Added automatic parsing of JSON strings in inline object templates [#39](https://github.com/spinje/pflow/pull/39)
- Added automatic coercion of dictionary and list inputs to JSON strings for string-typed parameters [#41](https://github.com/spinje/pflow/pull/41)
- Added type preservation for simple templates in nested structures to prevent double-serialization [#32](https://github.com/spinje/pflow/pull/32)
- Added the original input item to batch results as an `item` field for self-contained downstream processing [#54](https://github.com/spinje/pflow/pull/54)
- Added support for auto-serializing arrays and dictionaries to JSON when used in string template contexts [#19](https://github.com/spinje/pflow/pull/19)
- Added stderr check to shell node smart handling to ensure pipeline errors are caught [#58](https://github.com/spinje/pflow/pull/58)
- Added release context file and CLI summary to the `generate-changelog` workflow
- Fixed critical SIGPIPE issue causing silent process termination when subprocesses ignored large stdin data [#26](https://github.com/spinje/pflow/pull/26)
- Fixed workflow name mismatch bug by deriving the workflow name directly from its filename [#46](https://github.com/spinje/pflow/pull/46)
- Fixed optional workflow inputs without defaults to resolve to `None` instead of failing resolution [#49](https://github.com/spinje/pflow/pull/49)
- Fixed LLM usage tracking for batch node inner executions to ensure costs are correctly captured [#22](https://github.com/spinje/pflow/pull/22)
- Fixed issue where upstream shell stderr was not surfaced when downstream nodes failed [#52](https://github.com/spinje/pflow/pull/52)
- Fixed shell command validation to allow union types containing `str` or `any` [#61](https://github.com/spinje/pflow/pull/61)
- Fixed structured output mode skipping smart filtering and added Gemini thinking optimizations [#43](https://github.com/spinje/pflow/pull/43)
- Fixed static validation to correctly recognize batch output structures and dotted references like `${item.field}`
- Fixed `--output-format json` support for the `registry run` command
- Fixed validation failures by normalizing type strings to lowercase across the registry and nodes
- Fixed the planner to handle missing file paths gracefully
- Improved shell error visibility by surfacing `stderr` even when the exit code is 0 [#56](https://github.com/spinje/pflow/pull/56)
- Improved shell node validation by moving dictionary and list checks to compile time [#30](https://github.com/spinje/pflow/pull/30)
- Improved `git-log` node to support tag and branch refs for `since` and `until` parameters

---

## Skipped Changes (Verification)

Review these to ensure nothing was incorrectly classified as internal:

- fix: update generate-changelog workflow
- fix: simplify generate-changelog-simple workflow
- fix: remove flaky assertion on exact prompt text in metadata test
- Add complex shell examples to agent instructions
- docs: enhance agent feedback documentation with suggestions for clearer error reporting and output visibility
- docs: update cli-agent-instructions to clarify shell variable usage
- Simplify discovery response and improve test patterns
- docs: add research input for Task 108 inline debug flag
- Enhance CLI agent instructions with simplicity guidance
- Enhance documentation for agent feedback and batch results clarity
- Clarify LLM node usage and formatting guidance in agent instructions
- docs: update CLI agent instructions for shell node usage
- docs: add Task 109 for sandbox bypass security controls
- docs: add feedback on agent complaints and suggestions for improvement
- docs: update CLI agent instructions for clarity and best practices
- draft for release workflow
- docs: add deprecation warnings for planner and auto-repair features
- fix: add defensive error handling and additional tests for param coercion [skip review]
- docs: restructure documentation with "How it works" tab for technical content
- update real workflows for changelog generation
- docs: improve agent instructions with consistent extraction/transformation guidance
- docs: update CLAUDE.md to enhance pflow usage instructions
- docs: streamline User Decisions section in CLAUDE.md
- docs: clarify that agents must read instruction parts in full
- docs: add Task 108 - Smart Trace Debug Output for Agent Iteration
- docs: add agent steering principles to CLAUDE.md
- docs: update create-pr.md to include task section
- docs: sync CLAUDE.md task list with actual state
- docs: add shell command development example to Task 106 handover
- docs: add --run-node as future enhancement for Task 106
- docs: add comprehensive format exploration for Task 107
- feat: add Task 107 - Markdown Workflow Format
- refactor: improve create-task and braindump command prompts
- feat: add Task 106 - Automatic Workflow Iteration Cache
- feat: add generate-changelog example workflow
- forgot to add task review to task 105
- docs: clarify batch results contain inner node outputs
- docs: add 'No Parallel Paths' to workflow limitations
- more simple real workflow
- fix: improve rm command safety checks in pre_tool_use.py
- docs: update performance metrics and vision cost breakdown in README
- docs: add LLM capture documentation and interleaved batch test
- chore: rename GitHub repo references from anthropics to spinje
- remove statusline from project
- cleanup
- fix: eliminate non-LLM skipped tests by fixing or removing them
- fix: add example IR validation test and fix schema compliance issues
- fix: implement skipped test_missing_variable_keeps_template test
- fix: update test to use existing echo node instead of non-existent test-node
- fix: resolve race condition in SettingsManager causing intermittent test failures
- test
- chore: add frontmatter to commands and simplify read_context logic
- add note to task 99

---

## Task Implementation Reviews

### Task 96

# Task 96 Review: Support Batch Processing in Workflows

## Metadata

- **Implementation Date**: 2024-12-23 to 2024-12-27
- **Branch**: `feat/batch-processing`
- **Total Tests Added**: 140 batch-related tests
- **Files Changed**: 12 files (6 core, 6 test)

## Executive Summary

Task 96 implemented both sequential and parallel batch processing for pflow workflows, enabling a single node to process multiple items from an array with isolated execution contexts. The implementation required significant architectural discoveries—most critically, changing from `BatchNode` to `Node` inheritance to avoid thread-safety issues, and establishing the deep copy pattern for thread isolation that Task 39 will reuse.

## Implementation Overview

### What Was Built

1. **Sequential Batch Processing (Phase 1)**: Process items one at a time with isolated `dict(shared)` context per item
2. **Parallel Batch Processing (Phase 2)**: Concurrent execution using `ThreadPoolExecutor` with deep-copied node chains per thread
3. **Batch Metadata**: Rich timing stats and execution mode info that auto-flows into workflow traces
4. **Batch Error Display**: CLI and MCP display enhancements showing "8/10 items succeeded, 2 failed" summaries

### Deviations from Original Spec

| Spec Said | Actually Built | Reason |
|-----------|---------------|--------|
| Inherit from `BatchNode` | Inherit from `Node` directly | `BatchNode` uses `self.cur_retry` which races in parallel |
| Use MRO trick for retry | Local `retry` variable | Thread-safe, clearer code |
| Phase 2 "later" | Implemented in same PR | Synergy with Task 39, context was fresh |

### Implementation Approach

The wrapper chain position was the critical architectural decision. Batch wraps OUTSIDE namespace:

```
Instrumented._run(shared)
  → PflowBatchNode._run(shared)           # Injects ${item} at ROOT level
    → NamespacedNodeWrapper._run(shared)  # Creates proxy
      → TemplateAwareNodeWrapper._run(proxy)  # Resolves ${item}
        → ActualNode._run(proxy)
```

If batch were inside namespace, `shared["item"] = x` would write to `shared["node_id"]["item"]`, and template resolution for `${item}` would fail.

## Files Modified/Created

### Core Changes

| File | Purpose |
|------|---------|
| `src/pflow/core/ir_schema.py` | Added `BATCH_CONFIG_SCHEMA` with 7 fields: `items`, `as`, `error_handling`, `parallel`, `max_concurrent`, `max_retries`, `retry_wait` |
| `src/pflow/runtime/batch_node.py` | **NEW** - 500+ line `PflowBatchNode` class with sequential and parallel execution |
| `src/pflow/runtime/compiler.py` | Integrated batch wrapper at line ~680, between Namespace and Instrumented |
| `src/pflow/execution/execution_state.py` | Added batch detection via `batch_metadata` key |
| `src/pflow/execution/formatters/success_formatter.py` | Added `_format_batch_node_line()` and `_format_batch_errors_section()` |
| `src/pflow/cli/main.py` | Updated `_format_node_status_line()` and added `_display_batch_errors()` |

### Test Files

| File | Tests | Critical Tests |
|------|-------|----------------|
| `tests/test_core/test_ir_schema.py` | 26 | `test_batch_config_items_not_template` - validates template pattern |
| `tests/test_runtime/test_batch_node.py` | 83 | `test_parallel_retry_resets_namespace` - regression test for namespace reset bug |
| `tests/test_runtime/test_compiler_batch.py` | 15 | `test_batch_node_in_wrapper_chain` - validates wrapper order |
| `tests/test_execution/formatters/test_success_formatter.py` | 23 | `test_batch_errors_capped_at_5` - prevents overwhelming output |

## Integration Points & Dependencies

### Incoming Dependencies

| Component | Depends On | Via |
|-----------|-----------|-----|
| Workflow Compiler | `PflowBatchNode` | `compile_ir_to_flow()` creates batch wrapper when `batch` config present |
| Trace System | Batch output | `batch_metadata` key captured in `shared_after` |
| CLI Display | Batch step fields | `is_batch`, `batch_total`, `batch_failed` in execution steps |
| MCP Display | Batch step fields | Same fields via `format_success_as_text()` |

### Outgoing Dependencies

| This Task | Depends On | Via |
|-----------|-----------|-----|
| Template Resolution | `TemplateResolver.resolve_value()` | Resolves `${node.items}` to actual array |
| Namespace Store | `NamespacedSharedStore.keys()` | Returns union of namespace + root keys (critical for `${item}` visibility) |
| Instrumented Wrapper | `InstrumentedNodeWrapper` | Wraps batch for metrics/tracing |

### Shared Store Keys

| Key Pattern | Purpose | Data Structure |
|-------------|---------|----------------|
| `shared[node_id].results` | Array of per-item results | `list[dict]` |
| `shared[node_id].batch_metadata` | Execution metadata | `{"parallel": bool, "timing": {...}, ...}` |
| `shared[node_id].errors` | Error details | `list[{"index": int, "item": Any, "error": str}]` or `None` |
| `shared[item_alias]` | Current item during iteration | `Any` (injected at root level) |

## Architectural Decisions & Tradeoffs

### Key Decisions

| Decision | Reasoning | Alternative Rejected |
|----------|-----------|---------------------|
| Inherit from `Node` not `BatchNode` | `BatchNode` uses `self.cur_retry` instance state that races in parallel | MRO trick was clever but obscure and not thread-safe |
| Deep copy node chain per thread | `TemplateAwareNodeWrapper` mutates `inner_node.params`—races without isolation | Threading lock would serialize execution, defeating parallelism |
| Shallow copy shared store | `__llm_calls__` list needs to accumulate across all items (GIL-protected) | Deep copy would isolate tracking, breaking metrics |
| Batch outside namespace wrapper | Item alias must be at root level for `${item}` to resolve | Inside would write to `shared["node_id"]["item"]` |
| No shared concurrency module | ~100 lines inline, Task 39 follows same pattern | Premature abstraction; can extract later if needed |

### Technical Debt Incurred

| Debt | Reason | Future Fix |
|------|--------|------------|
| `exec()` raises `NotImplementedError` | Inherited from Node but never called (we override `_exec`) | Document clearly, or restructure inheritance |
| `# noqa: C901` on `_collect_parallel_results` | Concurrent result collection is inherently complex | Accept complexity, document well |

## Testing Implementation

### Test Strategy Applied

1. **Unit tests first**: `PflowBatchNode` in isolation with mock inner nodes
2. **Integration tests**: Through compiler with real node types (ValueNode, ShellNode)
3. **Thread safety tests**: Verify isolation with concurrent execution
4. **Regression tests**: Specific tests for bugs found during review

### Critical Test Cases

| Test | What It Catches |
|------|-----------------|
| `test_parallel_retry_resets_namespace` | Namespace pollution between retries in parallel mode |
| `test_batch_with_template_resolution` | `set_params()` forwarding to inner node chain |
| `test_batch_node_in_wrapper_chain` | Correct wrapper order (Batch outside Namespace) |
| `test_parallel_preserves_order` | Result ordering despite completion order |
| `test_llm_calls_accumulated_correctly` | `__llm_calls__` shared via shallow copy |

## Unexpected Discoveries

### Gotchas Encountered

1. **`set_params()` doesn't forward by default**: `BaseNode.set_params()` only sets `self.params`. Batch must override to forward to inner node, or `TemplateAwareNodeWrapper` never sees templates.

2. **NamespacedSharedStore.keys() returns UNION**: The proxy returns both namespace keys AND root keys. This is WHY `${item}` works—template resolution sees root-level keys through the proxy.

3. **Shell node outputs strings, not arrays**: `${shell.stdout}` is a string like `'["a","b","c"]\n'`. For batch processing, use HTTP node (parses JSON) or other array-producing nodes.

4. **Dual display paths**: CLI has `_display_execution_summary()` in `main.py`, MCP uses `format_success_as_text()` in `success_formatter.py`. BOTH needed updating for batch display.

### Edge Cases Found

| Edge Case | Handling |
|-----------|----------|
| Empty items array | Returns empty results, no errors |
| Single item | Works correctly in both modes |
| `max_concurrent=1` | Effectively sequential (useful for debugging) |
| Item is `0` or `""` (falsy) | Use `if key in shared` not `shared.get(key) or ...` |
| Partial writes before failure | Namespace reset on each retry |

## Patterns Established

### Reusable Patterns

**1. Deep Copy for Thread Isolation**
```python
def process_item(idx, item):
    thread_node = copy.deepcopy(self.inner_node)  # Thread-local copy
    return thread_node._run(item_shared)
```
Task 39's `ParallelGroupNode` will use this exact pattern.

**2. Thread-Safe Retry with Local Variable**
```python
for retry in range(self.max_retries):  # Local, not self.cur_retry
    try:
        result = self._execute(item)
        return result
    except Exception as e:
        if retry < self.max_retries - 1:
            time.sleep(self.retry_wait)
            continue
        raise
```

**3. Shallow Copy for Shared Mutable Tracking**
```python
item_shared = dict(shared)  # Shallow: shares __llm_calls__ list
item_shared[self.node_id] = {}  # Fresh namespace per item
item_shared[self.item_alias] = item  # Inject at root
```

**4. Batch Detection via Metadata Key**
```python
if isinstance(node_output, dict) and "batch_metadata" in node_output:
    # It's a batch node
```

### Anti-Patterns to Avoid

| Anti-Pattern | Why It Fails |
|--------------|--------------|
| `shared.get("item") or shared.get("file")` | Fails when item is `0` or `""` (falsy) |
| Deep copy of shared store | Breaks `__llm_calls__` accumulation |
| Batch inside namespace wrapper | `${item}` won't resolve (wrong level) |
| `self.cur_retry` for parallel retry | Races between threads |

## Breaking Changes

### API/Interface Changes

| Change | Impact |
|--------|--------|
| New `batch` property on nodes | Additive, no breaking change |
| New output keys (`batch_metadata`, etc.) | Additive, existing code ignores |

### Behavioral Changes

| Change | Impact |
|--------|--------|
| Batch nodes show "8/10 items succeeded" | Improved UX, no breaking change |
| Trace files include batch timing | More data in traces, no breaking change |

## Future Considerations

### Extension Points

1. **Task 39 Integration**: `ParallelGroupNode` should use same deep copy pattern, same `max_concurrent` config
2. **Async Batch**: Replace `ThreadPoolExecutor` with `asyncio.gather` + `to_thread()` for async nodes
3. **Nested Batch**: Batch node containing another batch node (not tested, may work)

### Scalability Concerns

| Concern | Mitigation |
|---------|------------|
| Deep copy overhead for large node chains | Measured at ~100μs vs ~2000ms for LLM calls (0.05%) |
| Memory with many concurrent items | `max_concurrent` defaults to 10, caps at 100 |
| Error list size with many failures | Display capped at 5 errors |

## AI Agent Guidance

### Quick Start for Related Tasks

**For Task 39 (Parallel Nodes)**:
1. Read `batch_node.py` lines 420-480 (`_exec_parallel`)
2. Copy the deep copy pattern exactly
3. Your `ParallelGroupNode` is ~20 lines using established patterns

**For any wrapper chain work**:
1. Read `compiler.py` lines 650-700 to understand wrapper order
2. Batch is OUTSIDE namespace—this is critical and non-negotiable

**For template resolution issues**:
1. Check if `set_params()` forwards to inner node
2. Check if item alias is injected at ROOT level (not in namespace)
3. Verify `NamespacedSharedStore.keys()` behavior

### Common Pitfalls

| Pitfall | Solution |
|---------|----------|
| Template returns `None` for `${item}` | Add `set_params()` forwarding to inner node |
| Parallel tests pass locally, fail in CI | Use `dict(shared)` shallow copy, not deep copy |
| Retry counter shared between items | Use local `retry` variable, not `self.cur_retry` |
| Only CLI or only MCP shows batch info | Update BOTH display paths |

### Test-First Recommendations

When modifying batch processing:
1. Run `test_batch_node_in_wrapper_chain` first (validates architecture)
2. Run `test_parallel_retry_resets_namespace` (catches context pollution)
3. Run full batch suite: `pytest tests/test_runtime/test_batch_node.py -v`

When modifying display:
1. Run `test_batch_errors_capped_at_5` (validates truncation)
2. Run `pytest tests/test_execution/formatters/test_success_formatter.py -v`

---

## Appendix: Key Code Locations

| Concept | File | Line Range |
|---------|------|------------|
| Batch schema | `ir_schema.py` | 117-175 |
| Wrapper chain insertion | `compiler.py` | 671-689 |
| Sequential execution | `batch_node.py` | 170-200 |
| Parallel execution | `batch_node.py` | 420-480 |
| Thread-safe retry | `batch_node.py` | 200-260 |
| Batch detection | `execution_state.py` | 85-100 |
| CLI batch display | `main.py` | `_format_node_status_line()` |
| MCP batch display | `success_formatter.py` | `_format_batch_node_line()` |

---

*Generated from implementation context of Task 96*

### Task 102

# Task 102 Review: Remove Parameter Fallback Pattern

## Metadata
- **Implementation Date**: 2024-12-30
- **Branch**: `fix/namespace-collision`
- **Related Tasks**: Task 9 (Namespacing), Task 11 (First file nodes where pattern originated)

## Executive Summary

Removed the "shared store fallback" pattern from all 20 platform nodes (~60 parameters) that caused silent failures when node IDs or workflow inputs matched parameter names. The fix aligns pflow with PocketFlow's original design philosophy where params and shared store are separate channels, with templates serving as the explicit data wiring mechanism.

## Implementation Overview

### What Was Built

**Original spec proposed 4 options:**
1. Filter namespaces from visibility (heuristic-based)
2. Invert priority (params first, then shared)
3. Add collision detection (compile-time errors)
4. Explicit namespace prefix (breaking change)

**What was actually implemented: None of the above.**

Through deep investigation, we discovered the root cause was the fallback pattern itself. The solution was simpler and more correct:
- **Remove the fallback entirely** - nodes read only from `self.params`
- **Templates handle all wiring** - `${var}` resolves from shared store into params
- **Collision detection unnecessary** - with params-only, naming collisions simply don't matter

### Implementation Approach

1. **Investigation Phase**: Launched 6 parallel subagents to understand:
   - NamespacedSharedStore implementation
   - All nodes using fallback pattern
   - Template resolution flow
   - Workflow input handling
   - PocketFlow's original design

2. **Key Discovery**: The fallback pattern was introduced in Task 11 (file nodes) with NO documented rationale. It predates templates and conflicts with namespacing (Task 9).

3. **Execution**: Parallel subagents for mechanical changes, manual review for complex parts.

## Files Modified/Created

### Core Changes

**Node Implementations (20 files)**
| Path | Parameters Changed |
|------|-------------------|
| `src/pflow/nodes/file/read_file.py` | file_path, encoding |
| `src/pflow/nodes/file/write_file.py` | content, file_path, encoding, content_is_binary |
| `src/pflow/nodes/file/copy_file.py` | source_path, dest_path |
| `src/pflow/nodes/file/move_file.py` | source_path, dest_path |
| `src/pflow/nodes/file/delete_file.py` | file_path |
| `src/pflow/nodes/git/*.py` (6 files) | branch, remote, working_directory, message, files, etc. |
| `src/pflow/nodes/github/*.py` (4 files) | repo, issue_number, title, body, head, base, state, limit, etc. |
| `src/pflow/nodes/http/http.py` | url, method, body, headers, params, timeout, auth_token, api_key |
| `src/pflow/nodes/llm/llm.py` | prompt, system, images |
| `src/pflow/nodes/shell/shell.py` | stdin |
| `src/pflow/nodes/claude/claude_code.py` | prompt, output_schema |
| `src/pflow/nodes/test/echo.py` | message, count, data |

**Interface Docstrings (23 files)**
Changed `- Reads: shared["key"]` to `- Params: key` format to accurately reflect new behavior.

**Documentation (9 files)**
- `src/pflow/nodes/CLAUDE.md` - New "Parameter-Only Pattern" section
- `architecture/core-concepts/shared-store.md` - Updated precedence rules
- `.taskmaster/knowledge/decisions.md` - Added architectural decision record

### Test Files

**New File Created:**
- `tests/test_runtime/test_namespace_collision_regression.py` - 15 critical regression tests

**Files Updated (~17):**
- Tests that put data in `shared` expecting nodes to read it
- Tests explicitly verifying "shared-first" behavior
- Integration tests across file/git/github/http/llm/shell/claude nodes

## Integration Points & Dependencies

### Incoming Dependencies
- **CLI** → Nodes via `compile_ir_to_flow()` - Passes initial_params which become template context
- **Planner** → Nodes via IR generation - Must generate templates for data flow
- **Template System** → Nodes via `TemplateAwareNodeWrapper` - Resolves `${var}` into params

### Outgoing Dependencies
- **Nodes** → `self.params` only - No more shared store reads for declared parameters
- **Nodes** → Shared store for outputs only - Write via `shared["key"] = value`

### Critical Coupling
**Template resolution MUST happen before node execution.**

The wrapper chain order is critical:
```
TemplateAwareNodeWrapper (resolves ${var} into params)
  → NamespacedNodeWrapper (isolates writes)
    → InstrumentedNodeWrapper (metrics/tracing)
      → ActualNode (reads from self.params)
```

## Architectural Decisions & Tradeoffs

### Key Decisions

| Decision | Reasoning | Alternative Rejected |
|----------|-----------|---------------------|
| Remove fallback entirely | Aligns with PocketFlow philosophy, templates handle wiring | Filter namespaces (heuristic-based, band-aid) |
| Params-only pattern | Explicit > implicit, no magic based on naming | Invert priority (still implicit fallback) |
| No collision detection | Unnecessary with params-only design | Would add complexity for no benefit |
| Update Interface docstrings | `Reads: shared["x"]` was now misleading | Leave as-is (confusing for future agents) |

### Technical Debt Incurred

None. This task reduced technical debt by:
1. Removing undocumented pattern that conflicted with namespacing
2. Aligning with PocketFlow's design philosophy
3. Making data flow explicit through templates

## Testing Implementation

### Test Strategy Applied

1. **Regression tests first**: Created tests for exact bugs before implementation
2. **Behavioral tests**: Verified new semantics (static params win, falsy values preserved)
3. **Integration verification**: Full test suite must pass

### Critical Test Cases

| Test | What It Catches |
|------|-----------------|
| `test_node_named_images_does_not_collide_with_llm_images_param` | The exact LLM + namespace dict bug |
| `test_input_named_url_does_not_override_http_url_template` | The exact HTTP + raw input bug |
| `test_static_url_param_not_overridden_by_input` | Static params respected (new behavior) |
| `test_falsy_values_not_overridden_by_shared_store` | `0`, `False`, `""` preserved (behavioral change) |

## Unexpected Discoveries

### Gotchas Encountered

1. **Interface docstrings were misleading** - Discovered mid-implementation that `- Reads: shared["key"]` format was now wrong. Required updating 23 files.

2. **More tests than expected** - Initial grep found ~3 test files. Actual count was ~17 because many integration tests put data in `shared` expecting nodes to read it.

3. **Three pattern variants existed**:
   - `shared.get("x") or params.get("x")` (most common)
   - `shared.get("x") if "x" in shared else params.get("x")` (HTTP, LLM)
   - `if "x" in shared: ... elif "x" in params: ...` (write_file content)

### Edge Cases Found

1. **Falsy values**: Old `or` pattern fell through on `0`, `False`, `""`. New pattern preserves them.
2. **None values**: `self.params.get("x")` returns `None` if not set, which is correct behavior.
3. **Required params**: Nodes should use `self.params["x"]` (KeyError) or explicit check for required params.

## Patterns Established

### Reusable Patterns

**The Params-Only Pattern (MANDATORY for all nodes):**
```python
# ✅ CORRECT - Read from params only
url = self.params.get("url")
timeout = self.params.get("timeout", 30)  # With default

# ❌ WRONG - Never use fallback
url = shared.get("url") or self.params.get("url")
```

**Template Wiring in IR:**
```json
{
  "inputs": {"api_url": {"type": "string"}},
  "nodes": [{
    "type": "http",
    "params": {"url": "${api_url}"}  // Explicit wiring via template
  }]
}
```

**Interface Docstring Format:**
```python
"""
Interface:
- Params: url: str  # API endpoint (required)
- Params: timeout: int  # Request timeout in seconds (default: 30)
- Writes: shared["response"]: str  # HTTP response body
"""
```

### Anti-Patterns to Avoid

1. **Never add shared store fallback** - The bug will return
2. **Never assume naming = wiring** - Use explicit templates
3. **Never use `Reads: shared["x"]` in docstrings** - Use `Params: x` format

## Breaking Changes

### Behavioral Changes

| Before | After |
|--------|-------|
| `shared["url"]` overrides `params["url"]` | `params["url"]` always used |
| Falsy values (`0`, `False`, `""`) fall through | Falsy values preserved |
| Naming collision = silent bug | Naming collision = no effect |

### No API Changes
External interfaces unchanged. Internal node behavior fixed.

## Future Considerations

### Extension Points

When creating new nodes:
1. Use `self.params.get("x")` pattern exclusively
2. Use `- Params:` format in Interface docstrings
3. Write to shared store for outputs only

### Potential Improvements

1. **Linter rule**: Detect `shared.get(...) or self.params.get(...)` pattern
2. **Template validation**: Could warn if param name matches node ID (educational, not blocking)

## AI Agent Guidance

### Quick Start for Related Tasks

**When implementing a new node:**
1. Read `src/pflow/nodes/CLAUDE.md` first (updated with params-only pattern)
2. Use `src/pflow/nodes/http/http.py` as reference implementation
3. Never read from shared store for declared parameters

**When debugging "wrong value" issues:**
1. Check if node ID matches a parameter name → Not a problem anymore
2. Check if workflow input matches a parameter name → Not a problem anymore
3. Check template syntax → `${var}` must be correctly formed

**Key files to understand the system:**
- `src/pflow/runtime/node_wrapper.py` - Template resolution
- `src/pflow/runtime/namespaced_store.py` - Namespace isolation
- `tests/test_runtime/test_namespace_collision_regression.py` - Critical regression tests

### Common Pitfalls

1. **Don't copy old node code blindly** - It might have the old fallback pattern
2. **Don't assume `shared["param"]` works** - Use templates in IR
3. **Don't forget Interface docstrings** - They must say `Params:` not `Reads: shared[...]`

### Test-First Recommendations

When modifying any node:
1. Run `pytest tests/test_runtime/test_namespace_collision_regression.py` first
2. Run the specific node's test file
3. Run full suite before committing

---

*Generated from implementation context of Task 102*

### Task 103

# Task 103 Review: Preserve Inline Object Type in Template Resolution

## Metadata

- **Implementation Date**: 2026-01-01
- **Branch**: `feat/inline-object-type`
- **Pull Request**: #32

## Executive Summary

Fixed the double-serialization bug where `{"key": "${dict_var}"}` produced `{"key": "{\"nested\": \"value\"}"}` instead of `{"key": {"nested": "value"}}`. Renamed `resolve_string` to `resolve_template` with type-preserving behavior for simple templates (`${var}`), while maintaining string interpolation for complex templates (`Hello ${name}`). This fix benefits ALL nodes automatically through the wrapper chain.

## Implementation Overview

### What Was Built

1. **Type-preserving template resolution**: Simple templates (`${var}`) now return the original value type (dict, list, int, bool, None). Complex templates (`"Hello ${name}"`) still return strings.

2. **Helper methods**: Added `is_simple_template()` and `extract_simple_template_var()` to TemplateResolver for reuse.

3. **Shared pattern constant**: Extracted `_VAR_NAME_PATTERN` to ensure consistency between `TEMPLATE_PATTERN` and `SIMPLE_TEMPLATE_PATTERN`.

4. **Refactored node_wrapper**: Uses shared helper instead of duplicating regex.

### Deviation from Original Spec

The spec suggested fixing only `resolve_nested()`. We chose a more comprehensive approach:
- Renamed `resolve_string` → `resolve_template` for API clarity
- Fixed `workflow_executor.py` too (nested workflow param mapping had same bug)
- Added strict pattern validation (PR review feedback)

## Files Modified/Created

### Core Changes

| File | Change |
|------|--------|
| `src/pflow/runtime/template_resolver.py` | Added `_VAR_NAME_PATTERN`, `SIMPLE_TEMPLATE_PATTERN`, `is_simple_template()`, `extract_simple_template_var()`. Renamed `resolve_string` → `resolve_template` with type preservation. Updated `resolve_nested` to call new method. |
| `src/pflow/runtime/node_wrapper.py` | `_resolve_simple_template()` now uses `TemplateResolver.extract_simple_template_var()`. Changed call from `resolve_string` to `resolve_template`. |
| `src/pflow/runtime/workflow_executor.py` | Changed `resolve_string` → `resolve_template` for nested workflow param mapping. |

### Test Files

| File | Change | Priority |
|------|--------|----------|
| `tests/test_runtime/test_template_type_preservation.py` | **NEW** - 13 tests for type preservation behavior | Critical |
| `tests/test_integration/test_shell_stdin_type_preservation.py` | **NEW** - 4 integration tests for shell stdin | Critical |
| `tests/test_runtime/test_template_resolver.py` | Updated 22 calls to `resolve_template` | High |
| `tests/test_runtime/test_node_wrapper_nested_resolution.py` | Fixed 2 assertions expecting old stringified behavior | High |
| `tests/test_nodes/test_mcp/test_json_text_parsing.py` | Rewrote tests for new type-preserving behavior | Medium |
| `tests/test_runtime/test_template_resolver_arrays.py` | Updated 6 calls | Low |
| `tests/test_runtime/test_template_resolver_nested.py` | Updated 2 calls | Low |
| `tests/test_runtime/test_template_array_notation.py` | Updated 1 call | Low |

## Integration Points & Dependencies

### Incoming Dependencies

All nodes benefit automatically:
```
TemplateAwareNodeWrapper
    └── calls TemplateResolver.resolve_nested()
        └── calls TemplateResolver.resolve_template()
            └── Type preserved for simple templates
```

Specific nodes affected:
- **Shell node**: `stdin` can now be `{"a": "${data-a}", "b": "${data-b}"}` without double-encoding
- **HTTP node**: `body`, `headers`, `params` preserve types
- **MCP node**: Dynamic tool arguments preserve types
- **LLM node**: Structured params preserve types
- **All other nodes**: Same benefit through wrapper chain

### Outgoing Dependencies

```
resolve_template() depends on:
├── extract_simple_template_var() - Pattern detection
├── variable_exists() - Check if var is in context
├── resolve_value() - Get the actual value (any type)
└── _convert_to_string() - Only for complex templates
```

### Shared Store Keys

No new shared store keys. Template resolution reads from:
- `shared` store (runtime data)
- `initial_params` (planner parameters)

## Architectural Decisions & Tradeoffs

### Key Decisions

| Decision | Reasoning | Alternative Considered |
|----------|-----------|----------------------|
| Rename `resolve_string` → `resolve_template` | Name should reflect return type (`Any`, not `str`) | Keep name, change behavior (confusing API) |
| Delete `resolve_string` entirely | No backward compat needed per user | Keep as deprecated alias (adds noise) |
| Extract `_VAR_NAME_PATTERN` | Single source of truth for valid variable pattern | Duplicate regex in both patterns (drift risk) |
| Fix in `TemplateResolver`, not `node_wrapper` | Fixes all callers automatically, single location | Fix in each caller (scattered, error-prone) |

### Technical Debt Incurred

None significant. The implementation is cleaner than before:
- Eliminated duplicate regex in node_wrapper
- Consolidated detection logic in TemplateResolver
- Stricter validation after PR review

## Testing Implementation

### Test Strategy Applied

1. **Unit tests**: Direct testing of `resolve_template()` behavior
2. **Integration tests**: Shell node stdin with real workflow execution
3. **Edge case tests**: Pattern boundary (simple vs complex vs invalid)
4. **Regression tests**: Updated existing tests expecting old behavior

### Critical Test Cases

| Test | What It Validates |
|------|-------------------|
| `test_inline_object_preserves_dict_type` | THE primary bug fix - `{"key": "${dict}"}` preserves inner dict |
| `test_shell_stdin_inline_object_not_double_encoded` | End-to-end with real shell execution |
| `test_complex_template_in_stdin_still_stringifies` | `"Hello ${name}"` still returns string |
| `test_invalid_variable_names_not_simple` | `${123}`, `${ var }` correctly rejected |

## Unexpected Discoveries

### Gotchas Encountered

1. **Tests expecting old behavior**: Two tests in `test_node_wrapper_nested_resolution.py` explicitly tested the broken behavior with comments like "Template resolution converts numbers to strings". Had to update these to expect correct behavior.

2. **Original SIMPLE_TEMPLATE_PATTERN too permissive**: Initial pattern `r"^\$\{([^}]+)\}$"` matched invalid names like `${123}`, `${ var }`. Fixed by extracting shared pattern from TEMPLATE_PATTERN.

3. **workflow_executor.py also affected**: Nested workflow parameter mapping had the same bug. Would have been missed if we only fixed `resolve_nested()`.

### Edge Cases Found

| Edge Case | Behavior |
|-----------|----------|
| `${missing}` (unresolved) | Stays as `${missing}` in output |
| `${data.field}` (path) | Simple template - preserves type |
| `${items[0]}` (array index) | Simple template - preserves type |
| `${a}${b}` (adjacent) | Complex template - returns string |
| ` ${var}` (leading space) | Complex template - returns string |
| `$${var}` (escaped) | Not a template - literal `${var}` |

## Patterns Established

### Reusable Patterns

**Simple vs Complex Template Detection**:
```python
# Simple template pattern - preserves type
SIMPLE_TEMPLATE_PATTERN = re.compile(rf"^\$\{{({_VAR_NAME_PATTERN})\}}$")

# Usage:
if TemplateResolver.is_simple_template(value):
    return TemplateResolver.resolve_value(var_name, context)  # Type preserved
else:
    return TemplateResolver.resolve_string(value, context)  # String interpolation
```

**Shared Pattern Extraction**:
```python
# Extract reusable pattern for DRY
_VAR_NAME_PATTERN = r"[a-zA-Z_][\w-]*(?:(?:\[\d+\])?(?:\.[a-zA-Z_][\w-]*(?:\[\d+\])?)*)?"

# Use in multiple regex
TEMPLATE_PATTERN = re.compile(rf"(?<!\$)\$\{{({_VAR_NAME_PATTERN})\}}")
SIMPLE_TEMPLATE_PATTERN = re.compile(rf"^\$\{{({_VAR_NAME_PATTERN})\}}$")
```

### Anti-Patterns to Avoid

1. **Don't duplicate regex patterns**: If you need to detect simple templates elsewhere, use `TemplateResolver.is_simple_template()`, don't copy the regex.

2. **Don't call `_convert_to_string()` for simple templates**: The whole point is to preserve types. Only use it for complex templates.

3. **Don't assume `resolve_template()` returns string**: It returns `Any` now. Check `is_simple_template()` if you need to know the return type.

## Breaking Changes

### API/Interface Changes

| Change | Migration |
|--------|-----------|
| `resolve_string()` removed | Use `resolve_template()` instead |
| Return type of template resolution | Now `Any` instead of always `str` |

### Behavioral Changes

| Before | After |
|--------|-------|
| `resolve_nested({"key": "${dict}"})` → `{"key": "{\"nested\":\"value\"}"}` | `{"key": {"nested": "value"}}` |
| Simple templates in nested structures stringified | Type preserved |
| `int` in nested template became `"42"` | Stays `42` |
| `bool` in nested template became `"True"` | Stays `True` |

## Future Considerations

### Extension Points

- **Type coercion**: If a node expects string but receives dict from simple template, the JSON auto-parsing in node_wrapper handles this. Could be extended for other coercions.

- **Custom serialization**: If someone needs the old behavior (stringify simple templates), they could add a `force_string=True` parameter to `resolve_template()`.

### Scalability Concerns

None. Template resolution is O(n) with template count, unchanged from before.

## AI Agent Guidance

### Quick Start for Related Tasks

1. **Read first**:
   - `src/pflow/runtime/template_resolver.py:336-434` - The `resolve_template()` method
   - `tests/test_runtime/test_template_type_preservation.py` - All behavior documented in tests

2. **Key insight**: Simple template = entire string is `${var}` → type preserved. Anything else → string.

3. **Pattern to follow**: Use `TemplateResolver.is_simple_template()` and `extract_simple_template_var()` for any new detection logic.

### Common Pitfalls

1. **Don't assume string return**: `resolve_template("${data}", ctx)` might return a dict. Check before string operations.

2. **Pattern consistency**: If you modify `_VAR_NAME_PATTERN`, both `TEMPLATE_PATTERN` and `SIMPLE_TEMPLATE_PATTERN` change. Test both.

3. **Test the boundary**: When adding template features, test both `${var}` (simple) and `"prefix ${var}"` (complex) cases.

### Test-First Recommendations

When modifying template resolution:
1. Run `test_template_type_preservation.py` first - covers core behavior
2. Run `test_shell_stdin_type_preservation.py` - integration test
3. Run full `tests/test_runtime/test_template_*.py` - all template tests

```bash
uv run pytest tests/test_runtime/test_template_type_preservation.py tests/test_integration/test_shell_stdin_type_preservation.py -v
```

---

*Generated from implementation context of Task 103*

### Task 105

# Task 105 Review: Auto-Parse JSON Strings During Nested Template Access

## Metadata
- **Implementation Date**: 2026-01-01
- **Branch**: `feat/auto-parse-json`
- **Pull Request**: https://github.com/spinje/pflow/pull/37

## Executive Summary

Implemented automatic JSON parsing during template path traversal, enabling `${node.stdout.field}` when stdout contains a JSON string. This required coordinating two systems (compile-time validator + runtime resolver) and establishing a shared JSON utility that consolidated 7 duplicate implementations.

## Implementation Overview

### What Was Built

1. **Core Feature**: JSON auto-parsing in `template_resolver.py` during path traversal
2. **Shared Utility**: `json_utils.py` with `try_parse_json()` returning `(success, result)` tuple
3. **Validator Relaxation**: Allow nested access on `str` types with compile-time warning
4. **Error Message Improvements**: JSON-specific hints and better path-aware suggestions

### Key Deviations from Original Plan

1. **Phase 4 was not in original plan** - Discovered validator/resolver mismatch during testing. Had to relax validator to allow `str` nested access.
2. **Error message improvements added post-review** - Code review feedback led to JSON-specific runtime hints and improved suggestion logic.
3. **Removed low-value tests** - Deleted 6 tests, replaced with 1 high-value behavioral test. Tests should verify behavior, not implementation.

## Files Modified/Created

### Core Changes

| File | Change | Impact |
|------|--------|--------|
| `src/pflow/core/json_utils.py` | **NEW** - Shared JSON parsing utility | Eliminates 7 duplicates, single source of truth |
| `src/pflow/runtime/template_resolver.py` | Added `_try_parse_json_for_traversal()` and `_get_dict_value()` helpers | Core feature - parses JSON during path traversal |
| `src/pflow/runtime/template_validator.py` | Added `_check_type_allows_traversal()`, `_strip_array_indices()` | Allows `str` nested access with warning |
| `src/pflow/runtime/node_wrapper.py` | Added `_detect_json_parse_hints()`, improved `_generate_suggestions()` | Better error messages for JSON failures |
| `src/pflow/runtime/batch_node.py` | Use `try_parse_json()` | Consolidation |

### Test Files

| File | Purpose | Critical? |
|------|---------|-----------|
| `tests/test_core/test_json_utils.py` | Utility tests (16 tests) | YES - validates core parsing logic |
| `tests/test_runtime/test_template_resolver_json_parsing.py` | Feature tests (21 tests) | YES - especially `TestValidationConsistency` |
| `tests/test_integration/test_json_nested_access_e2e.py` | E2E workflow tests (9 tests) | YES - proves feature works end-to-end |
| `tests/test_runtime/test_template_validator_warnings.py` | Warning behavior tests (8 tests) | MEDIUM - validates compile-time warnings |

## Integration Points & Dependencies

### Critical Integration: Validator ↔ Resolver

```
Compile Time                          Runtime
     │                                    │
     ▼                                    ▼
┌─────────────────┐              ┌─────────────────┐
│ TemplateValidator│              │ TemplateResolver │
│                 │              │                 │
│ Sees: stdout:str│              │ Sees: stdout=   │
│ Decision: ALLOW │──────────────│ '{"field":"v"}' │
│ (with warning)  │  Must agree  │ Action: PARSE   │
└─────────────────┘              └─────────────────┘
```

**Lesson**: When adding runtime "magic", always check if compile-time validation needs updating. These systems MUST agree on what's valid.

### Shared Store Keys

No new shared store keys. Feature operates on existing node output keys.

### Outgoing Dependencies

- `json_utils.try_parse_json()` is now used by:
  - `template_resolver.py` (path traversal)
  - `node_wrapper.py` (target-side auto-parse)
  - `batch_node.py` (items parsing)

## Architectural Decisions & Tradeoffs

### Decision 1: Two-Value Return Pattern

**Choice**: `try_parse_json()` returns `(success: bool, result: Any)`

**Reasoning**: Distinguishes three cases:
- `(True, {"key": "value"})` - Parsed successfully to dict
- `(True, None)` - Parsed successfully to JSON null
- `(False, "original string")` - Parse failed

**Alternative considered**: Return parsed value or original. Rejected because can't distinguish "parsed to None" from "failed to parse".

### Decision 2: Parse Only During Traversal

**Choice**: `${node.stdout}` returns raw string, `${node.stdout.field}` triggers parsing

**Reasoning**: Backward compatible. Existing workflows expecting raw strings continue to work.

**Alternative considered**: Always parse if valid JSON. Rejected because changes behavior for existing workflows.

### Decision 3: Warning for `str`, Not `any`

**Choice**: Only `str` type generates compile-time warning for nested access

**Reasoning**:
- `dict`/`object`: Trusted structured data, no warning
- `any`: Node author explicitly declared "could be anything", no warning
- `str`: JSON auto-parsing is implicit/surprising, warn user

### Technical Debt Incurred

1. **No caching of parsed JSON** - Same string parsed multiple times if accessed with different paths. Acceptable for MVP (parsing is <1ms vs node execution 100-1000ms).
2. **Display layer doesn't deduplicate warnings** - Multiple templates on same output generate multiple warnings. Could group in future.

## Testing Implementation

### Test Strategy Applied

**Principle**: Test behavior, not implementation. Focus on:
1. Does the feature work? (E2E tests)
2. Do the edge cases behave correctly? (Unit tests)
3. Do the systems agree? (Validation consistency tests)

### Critical Test Cases

| Test | What It Validates | Why Critical |
|------|-------------------|--------------|
| `TestValidationConsistency.test_exists_agrees_with_resolve_for_valid_json` | `variable_exists()` and `resolve_value()` agree | Prevents validator/resolver mismatch |
| `TestJsonNestedAccessE2E.test_shell_json_output_nested_access` | Original feature request scenario | Proves feature works |
| `TestRecursiveJsonParsing.test_nested_json_string_is_parsed_at_each_level` | JSON-in-JSON works | Edge case users will hit |
| `test_distinguishes_parsed_none_from_parse_failure` | Two-value return correctness | API contract |

## Unexpected Discoveries

### Gotcha 1: Validator Blocks Valid Runtime Patterns

**Discovery**: E2E tests failed because validator saw `stdout: str` and rejected nested access.

**Solution**: Relaxed validator to allow `str` nested access with warning.

**Impact**: Created new coordination requirement between validator and resolver.

### Gotcha 2: Array Access on JSON Strings

**Discovery**: `${node.stdout[0].field}` requires special handling in validator.

**Solution**: Added `_strip_array_indices()` to extract base key before validation.

### Gotcha 3: Error Message Suggestions Were Wrong

**Discovery**: For `${mynode.stdout}` (typo), suggestion was `${my-node}` not `${my-node.stdout}`.

**Solution**: Updated `_generate_suggestions()` to preserve full path when suggesting corrections.

## Patterns Established

### Pattern 1: Two-Value Return for Parse Operations

```python
def try_parse_json(value: str) -> tuple[bool, Any]:
    """Return (success, result) to distinguish parse-to-None from failure."""
    try:
        return (True, json.loads(value))
    except (json.JSONDecodeError, ValueError):
        return (False, value)
```

**When to use**: Any parse operation where the result could legitimately be None/empty.

### Pattern 2: Helper Methods for Dual-Path Consistency

```python
# In template_resolver.py
@staticmethod
def _get_dict_value(value: Any, key: str) -> tuple[bool, Any]:
    """Shared logic for resolve_value() and _traverse_path_part()."""
```

**When to use**: When two code paths must behave identically. Extract shared logic.

### Pattern 3: Actionable Warning Messages

```python
reason=(
    f"Nested access on '{output_type}' requires valid JSON at runtime. "
    f"Non-JSON strings cause 'Unresolved variables' error."
)
```

**Structure**: [What's required] + [What happens if violated]

### Anti-Pattern: Testing Implementation Details

**Don't**: Test that a specific function is called with specific arguments.
**Do**: Test that the observable behavior is correct.

Example: Don't test "JSON parsing was attempted". Test "nested access resolves to expected value".

## Breaking Changes

None. Feature is purely additive:
- Previously failing patterns (`${node.stdout.field}` on JSON string) now work
- Previously working patterns unchanged

## AI Agent Guidance

### Quick Start for Related Tasks

1. **Read first**:
   - `src/pflow/runtime/template_resolver.py` - `_try_parse_json_for_traversal()`
   - `src/pflow/runtime/template_validator.py` - `_check_type_allows_traversal()`

2. **Key insight**: Validator and resolver must agree. If you change one, check the other.

3. **Test pattern**: Always include a test that verifies `variable_exists()` agrees with `resolve_value()`.

### Common Pitfalls

1. **Don't forget the validator** - If you add runtime behavior for a type, the validator might block it at compile time.

2. **Two-value return matters** - Don't change `try_parse_json()` to return parsed-or-original. The tuple is intentional.

3. **Preserve full path in errors** - When generating suggestions for `${node.output.field}`, suggest `${other-node.output.field}`, not just `${other-node}`.

### Test-First Recommendations

When modifying template resolution:
1. Run `tests/test_runtime/test_template_resolver_json_parsing.py` - Core feature
2. Run `tests/test_runtime/test_template_validator_warnings.py` - Warning behavior
3. Run `tests/test_integration/test_json_nested_access_e2e.py` - Full flow

---

*Generated from implementation context of Task 105*

---

## Documentation Changes

## docs/CLAUDE.md
Commits: 7bebe4b docs: update CLAUDE.md to specify linking to architecture docs in pflow repo,5491e63 docs: restructure documentation with "How it works" tab for technical content
Changes:
-1. **Verify the command exists** - Run `pflow --help` or check `src/pflow/cli/`
+1. **Verify the command exists** - Run `pflow --help` or check `src/pflow/cli/` or `src/pflow/mcp_server/tools/`
-4. **Check current usage patterns** - Run `pflow instructions usage` to see what we tell agents
+4. **Check current usage patterns** - Run `pflow instructions usage` to see what we tell agents.
+├── how-it-works/                # Technical deep-dives for curious users
-The docs have three main tabs:
+The docs have four main tabs:
+- **How it works** - Technical deep-dives for curious users who want to understand internals
+> **Critical perspective**: Just because you have implementation details in your context doesn't mean they're relevant to pflow users. Put yourself in their shoes before writing - most users just want their AI agent to accomplish tasks, not understand how pflow works internally.
+
-**For implementation details**: Link to `architecture/` docs, don't duplicate.
+**For implementation details**: Link to `architecture/` docs in pflow repo if not present in `how-it-works/`, don't duplicate.
+
+**For technical deep-dives**: Use the "How it works" tab. Keep Reference and Guides focused on practical usage - save detailed explanations of internals, design decisions, and "why it works this way" for the "How it works" section. Use accordions in Reference/Guides only for truly helpful context, not to dump technical details.
+### Setting expectations
+
+Use Info or Note callouts to clarify who does what (pattern from CLI reference):
+
+```mdx
+<Info>
+  **Your agent handles this.** [Brief explanation of what users see/experience]
+</Info>
+```
+
+This helps users understand they don't need to memorize technical details - their agent does the work.
+

## docs/docs.json
Commits: 5491e63 docs: restructure documentation with "How it works" tab for technical content
Changes:
+      {
+        "tab": "How it works",
+        "icon": "lightbulb",
+        "pages": [
+          "how-it-works/template-variables",
+          "how-it-works/batch-processing"
+        ]
+      },

## docs/guides/debugging.mdx
Commits: 3813eab docs: add deprecation warnings for planner and auto-repair features
Changes:
-<Note>
-pflow also has experimental features like a built-in planner and auto-repair. See [Experimental features](/reference/experimental) for details.
-</Note>
-

## docs/how-it-works/batch-processing.mdx
Commits: 19604c7 fix: address code review feedback for batch item feature,928111c feat: include original item in batch results for self-contained downstream processing 5491e63 docs: restructure documentation with "How it works" tab for technical content
Changes:
+---
+title: "Batch processing"
+description: "Process arrays of items through workflows"
+icon: "list"
+---
+
+<Info>
+  **For the curious.** Your AI agent configures batch processing when needed. This explains what happens when you ask to process many items (files, API results, etc.) and what to expect during execution.
+</Info>
+
+Batch processing is how pflow runs a single node multiple times - once for each item in an array - with isolated contexts and automatic error handling. Think of it like a for-loop in your workflow.
+
+## When batch processing happens
+
+Your agent uses batch processing when tasks involve:
+- Processing each file in a directory listing
+- Analyzing each item from an API response
+- Running the same LLM prompt on multiple inputs
+- Transforming each element in an array
+
+**Example scenario:** When you ask to classify 100 GitHub issues, your agent configures a batch node to process each issue.
+
+## How it works
+
+A `batch` configuration is added to a node:
+
+```json
+{
+  "nodes": [
+    {
+      "id": "list_issues",
+      "type": "http",
+      "params": {
+        "url": "https://api.github.com/repos/owner/repo/issues"
+      }
+    },
+    {
+      "id": "classify",
+      "type": "llm",
+      "batch": {
+        "items": "${list_issues.response}",
+        "as": "issue"
+      },
+      "params": {
+        "prompt": "Classify this issue: ${issue.title}"
+      }
+    }
+  ]
+}
+```

## docs/how-it-works/template-variables.mdx
Commits: 5491e63 docs: restructure documentation with "How it works" tab for technical content
Changes:
+---
+title: "Template variables"
+description: "Dynamic data flow in workflows"
+icon: "braces"
+---
+
+<Info>
+  **For the curious.** Your AI agent handles template variables automatically. This explains how data flows between nodes and what's happening when you see `${variable}` syntax in workflows or traces.
+</Info>
+
+Template variables are how pflow connects data between nodes in workflows. When you see `${variable}` syntax, it's pulling data from previous nodes, workflow inputs, or nested structures.
+
+## Basic syntax
+
+The `${variable}` syntax accesses values from the shared store:
+
+```json
+{
+  "id": "summarize",
+  "type": "llm",
+  "params": {
+    "prompt": "${read.content}"
+  }
+}
+```
+
+Here, `${read.content}` pulls the `content` output from the `read` node.
+
+## Nested access
+
+Template variables can traverse deeply nested structures:
+
+```json
+{
+  "prompt": "${api.response.items[0].name}"
+}
+```
+
+This traverses:
+1. `api` node's output
+2. `response` key
+3. `items` array
+4. First element (`[0]`)
+5. `name` field
+
+## Type preservation
+
+<Note>
+  Template variables preserve the original data type when used alone. When combined with text, they become strings.
+</Note>

## docs/reference/cli/index.mdx
Commits: 3813eab docs: add deprecation warnings for planner and auto-repair features
Changes:
-| `--trace-planner` | Save planner execution trace |
-| `--auto-repair` | Enable automatic workflow repair on failure |
+  | `--trace-planner` | Save planner execution trace |
+  | `--auto-repair` | Enable automatic workflow repair on failure |
+
+  <Note>
+    This is not the recommended way to use pflow. It may be deprecated in favor of using pflow through AI agents via the CLI or MCP server.
+  </Note>
-- **Planner traces**: `planner-trace-{timestamp}.json` (with `--trace-planner`)

## docs/reference/cli/registry.mdx
Commits: cad506b fix: make --output-format json work for registry run,8161282 fix: structure output mode skips LLM filtering, add Gemini thinking optimizations
Changes:
-- `--output-format text|json` - Output format (default: text)
+- `--output-format json` - Raw JSON output, bypasses structure mode
-| `smart` (default) | Shows template paths with values, truncates large data | General debugging and testing |
-| `structure` | Shows template paths only, no values | When output contains sensitive data |
-| `full` | Shows all fields with full values, no truncation | Manual inspection only (not recommended for agents) |
+| `smart` (default) | Shows template paths with values. Filters large outputs (>30 fields) using LLM. | General debugging and AI agent workflows |
+| `structure` | Shows template paths only, no values. No filtering - fast, no LLM overhead. | Sensitive data, quick exploration |
+| `full` | Shows all fields with full values, no truncation or filtering | Manual inspection only (not recommended for agents) |
-Shows template paths with actual values. Large values are truncated to keep output readable:
+Shows template paths with actual values. Large values are truncated to keep output readable. For outputs with more than 30 fields, uses an LLM to intelligently filter to the most relevant fields:
-Shows only template paths without values - useful when outputs contain sensitive data:
+Shows only template paths without values - useful when outputs contain sensitive data or when you want fast output without LLM overhead. Unlike smart mode, structure mode shows all fields without filtering:
-**Text mode (`--output-format text`):**
+### Raw JSON output
-Shows full output values in human-readable format:
+Use `--output-format json` to bypass structure mode and get raw, machine-parseable output:
+```bash
+pflow registry run read-file file_path=/tmp/test.txt --output-format json
-✓ Node executed successfully
-
-content: "Hello world..."
-metadata:
-  size: 123
-  created: "2024-01-15"
-
-Execution time: 45ms
-```
-
-**JSON mode (`--output-format json`):**
-
-Returns structured JSON for programmatic use:
+  "node_type": "read-file",
+<Note>
+  Agents always use structure mode for template path discovery. The `--output-format json` flag is for debugging and scripting when you need raw output values.
+</Note>
+

## docs/reference/cli/settings.mdx
Commits: 4a10115 fix: address review feedback for smart filter optimizations [skip review],8161282 fix: structure output mode skips LLM filtering, add Gemini thinking optimizations
Changes:
-Set the model for smart field filtering (structure-only mode with large responses).
+Set the model for smart field filtering (used when `smart` output mode filters large API responses).
-# Use a fast model for filtering
-pflow settings llm set-filtering gemini-3-flash-preview
+# Use a fast, cheap model for filtering
+pflow settings llm set-filtering gemini-2.5-flash-lite
+<Tip>
+  Smart filtering is a simple task - use a fast, cheap model.
+
+  **Recommended (fast + cheap):**
+
+  | Provider | Model | Overhead | Notes |
+  |----------|-------|----------|-------|
+  | Google | `gemini-2.5-flash-lite` | ~2-3s | Best budget option |
+  | OpenAI | `gpt-5-mini` | ~1-2s | Runner-up budget |
+  | Anthropic | `claude-haiku-4-5` | ~1-2s | Third place budget |
+
+  **Alternative (higher cost):**
+
+  | Provider | Model | Overhead | Notes |
+  |----------|-------|----------|-------|
+  | Anthropic | `claude-sonnet-4-5` | ~1-2s | Best premium option |
+  | Google | `gemini-3-flash-preview` | ~2-3s | Runner-up premium |
+  | OpenAI | `gpt-5.2` | ~5-6s | Slower, not recommended |
+
+  All models produce equivalent quality for this task. Timings are approximate and vary with network latency.
+</Tip>
+
-| `smart` (default) | Shows template paths with values, truncates large data |
-| `structure` | Shows template paths only, no values |
-| `full` | Shows all fields with full values, no truncation |
+| `smart` (default) | Shows template paths with values. Uses LLM to filter large outputs (>30 fields) to relevant fields. |
+| `structure` | Shows template paths only (no values). No filtering - shows all fields. Fast, no LLM overhead. |
+| `full` | Shows all fields with full values, no truncation or filtering. |
-  Use `structure` mode when working with sensitive data. Avoid `full` mode with AI agents - it bypasses all truncation and can consume excessive tokens.
+  Use `structure` mode when working with sensitive data or when you want fast output without LLM overhead. Avoid `full` mode with AI agents - it shows all values without truncation and can consume excessive tokens.

## docs/reference/experimental.mdx
Commits: 3813eab docs: add deprecation warnings for planner and auto-repair features
Changes:
-These features are available in pflow but considered experimental. They may change, have rough edges, or be replaced in future versions. Use them if they solve a problem for you, but be aware of the caveats.
+These features are available in pflow but considered experimental and/or may be deprecated in the future. They may change, have rough edges, or be replaced in future versions. Use them if they solve a problem for you, but be aware of the caveats.
-
-This is **experimental**. Using an external agent (Claude Code, Cursor, etc.) is the recommended approach - it's more capable and provides a better experience.
+<Note>
+  This is **experimental**. Using an external agent (Claude Code, Cursor, etc.) is the recommended approach - it's more capable and provides a better experience. Using your own agents also has the benefit of using a payed subscription instead of paying for every LLM call using API pricing.
+</Note>

## docs/reference/nodes/claude-code.mdx
Commits: badf76a fix(claude-code): update default model and fix documentation URLs [skip review],8f84825 feat(claude-code): migrate to Claude Agent SDK and streamline parameters
Changes:
-### Task parameters
+### Prompt parameters
-| `task` | str | Yes | - | Task description (max 10,000 chars) |
-| `context` | str/dict | No | - | Additional context for the task |
+| `prompt` | str | Yes | - | The prompt to send to Claude (max 10,000 chars) |
-| `working_directory` | str | No | Current dir | Project root directory |
+| `cwd` | str | No | Current dir | Working directory for Claude |
-| `allowed_tools` | list | No | `["Read", "Write", "Edit", "Bash"]` | Permitted tools |
+| `allowed_tools` | list | No | All tools | Permitted tools |
+| `timeout` | int | No | `300` | Execution timeout in seconds (30-3600) |
+| `resume` | str | No | - | Session ID to resume a previous conversation |
+| `sandbox` | dict | No | - | Sandbox configuration for command isolation |
+
+### Sandbox configuration
+
+The `sandbox` parameter controls command execution isolation. See the [Claude Agent SDK sandbox documentation](https://platform.claude.com/docs/en/agent-sdk/python#sandbox-configuration) for full details.
+
+| Key | Type | Description |
+|-----|------|-------------|
+| `enabled` | bool | Enable sandbox mode |
+| `autoAllowBashIfSandboxed` | bool | Auto-allow bash when sandboxed |
+| `excludedCommands` | list | Commands that bypass sandbox (e.g., `["docker"]`) |
+| `allowUnsandboxedCommands` | bool | Allow model to request unsandboxed execution |
+| `network` | dict | Network settings (`allowLocalBinding`, `allowUnixSockets`) |
-| `llm_usage` | dict | Token usage metrics |
-  "duration_api_ms": 2800,
-  "num_turns": 2,
-    "task": "Review this code for security issues",
-    "context": "${code.content}",
+    "prompt": "Review this code for security issues: ${code.content}",
-        "task": "Write a Python function to calculate Fibonacci numbers",
+        "prompt": "Write a Python function to calculate Fibonacci numbers",
-        "task": "Review this authentication code",
-        "context": "${read.content}",
+        "prompt": "Review this authentication code: ${read.content}",
-### File modification
+### File modification with sandbox
-        "task": "Refactor the User class to use dataclasses",
-        "working_directory": "/path/to/project",
-        "allowed_tools": ["Read", "Write", "Edit"]
+        "prompt": "Refactor the User class to use dataclasses",
+        "cwd": "/path/to/project",
+        "allowed_tools": ["Read", "Write", "Edit"],
+        "sandbox": {
+          "enabled": true,
+          "autoAllowBashIfSandboxed": true,
+          "excludedCommands": ["docker"]
+        }
+| `Task` | Spawn subagents |
+| `Glob` | Find files by pattern |

## docs/reference/nodes/file.mdx
Commits: 5491e63 docs: restructure documentation with "How it works" tab for technical content
Changes:
-  The `confirm_delete` parameter must come from the shared store, not node parameters. This safety mechanism prevents accidental deletions from workflow configuration.
+  **Security exception**: The `confirm_delete` parameter must be set in the shared store (via a previous node or template variable). It cannot be provided directly in node parameters. This prevents accidental deletions from workflow configuration files.

## docs/reference/nodes/index.mdx
Commits: 5491e63 docs: restructure documentation with "How it works" tab for technical content
Changes:
-1. **Read inputs** from the shared store or node parameters
+1. **Read inputs** from node parameters
-Nodes receive data in two ways:
+Nodes receive data through parameters. Parameters can be static values or template variables that pull from the shared store:
-| Source | When to use | Example |
-|--------|-------------|---------|
-| **Parameters** | Fixed values set when building the workflow | `"model": "gpt-5.2"` |
-| **Shared store** | Dynamic values from previous nodes | `"prompt": "${summarize.response}"` |
+| Type | When to use | Example |
+|------|-------------|---------|
+| **Static parameters** | Fixed values set when building the workflow | `"model": "gpt-4"` |
+| **Template variables** | Dynamic values from previous nodes or workflow inputs | `"prompt": "${summarize.response}"` |
-Template variables like `${node_id.key}` pull data from the shared store at runtime. You can access nested fields and array elements directly: `${api.response.items[0].name}`.
+Template variables like `${node_id.key}` are resolved at runtime from the shared store and injected into node parameters. You can access nested fields and array elements directly: `${api.response.items[0].name}`.
+
+Nodes write their outputs to the shared store, making them available for template variables in subsequent nodes. See [Template variables](/reference/template-variables) for complete syntax and examples.

## docs/reference/nodes/shell.mdx
Commits: 5491e63 docs: restructure documentation with "How it works" tab for technical content
Changes:
+## Validation and error handling
+
+<Info>
+  **Your agent handles this.** pflow validates shell commands at workflow creation time. If you see errors about structured data in commands, the error message will explain how to fix it - typically by moving data to the `stdin` parameter.
+</Info>
+
+The shell node validates that dicts and lists aren't embedded directly in command strings (which would break shell parsing). Data should go through `stdin` instead:
+
+```json
+{
+  "stdin": "${api.response}",
+  "command": "jq -r '.data.name'"
+}
+```
+
+<Accordion title="Technical details">
+  When a workflow is created, pflow checks if command templates contain dict or list variables. If found, you'll see an error like:
+
+  ```
+  Shell node 'process': cannot use ${api.response} (type: object) in command parameter.
+
+  PROBLEM: Object data embedded in shell commands breaks shell parsing. Dict/list
+  data contains special characters (quotes, braces, spaces) that break command syntax.
+
+  FIX: Move data to stdin, keep command simple:
+    {"stdin": "${api.response}", "command": "jq '.field'"}
+  ```
+
+  This validation happens at workflow creation time (compile-time), not during execution, so you get immediate feedback.
+
+  **Why this matters:** Shell parsers expect text, and JSON data contains special characters (`{`, `}`, `"`, spaces) that have meaning to the shell. Even with careful quoting, it's fragile. The `stdin` approach is safer and more reliable.
+</Accordion>
+

## docs/roadmap.mdx
Commits: 3813eab docs: add deprecation warnings for planner and auto-repair features,fc2dd3c documentation updates and cleanup [skip review]
Changes:
+- **Batch processing** — process arrays of items through a single node (sequential or parallel)
-- Parallel execution — run independent nodes concurrently
-- Nested workflow support in the planner
+- Task parallelism — run independent nodes concurrently (fan-out/fan-in)
+- Nested workflow support

---

## Draft Entries with Context

```json
[
  {
    "draft": "Fixed validation failures by normalizing type strings to lowercase across the registry and nodes",
    "index": 4,
    "context": {
      "commit_message": "fix: normalize type strings to lowercase to prevent validation failures",
      "task_number": null,
      "files_changed": "src/pflow/mcp/registrar.py, src/pflow/nodes/llm/llm.py, src/pflow/registry/metadata_extractor.py, tests/test_planning/test_ir_models.py, tests/test_planning/test_registry_helper.py ... and 3 more files",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Fixed shell commands to allow union types containing str or any [#61](https://github.com/spinje/pflow/pull/61)",
    "index": 7,
    "context": {
      "commit_message": "Merge pull request #61 from spinje/fix/shell-union-type-validation",
      "task_number": null,
      "files_changed": "src/pflow/runtime/template_validator.py, tests/test_runtime/test_template_validator_types.py",
      "pr_title": "fix: allow shell commands to use union types containing str/any (fixes #60)",
      "pr_body": "## Summary\n\nThe shell command type validator was blocking `dict|str` unions even though runtime coercion handles dict → JSON string correctly. This caused unnecessary friction when using HTTP responses in shell commands.\n\nFixes #60\n\n## Changes\n\n### Three-tier validation system:\n\n1. **Tier 1 (auto-allow)**: Unions containing safe types (`str`, `string`, `any`) are now allowed automatically\n2. **Tier 2 (quote escape)**: `'${var}'` syntax allows explicit opt-in for pure structured types\n3. **Bug fix**: Extract base types from generics (`list[dict]` → `list`) before checking\n\n### Behavior Matrix\n\n| Type | `${x}` (unquoted) | `'${x}'` (quoted) |\n|------|-------------------|-------------------|\n| `dict\\|str` | ✅ allowed (Tier 1) | ✅ allowed |\n| `list\\|any` | ✅ allowed (Tier 1) | ✅ allowed |\n| `dict\\|list` | ❌ blocked | ✅ allowed (Tier 2) |\n| `dict` | ❌ blocked | ✅ allowed (Tier 2) |\n| `list[dict]` | ❌ blocked (bug fix) | ✅ allowed (Tier 2) |\n\n### Files Changed\n\n- `src/pflow/runtime/template_validator.py` - Added validation logic, helper functions, error message updates\n- `tests/test_runtime/test_template_validator_types.py` - Added 4 new test classes with comprehensive coverage\n\n## Testing\n\n```bash\nmake test   # 3946 tests pass\nmake check  # Linting and type checks pass\n```\n\nNew test classes added:\n- `TestShellCommandUnionTypes` - Tier 1 tests\n- `TestShellCommandGenericTypes` - Fix 0 tests  \n- `TestShellCommandQuoteEscape` - Tier 2 tests\n- `TestShellCommandRegressions` - Regression tests",
      "pr_link": "https://github.com/spinje/pflow/pull/61",
      "pr_number": 61
    }
  },
  {
    "draft": "Added stderr check to shell node smart handling to catch pipeline errors [#58](https://github.com/spinje/pflow/pull/58)",
    "index": 8,
    "context": {
      "commit_message": "Merge pull request #58 from spinje/fix/shell-smart-handling-stderr-check",
      "task_number": 110,
      "files_changed": ".taskmaster/tasks/task_110/starting-context/braindump-investigation-journey.md, .taskmaster/tasks/task_110/task-110.md, CLAUDE.md, src/pflow/cli/main.py, src/pflow/cli/resources/cli-agent-instructions.md ... and 5 more files",
      "pr_title": "fix: add stderr check to shell smart handling to catch pipeline errors (fixes #57)",
      "pr_body": "## Summary\n\nFixes a bug where shell nodes incorrectly treated pipeline failures as success when the command contained `grep`, `rg`, `which`, or `command -v`. This caused silent failures that were difficult to debug.\n\n**Before:** `grep test | sed 's/bad_regex/'` would succeed silently even when sed failed\n**After:** Same command correctly fails with the sed error message\n\n## Task\n\nTask 110\n\n## Changes\n\n- Add stderr check to `_is_safe_non_error()` - only apply smart handling when stderr is empty\n- Store `smart_handled` flag in shared store for visibility\n- Show `[no matches]`/`[not found]` tags in CLI node status line\n- Add `smart_handled` and `smart_handled_reason` to JSON output for agent consumption\n- Extract shell metadata logic into `_add_shell_node_metadata()` helper\n\n## File Stats\n\n```\n .taskmaster/tasks/task_110/...braindump-investigation-journey.md | 160 +++\n .taskmaster/tasks/task_110/task-110.md                           | 123 +++\n CLAUDE.md                                                        |   1 +\n src/pflow/cli/main.py                                            |   9 +\n src/pflow/execution/execution_state.py                           |  42 +-\n src/pflow/nodes/shell/shell.py                                   |  66 +-\n tests/test_nodes/test_shell_smart_handling.py                    | 268 ++++++\n 7 files changed, 643 insertions(+), 26 deletions(-)\n```\n\n## Testing\n\n- 9 new tests covering both success cases (grep no match) and failure cases (downstream errors)\n- All 3924 tests pass\n- `make check` passes\n\n```bash\nmake test  # All tests pass\nmake check # Linting and type checks pass\n```\n\nFixes #57",
      "pr_link": "https://github.com/spinje/pflow/pull/58",
      "pr_number": 58
    }
  },
  {
    "draft": "Surfaced shell stderr output when exit code is 0 [#56](https://github.com/spinje/pflow/pull/56)",
    "index": 9,
    "context": {
      "commit_message": "Merge pull request #56 from spinje/fix/shell-stderr-visibility",
      "task_number": 108,
      "files_changed": ".taskmaster/tasks/task_108/starting-context/braindump-stderr-visibility-overlap.md, src/pflow/cli/main.py, src/pflow/cli/resources/cli-agent-instructions.md, src/pflow/execution/execution_state.py, tests/test_cli/test_shell_stderr_warnings.py ... and 1 more files",
      "pr_title": "fix: surface shell stderr when exit_code=0 (fixes #55)",
      "pr_body": "## Summary\n\nShell node stderr was hidden when `exit_code=0`, making pipeline failures invisible. This caused silent failures where workflows appeared successful but produced wrong/empty results.\n\n**Before:**\n```\n✓ Workflow completed in 0.4s\nNodes executed (1):\n  ✓ extract-urls (6ms)\nWorkflow executed successfully\n```\n\n**After:**\n```\n⚠️ Workflow completed in 0.367s\nNodes executed (1):\n  ⚠ extract-urls (5ms)\n\n⚠️  Shell stderr (exit code 0):\n  • extract-urls: sed: 1: \"s/.*?/bad/\": RE error: repetition-operator operand invalid\n\nWorkflow executed successfully\n```\n\n## Changes\n\n- **execution_state.py**: Add `has_stderr`/`stderr` detection in `build_execution_steps()` for shell nodes with `exit_code=0`\n- **main.py**: \n  - Add `_display_stderr_warnings()` function to show stderr content\n  - Add `_display_workflow_completion_status()` helper for status indicators\n  - Show ⚠️ workflow indicator when any node has stderr\n  - Show ⚠ node indicator for nodes with stderr\n- **tests**: 28 new tests covering CLI output, JSON output, and edge cases\n\n## File Stats\n\n```\n .../braindump-stderr-visibility-overlap.md         | 130 +++\n src/pflow/cli/main.py                              |  63 +-\n src/pflow/execution/execution_state.py             |  14 +\n tests/test_cli/test_shell_stderr_warnings.py       | 462 +++++++++\n tests/test_execution/test_execution_state.py       | 240 +++++\n 5 files changed, 896 insertions(+), 7 deletions(-)\n```\n\n## Testing\n\n- Run `make test` - all 3916 tests pass\n- Run `make check` - lint, type check, format all pass\n- Manual verification of all bug report scenarios\n\n## Notes\n\n- The fix is always-on (no flag required) since hidden errors are a debugging nightmare\n- JSON output includes `has_stderr: true` and `stderr: \"...\"` for AI agents\n- Complements Task 108's planned `--debug` flag (see braindump for details)",
      "pr_link": "https://github.com/spinje/pflow/pull/56",
      "pr_number": 56
    }
  },
  {
    "draft": "Included the original item in batch results for self-contained downstream processing [#54](https://github.com/spinje/pflow/pull/54)",
    "index": 16,
    "context": {
      "commit_message": "Merge pull request #54 from spinje/feat/include-original-item",
      "task_number": null,
      "files_changed": "architecture/core-concepts/schemas.md, docs/how-it-works/batch-processing.mdx, src/pflow/cli/resources/cli-agent-instructions.md, src/pflow/runtime/batch_node.py, src/pflow/runtime/template_validator.py ... and 2 more files",
      "pr_title": "feat: include original item in batch results for self-contained downstream processing",
      "pr_body": "## Summary\n\nEach batch result now includes an `item` field containing the original input, making results self-contained for downstream processing. When passing `${batch.results}` to an LLM for formatting, it sees both inputs and outputs together without needing external context.\n\n**Before:**\n```json\n{\"results\": [{\"stdout\": \"processed: apple\"}, {\"stdout\": \"processed: banana\"}]}\n```\n\n**After:**\n```json\n{\"results\": [{\"item\": \"apple\", \"stdout\": \"processed: apple\"}, {\"item\": \"banana\", \"stdout\": \"processed: banana\"}]}\n```\n\n## Changes\n\n- Add `result[\"item\"] = item` in `batch_node.py` (both `_exec_single` and `_exec_single_with_node`)\n- Log warning if node already outputs an `item` field (avoid silent overwrites)\n- Update `template_validator.py` to recognize `${node.results[0].item}` as valid\n- Update documentation in 4 files\n\n## Files Changed\n\n| File | Purpose |\n|------|---------|\n| `src/pflow/runtime/batch_node.py` | Core implementation + docstring |\n| `src/pflow/runtime/template_validator.py` | Add `item` to batch result schema |\n| `src/pflow/cli/resources/cli-agent-instructions.md` | Agent documentation |\n| `architecture/core-concepts/schemas.md` | Architecture docs |\n| `docs/how-it-works/batch-processing.mdx` | User-facing docs |\n| `tests/test_runtime/test_batch_node.py` | Updated tests + 3 new high-value tests |\n| `tests/test_runtime/test_template_validator.py` | New test for `${node.results[0].item}` validation |\n\n## Testing\n\n```bash\nmake test  # All 151 batch + validator tests pass\nmake check # Linting, type checking pass\n```\n\nHigh-value tests added:\n- `test_item_included_when_result_has_error_key`\n- `test_item_overwrite_warning_logged`\n- `test_batch_results_item_field_validated`\n\nCloses #53",
      "pr_link": "https://github.com/spinje/pflow/pull/54",
      "pr_number": 54
    }
  },
  {
    "draft": "Increased max_concurrent limit for batch processing in CLI agent instructions",
    "index": 20,
    "context": {
      "commit_message": "fix: increase max_concurrent limit for batch processing in CLI agent instructions",
      "task_number": null,
      "files_changed": "src/pflow/cli/resources/cli-agent-instructions.md",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Fixed issue where upstream shell stderr was not surfaced when downstream nodes failed [#52](https://github.com/spinje/pflow/pull/52)",
    "index": 21,
    "context": {
      "commit_message": "Merge pull request #52 from spinje/fix/shell-stderr-not-surfaced",
      "task_number": null,
      "files_changed": "src/pflow/cli/CLAUDE.md, src/pflow/cli/main.py, src/pflow/runtime/CLAUDE.md, src/pflow/runtime/batch_node.py, src/pflow/runtime/error_context.py ... and 4 more files",
      "pr_title": "fix: surface upstream shell stderr when downstream nodes fail",
      "pr_body": "## Summary\n\nWhen a shell node produces stderr but succeeds (exit code 0), and a downstream node fails due to unexpected output, the stderr was not visible in error messages. This made debugging significantly harder because the root cause was hidden.\n\nThis PR surfaces upstream shell stderr in error messages when a downstream node fails due to bad upstream output.\n\n**Before:**\n```\nError 1 at node 'process-batch':\n  Message: Batch items must be an array, got str. Template '${extract-urls.stdout}' resolved to: ''\n```\n\n**After:**\n```\nError 1 at node 'process-batch':\n  Message: Batch items must be an array, got str. Template '${extract-urls.stdout}' resolved to: ''\n\n  ⚠️  Upstream node 'extract-urls' stderr:\n     grep: invalid option -- P\n     usage: grep [-abcdDEFGHhIiJLlMmnOopqRSsUVvwXxZz] ...\n```\n\n## Changes\n\n- **CLI display** (`main.py`): Added stderr display in verbose mode, extracted `_display_shell_error_details()` helper\n- **Shared utility** (`error_context.py`): New module with `extract_node_ids_from_template()` and `get_upstream_shell_stderr()` functions\n- **Batch node** (`batch_node.py`): Enhanced errors with upstream stderr context via `_enrich_with_upstream_stderr()` helper\n- **Node wrapper** (`node_wrapper.py`): Added upstream stderr to unresolved template errors\n\n## File Stats\n\n```\n src/pflow/cli/main.py                                |  23 ++-\n src/pflow/runtime/batch_node.py                      |  33 +++-\n src/pflow/runtime/error_context.py                   |  88 ++++++++++\n src/pflow/runtime/node_wrapper.py                    |  12 +-\n tests/test_cli/test_shell_stderr_display.py          |  78 +++++++++\n tests/test_runtime/test_batch_node_stderr_context.py | 236 +++++++++++++++++++++++++\n tests/test_runtime/test_node_wrapper_stderr_context.py| 136 +++++++++++++++\n 7 files changed, 596 insertions(+), 10 deletions(-)\n```\n\n## Testing\n\n- Added 19 new tests covering all scenarios\n- All tests pass: `make test`\n- Code quality checks pass: `make check`\n\nFixes #50",
      "pr_link": "https://github.com/spinje/pflow/pull/52",
      "pr_number": 52
    }
  },
  {
    "draft": "Stripped trailing newlines from shell node stdout by default [#51](https://github.com/spinje/pflow/pull/51)",
    "index": 22,
    "context": {
      "commit_message": "Merge pull request #51 from spinje/fix/shell-stdout-trailing-newline",
      "task_number": null,
      "files_changed": "scratchpads/stdout-newline-bug/README.md, scratchpads/stdout-newline-bug/run-tests.sh, scratchpads/stdout-newline-bug/test-filename-corruption.json, scratchpads/stdout-newline-bug/test-inline-object.json, scratchpads/stdout-newline-bug/test-json-path.json ... and 6 more files",
      "pr_title": "fix: strip trailing newlines from shell stdout by default",
      "pr_body": "## Summary\n\nShell commands like `echo`, `date`, `hostname` add trailing newlines for terminal formatting. When using `${node.stdout}` in file paths, this creates **silently corrupted filenames** with embedded newlines (e.g., `myfile\\n.txt`).\n\nThis fix adds a `strip_newline` parameter to the shell node (default: `true`) that strips trailing newlines from text stdout, matching bash `$()` convention.\n\n## Changes\n\n- Add `strip_newline` parameter to shell node (default: `true`)\n- Strip trailing newlines from text stdout in `post()` \n- Extract `_store_output()` helper to reduce cyclomatic complexity\n- Update 3 tests that expected trailing newlines\n- Add 4 high-value tests for `strip_newline` behavior\n- Update agent instructions documentation with opt-out note\n\n```\n src/pflow/cli/resources/cli-agent-instructions.md  |  2 +-\n .../instructions/mcp-agent-instructions.md         |  2 +-\n src/pflow/nodes/shell/shell.py                     | 58 +++++++++++++++-------\n tests/test_nodes/test_shell/test_shell.py          | 56 +++++++++++++++++++++\n tests/test_nodes/test_shell/test_shell_binary.py   |  6 +--\n 5 files changed, 100 insertions(+), 24 deletions(-)\n```\n\n## Behavior\n\n| Scenario | Before | After |\n|----------|--------|-------|\n| `echo hello` → `${node.stdout}` | `hello\\n` | `hello` |\n| File path `${node.stdout}.txt` | `hello\\n.txt` (corrupted) | `hello.txt` |\n| `strip_newline: false` | N/A | `hello\\n` (opt-out) |\n| Multi-line output | N/A | Internal `\\n` preserved |\n\n## Testing\n\n- `make test` - All 3857 tests pass\n- `make check` - All linting/type checks pass\n- Manual CLI verification with hex dump confirms fix\n\nFixes #48",
      "pr_link": "https://github.com/spinje/pflow/pull/51",
      "pr_number": 51
    }
  },
  {
    "draft": "Fixed optional inputs without defaults to resolve to `None` [#49](https://github.com/spinje/pflow/pull/49)",
    "index": 23,
    "context": {
      "commit_message": "Merge pull request #49 from spinje/fix/optional-inputs-fail-template",
      "task_number": null,
      "files_changed": "scratchpads/optional-input-resolution/README.md, scratchpads/optional-input-resolution/test-cases/01-optional-no-default.json, scratchpads/optional-input-resolution/test-cases/02-optional-with-default.json, scratchpads/optional-input-resolution/test-cases/03-mixed-templates.json, scratchpads/optional-input-resolution/test-cases/04-real-world-example.json ... and 4 more files",
      "pr_title": "fix: resolve optional inputs without defaults to None (fixes #47)",
      "pr_body": "## Summary\n\nFixes the bug where optional workflow inputs declared with `required: false` but no `default` value would fail template resolution with \"Unresolved variables\" error.\n\nNow, optional inputs without defaults resolve to `None`, allowing templates to work correctly.\n\n## Changes\n\n- **workflow_validator.py**: Add `None` to context for optional inputs without explicit defaults\n- **node_wrapper.py**: Filter error messages to only report actually unresolved variables (fixes misleading errors)\n- **test_null_defaults.py**: Updated existing test + added regression tests\n- **test_node_wrapper_template_validation.py**: Added tests for accurate error messages\n\n```\n src/pflow/runtime/node_wrapper.py                       |   7 ++-\n src/pflow/runtime/workflow_validator.py                 |   7 ++-\n tests/test_runtime/test_node_wrapper_template_validation.py | 77 +++++++++++++++\n tests/test_runtime/test_null_defaults.py                |  80 +++++++++++++++-\n 4 files changed, 164 insertions(+), 9 deletions(-)\n```\n\n## Before/After\n\n**Before:**\n```bash\n$ pflow workflow.json  # optional input not provided\n❌ Error: Unresolved variables in parameter 'stdin': ${optional_param}\n```\n\n**After:**\n```bash\n$ pflow workflow.json  # optional input not provided\n✓ Workflow completed\n{\"value\": null}\n```\n\n## Testing\n\n- All 3858 tests pass\n- `make check` passes (lint, type check, formatting)\n- Manually verified with bug report test cases\n\nRun `make test` to verify.",
      "pr_link": "https://github.com/spinje/pflow/pull/49",
      "pr_number": 49
    }
  },
  {
    "draft": "Fixed a bug where workflow names could mismatch with filenames by deriving the name directly from the filename [#46](https://github.com/spinje/pflow/pull/46)",
    "index": 25,
    "context": {
      "commit_message": "Merge pull request #46 from spinje/fix/workflow-name-filename-mismatch",
      "task_number": null,
      "files_changed": "src/pflow/core/CLAUDE.md, src/pflow/core/workflow_manager.py, src/pflow/planning/context_builder.py, tests/test_cli/test_workflow_save_cli.py, tests/test_cli/test_workflow_save_integration.py ... and 2 more files",
      "pr_title": "fix: derive workflow name from filename to prevent mismatch bug",
      "pr_body": "## Summary\n\nFixes a bug where the system would suggest a workflow name that was just reported as \"not found\".\n\n**Before:**\n```\n❌ Workflow 'slack-qa-analyzer' not found.\n\nDid you mean:\n  - slack-qa-analyzer   ← Same name it just said doesn't exist!\n```\n\n**After:**\n```\n❌ Workflow 'slack-qa-analyzer' not found.\n```\n\n## Root Cause\n\nThe workflow `name` was stored in two places that could get out of sync:\n1. **Filename**: `api-analysis.json`\n2. **Internal field**: `{\"name\": \"slack-qa-analyzer\", ...}`\n\n- `exists(name)` checked if `{name}.json` exists → used **filename**\n- `list_all()` read `\"name\"` field from inside files → used **internal field**\n\nWhen these differed (e.g., manual file rename), the system found the internal name for suggestions but couldn't locate the file.\n\n## Solution\n\nMake **filename the single source of truth** for workflow names:\n\n1. **Stop storing `name` in workflow files** - Remove from metadata wrapper\n2. **Inject name from filename at load time** - In `load()` and `list_all()`\n3. **Update context_builder** - Same pattern for planning system\n\n## Changes\n\n- `workflow_manager.py`: Remove name from wrapper, inject from filename\n- `context_builder.py`: Remove name validation, inject from filename\n- `CLAUDE.md`: Update storage format documentation\n- Tests: Update assertions, add regression test\n\n```\n src/pflow/core/CLAUDE.md                         |  4 ++-\n src/pflow/core/workflow_manager.py               |  8 ++++-\n src/pflow/planning/context_builder.py            |  9 +++--\n tests/test_cli/test_workflow_save_cli.py         |  3 +-\n tests/test_cli/test_workflow_save_integration.py |  3 +-\n tests/test_core/test_workflow_manager.py         | 42 +++++++++++++++++++++++-\n tests/test_planning/test_workflow_loading.py     | 28 ++++++++--------\n 7 files changed, 76 insertions(+), 21 deletions(-)\n```\n\n## Testing\n\n- All 3799 tests pass\n- Manual verification: `api-analysis.json` now correctly shows as `api-analysis`\n- Regression test added: `test_filename_is_source_of_truth_for_name`\n\nRun `make test` to verify.\n\nFixes #45",
      "pr_link": "https://github.com/spinje/pflow/pull/46",
      "pr_number": 46
    }
  },
  {
    "draft": "Fixed `--output-format json` support for the `registry run` command",
    "index": 27,
    "context": {
      "commit_message": "fix: make --output-format json work for registry run",
      "task_number": null,
      "files_changed": "docs/reference/cli/registry.mdx, src/pflow/cli/registry.py, tests/test_cli/test_registry_run.py",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Fixed an issue where structured output mode skipped LLM filtering and added Gemini thinking optimizations [#43](https://github.com/spinje/pflow/pull/43)",
    "index": 28,
    "context": {
      "commit_message": "Merge pull request #43 from spinje/fix/structure-mode-skip-llm-filtering",
      "task_number": null,
      "files_changed": "docs/reference/cli/registry.mdx, docs/reference/cli/settings.mdx, examples/real-workflows/release-announcements/README.md, examples/real-workflows/release-announcements/prompt.md, examples/real-workflows/release-announcements/workflow.json ... and 3 more files",
      "pr_title": "fix: structure output mode skips LLM filtering, add Gemini thinking optimizations",
      "pr_body": "## Summary\n\nFixes #42 - Structure output mode now skips LLM-based smart filtering entirely, and adds thinking optimizations for Gemini models to dramatically improve filtering performance.\n\n## Changes\n\n**Structure mode optimization:**\n- `output_mode: \"structure\"` now shows all paths without calling the LLM\n- Previously both \"smart\" and \"structure\" modes triggered filtering, causing unexpected 2-30s delays\n\n**Gemini thinking optimizations:**\n- Gemini 3 models: `thinking_level='minimal'` (32s → 5s)\n- Gemini 2.5 models: `thinking_budget=0` (22s → 5s)\n\n**Documentation:**\n- Updated `settings.mdx` with model recommendations table for filtering\n- Updated `registry.mdx` to clarify output mode behavior\n\n**Benchmark results (filtering overhead):**\n\n| Provider | Model | Overhead | Notes |\n|----------|-------|----------|-------|\n| Google | `gemini-2.5-flash-lite` | ~2-3s | Best budget option |\n| OpenAI | `gpt-5-mini` | ~1-2s | Runner-up budget |\n| Anthropic | `claude-haiku-4-5` | ~1-2s | Third place budget |\n| Anthropic | `claude-sonnet-4-5` | ~1-2s | Best premium option |\n| Google | `gemini-3-flash-preview` | ~2-3s | Runner-up premium |\n\n## Testing\n\n```bash\nmake test   # All 3850 tests pass\nmake check  # All linting/type checks pass\n```\n\n## Files changed\n\n- `src/pflow/execution/formatters/node_output_formatter.py` - Skip filtering for structure mode\n- `src/pflow/core/smart_filter.py` - Add Gemini thinking optimizations\n- `tests/.../test_node_output_formatter.py` - Add test for structure mode\n- `docs/reference/cli/settings.mdx` - Model recommendations\n- `docs/reference/cli/registry.mdx` - Output mode clarifications",
      "pr_link": "https://github.com/spinje/pflow/pull/43",
      "pr_number": 43
    }
  },
  {
    "draft": "Automatically coerce dictionary and list inputs to JSON strings for string-typed parameters [#41](https://github.com/spinje/pflow/pull/41)",
    "index": 30,
    "context": {
      "commit_message": "Merge pull request #41 from spinje/fix/json-parse-break",
      "task_number": null,
      "files_changed": "docs/CLAUDE.md, examples/real-workflows/README.md, examples/real-workflows/webpage-to-markdown/prompt.md, examples/real-workflows/webpage-to-markdown/workflow.json, scratchpads/bug-json-string-params/BUG-REPORT.md ... and 15 more files",
      "pr_title": "fix: auto-coerce dict/list to JSON string for str-typed parameters",
      "pr_body": "## Summary\n\nWhen MCP tools declare parameters as `str` but expect JSON content inside (e.g., Discord's `path_params`, `body_schema`), pflow now automatically serializes dict/list values to JSON strings.\n\nFixes #40\n\n## Changes\n\n- **New `param_coercion.py`**: Shared utility for type-aware coercion\n- **Workflow execution path**: `node_wrapper.py` coerces after template resolution\n- **CLI registry run path**: `registry_run.py` coerces before `node.set_params()`\n- **Updated tests**: Type validation tests updated to reflect new coercion behavior\n\n### Key Behavior\n\n| Input | Target Type | Result |\n|-------|-------------|--------|\n| `{\"key\": \"value\"}` | `str` | `'{\"key\": \"value\"}'` (JSON string) |\n| `[1, 2, 3]` | `str` | `'[1, 2, 3]'` (JSON string) |\n| `{\"key\": \"value\"}` | `dict` | `{\"key\": \"value\"}` (unchanged) |\n\n## Files Changed\n\n```\n src/pflow/core/param_coercion.py                   |  58 +++  (NEW)\n src/pflow/core/__init__.py                         |   2 +\n src/pflow/runtime/node_wrapper.py                  |   8 +\n src/pflow/cli/registry_run.py                      |  48 ++\n tests/test_core/test_param_coercion.py             | 119 +++++ (NEW)\n tests/test_runtime/test_node_wrapper_json_parsing.py | 120 +++++\n tests/test_runtime/test_node_wrapper_type_validation.py | 194 changes\n```\n\n## Testing\n\n- All 3844 tests pass\n- `make check` passes (linting, type checking)\n- Manual verification completed for both execution paths\n\nRun `make test` to verify all tests pass.",
      "pr_link": "https://github.com/spinje/pflow/pull/41",
      "pr_number": 41
    }
  },
  {
    "draft": "Added release context file and CLI summary to the generate-changelog workflow",
    "index": 35,
    "context": {
      "commit_message": "feat(generate-changelog): add release context file and CLI summary",
      "task_number": null,
      "files_changed": "examples/real-workflows/generate-changelog-simple/archive/workflow-simple.json, examples/real-workflows/generate-changelog-simple/prompt.md, examples/real-workflows/generate-changelog/README.md, examples/real-workflows/generate-changelog/prompt.md, examples/real-workflows/generate-changelog/workflow.json",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Fixed inline object templates to automatically parse JSON strings [#39](https://github.com/spinje/pflow/pull/39)",
    "index": 49,
    "context": {
      "commit_message": "Merge pull request #39 from spinje/feat/stdin-json-parse",
      "task_number": null,
      "files_changed": "src/pflow/cli/resources/cli-agent-instructions.md, src/pflow/runtime/template_resolver.py, tests/test_integration/test_inline_object_json_parsing_e2e.py, tests/test_runtime/test_template_resolver_inline_object_parsing.py",
      "pr_title": "fix: auto-parse JSON strings in inline object templates",
      "pr_body": "## Summary\n\nFixes #38 - When a simple template like `${node.stdout}` is used inside an inline object, the resolved JSON string is now automatically parsed to a dict/list/primitive.\n\n## Problem\n\nPreviously, combining JSON outputs in inline objects didn't work as expected:\n\n```json\n{\"stdin\": {\"data\": \"${shell.stdout}\"}}\n```\n\nWhere `shell.stdout` is `'{\"items\": [1,2,3]}\\n'` would result in:\n```python\n{\"stdin\": {\"data\": '{\"items\": [1,2,3]}\\n'}}  # String, not dict!\n```\n\nThis meant jq couldn't access `.data.items` because `.data` was a string.\n\n## Solution\n\nModified `resolve_nested()` in `template_resolver.py` to auto-parse JSON strings from simple templates. Now the same pattern produces:\n\n```python\n{\"stdin\": {\"data\": {\"items\": [1, 2, 3]}}}  # Parsed dict!\n```\n\nAll valid JSON types are parsed: objects, arrays, numbers, booleans, strings, and null.\n\n## Escape Hatch\n\nComplex templates remain as strings, providing control when raw JSON is needed:\n- `\"prefix ${var}\"` → stays string\n- `\"${var} \"` → stays string (trailing space)\n- `\"'${var}'\"` → stays string (with quotes)\n\n## Changes\n\n- Modified `resolve_nested()` to auto-parse JSON from simple templates\n- Added 37 unit tests covering all JSON types and edge cases\n- Added 6 E2E integration tests with shell → jq workflows\n\n```\n src/pflow/runtime/template_resolver.py                         |  48 ++-\n tests/test_integration/test_inline_object_json_parsing_e2e.py  | 153 ++++++++\n tests/test_runtime/test_template_resolver_inline_object_parsing.py | 263 +++++++++++++\n 4 files changed, 511 insertions(+), 48 deletions(-)\n```\n\n## Testing\n\n```bash\nmake test   # 3837 passed\nmake check  # All linting/type checks pass\n```",
      "pr_link": "https://github.com/spinje/pflow/pull/39",
      "pr_number": 39
    }
  },
  {
    "draft": "Fixed static validation to recognize batch output structures",
    "index": 52,
    "context": {
      "commit_message": "fix: static validation now understands batch output structure",
      "task_number": null,
      "files_changed": "src/pflow/runtime/template_validator.py, tests/test_runtime/test_template_validator.py",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Added automatic JSON string parsing during nested template access [#37](https://github.com/spinje/pflow/pull/37)",
    "index": 53,
    "context": {
      "commit_message": "Merge pull request #37 from spinje/feat/auto-parse-json",
      "task_number": 105,
      "files_changed": ".taskmaster/tasks/task_105/implementation/implementation-plan.md, .taskmaster/tasks/task_105/implementation/progress-log.md, .taskmaster/tasks/task_105/research/feature-request-json-string-nested-access.md, .taskmaster/tasks/task_105/research/findings.md, .taskmaster/tasks/task_105/research/path-traversal-analysis.md ... and 20 more files",
      "pr_title": "feat: auto-parse JSON strings during nested template access",
      "pr_body": "## Summary\n\nEnable `${node.stdout.field}` to work when `stdout` contains a JSON string like `'{\"field\": \"value\"}'`. Previously, path traversal failed because the resolver treated JSON strings as opaque strings.\n\nCloses #36\n\n## Changes\n\n- Add shared `json_utils.py` with `try_parse_json()` utility (two-value return for null vs failure)\n- Update `template_resolver.py` to parse JSON during path traversal\n- Update `template_validator.py` to allow `str` nested access with warning\n- Consolidate 5 duplicate JSON parsing implementations across codebase\n- Add comprehensive tests (10 unit, 21 resolver, 9 E2E)\n\n## Key Design Decisions\n\n1. **Parse only when traversing deeper** - `${node.stdout}` returns raw string, `${node.stdout.field}` triggers parsing\n2. **Validator warns for `str` type** - `dict`/`any` allowed silently, `str` generates warning about JSON auto-parsing\n3. **Two-value return** - `(success, result)` distinguishes \"parsed to None\" from \"failed to parse\"\n\n## Before/After\n\n```\nBefore:  ${node.stdout.iso}  → ❌ Unresolved variable error\nAfter:   ${node.stdout.iso}  → ✅ \"2026-01-01\"\n```\n\n## Created Docs\n\n- `.taskmaster/tasks/task_105/task-105.md`\n- `.taskmaster/tasks/task_105/implementation/progress-log.md`\n- `.taskmaster/tasks/task_105/implementation/implementation-plan.md`\n\n## Testing\n\n```bash\nmake test   # 3661 tests pass\nmake check  # All linting/type checks pass\n```\n\nSpecific feature tests:\n```bash\npytest tests/test_core/test_json_utils.py tests/test_runtime/test_template_resolver_json_parsing.py tests/test_integration/test_json_nested_access_e2e.py -v\n```",
      "pr_link": "https://github.com/spinje/pflow/pull/37",
      "pr_number": 37
    }
  },
  {
    "draft": "git-log node now supports tag/branch refs for since/until",
    "index": 54,
    "context": {
      "commit_message": "fix: git-log node now supports tag/branch refs for since/until",
      "task_number": null,
      "files_changed": "src/pflow/nodes/git/log.py, tests/test_nodes/test_git/test_git_log.py",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Preserved inline object types during template resolution [#32](https://github.com/spinje/pflow/pull/32)",
    "index": 56,
    "context": {
      "commit_message": "Merge pull request #32 from spinje/feat/inline-object-type",
      "task_number": 103,
      "files_changed": ".taskmaster/tasks/task_103/implementation/implementation-plan.md, .taskmaster/tasks/task_103/implementation/progress-log.md, .taskmaster/tasks/task_103/task-review.md, architecture/reference/template-variables.md, src/pflow/runtime/CLAUDE.md ... and 11 more files",
      "pr_title": "feat: preserve inline object type in template resolution (fixes #31)",
      "pr_body": "## Summary\n\nFixes the double-serialization bug where templates in nested structures like `{\"key\": \"${dict_var}\"}` would produce `{\"key\": \"{\\\"nested\\\": \\\"value\\\"}\"}` instead of the correct `{\"key\": {\"nested\": \"value\"}}`.\n\nThis enables the intuitive pattern of passing multiple data sources to shell commands:\n\n```json\n{\n  \"stdin\": {\"config\": \"${config}\", \"data\": \"${data}\"},\n  \"command\": \"jq '.config + .data'\"\n}\n```\n\n## Changes\n\n- **New helper methods** in `TemplateResolver`:\n  - `is_simple_template()` - detects if string is exactly `${var}`\n  - `extract_simple_template_var()` - extracts variable name from simple template\n  \n- **Renamed `resolve_string()` → `resolve_template()`** with type preservation:\n  - Simple templates (`${var}`) preserve original type (dict, list, int, bool, None)\n  - Complex templates (`\"Hello ${name}\"`) return interpolated strings\n  \n- **Updated callers**:\n  - `workflow_executor.py` - nested workflow param mapping\n  - `node_wrapper.py` - refactored to use shared helper (DRY)\n\n## Test Coverage\n\n- `test_template_type_preservation.py` - Core type preservation behavior (12 tests)\n- `test_shell_stdin_type_preservation.py` - Integration tests with shell node (4 tests)\n- Updated existing tests to use `resolve_template()` (36 occurrences across 5 files)\n\n## Created Docs\n\n- `.taskmaster/tasks/task_103/implementation/implementation-plan.md`\n- `.taskmaster/tasks/task_103/implementation/progress-log.md`\n\n## File Stats\n\n```\n 13 files changed, 1620 insertions(+), 98 deletions(-)\n```\n\n## Testing\n\n```bash\nmake test  # 3236 passed, 9 skipped\n```\n\n---\nTask-ID: 103",
      "pr_link": "https://github.com/spinje/pflow/pull/32",
      "pr_number": 32
    }
  },
  {
    "draft": "Moved shell node dict and list validation to compile time [#30](https://github.com/spinje/pflow/pull/30)",
    "index": 57,
    "context": {
      "commit_message": "Merge pull request #30 from spinje/fix/shell-dict-list-validation",
      "task_number": 103,
      "files_changed": ".taskmaster/knowledge/pitfalls.md, .taskmaster/tasks/task_103/starting-context/task-103-handover.md, .taskmaster/tasks/task_103/task-103.md, .taskmaster/tasks/task_104/starting-context/task-104-handover.md, .taskmaster/tasks/task_104/task-104.md ... and 6 more files",
      "pr_title": "fix: move shell dict/list validation to compile time",
      "pr_body": "## Summary\n\nFixes #29 - Shell node dict/list validation was broken (dead code + false positives).\n\nThe shell node's runtime checks for dict/list in commands had two problems:\n1. `_check_command_template_safety()` ran AFTER template resolution, so `extract_variables()` found nothing (dead code)\n2. `_warn_shell_unsafe_json()` regex `\\[.{0,500}?\\]` matched shell syntax like `[a-z]`, `[0-9]` (false positives)\n\nResult: dict/list in shell commands passed validation, then failed at runtime with cryptic shell syntax errors.\n\n## Changes\n\n- Add `_validate_shell_command_types()` in `template_validator.py` - runs at compile time BEFORE template resolution\n- Remove dead code from `shell.py` (`_check_command_template_safety`, `_warn_shell_unsafe_json`)\n- Add actionable error messages:\n  - Single template: suggests stdin parameter with example\n  - Multiple templates: suggests temp files workaround\n- Add 9 high-value tests covering workflow inputs, nested access, union types, and validation timing\n\n## Error Messages\n\n**Single template:**\n```\nShell node 'process': cannot use ${data} (type: object) in command parameter.\n\nPROBLEM: object data embedded in shell commands breaks parsing...\n\nFIX: Move data to stdin, keep command simple:\n  {\"stdin\": \"${data}\", \"command\": \"jq '.field'\"}\n```\n\n**Multiple templates:**\n```\nShell node 'process': multiple structured data templates in command: ${config} (object), ${data} (object)\n\nPROBLEM: Shell commands can only receive ONE data source via stdin.\n\nFIX OPTIONS:\n1. Use temp files - write each data source to a file, then read in shell\n2. Process each data source in separate shell nodes\n```\n\n## Future Enhancement (Task 103)\n\nThe ideal solution for multiple data sources would be inline object construction:\n```json\n{\"stdin\": {\"config\": \"${config}\", \"data\": \"${data}\"}, \"command\": \"jq '.config + .data'\"}\n```\n\nThis doesn't work yet due to double-serialization (templates inside dicts serialize to JSON strings instead of preserving type). **Task 103** tracks the fix: preserve type when resolving simple templates inside dict/list values.\n\n## Testing\n\n```bash\nmake test  # 3723 passed\nmake check # All checks pass\n```",
      "pr_link": "https://github.com/spinje/pflow/pull/30",
      "pr_number": 30
    }
  },
  {
    "draft": "Removed parameter fallback pattern from all nodes [#28](https://github.com/spinje/pflow/pull/28)",
    "index": 59,
    "context": {
      "commit_message": "Merge pull request #28 from spinje/fix/namespace-collision",
      "task_number": 102,
      "files_changed": ".taskmaster/knowledge/decisions.md, .taskmaster/knowledge/patterns.md, .taskmaster/tasks/task_102/implementation/implementation-plan.md, .taskmaster/tasks/task_102/implementation/progress-log.md, .taskmaster/tasks/task_102/task-102.md ... and 78 more files",
      "pr_title": "refactor: remove parameter fallback pattern from all nodes (closes #27)",
      "pr_body": "## Summary\n\nRemove the \"shared store fallback\" pattern from all nodes that caused silent failures when node IDs or workflow inputs matched parameter names. This is an architectural refactor that aligns pflow with PocketFlow's original design philosophy.\n\n## Problem\n\nThe fallback pattern `shared.get(\"x\") or params.get(\"x\")` caused two types of collisions:\n\n1. **Node ID Collision**: Node named \"images\" creates `shared[\"images\"] = {stdout: ...}`, LLM node finds this dict instead of template-resolved param\n2. **Workflow Input Collision**: Input named \"url\" creates `shared[\"url\"] = \"value\"`, HTTP node uses raw value instead of template transformation\n\n## Solution\n\n- Nodes now read only from `self.params`\n- Templates (`${var}`) handle all data wiring explicitly\n- No collision detection needed - naming collisions simply don't matter with params-only\n\n## Changes\n\n**Node implementations (20 files, ~60 parameters)**\n- All file, git, github, http, llm, shell, claude, and test nodes updated\n\n**Interface docstrings (23 files)**\n- Changed from `- Reads: shared[\"key\"]` to `- Params: key` format\n\n**Documentation (9 files)**\n- Updated CLAUDE.md files, architecture docs, and reference docs\n\n**Tests (~17 files, ~150 tests)**\n- Updated tests expecting shared-first behavior\n- Added 15 regression tests in `test_namespace_collision_regression.py`\n\n## Stats\n\n```\n83 files changed, 4611 insertions(+), 909 deletions(-)\n```\n\n## Task Documentation\n\n- `.taskmaster/tasks/task_102/task-102.md` - Task specification\n- `.taskmaster/tasks/task_102/task-review.md` - Implementation review\n- `.taskmaster/tasks/task_102/implementation/progress-log.md` - Full timeline\n\n## Testing\n\n```bash\nmake test   # 3646 passed, 119 skipped\nmake check  # All passing\n```\n\nCloses #27",
      "pr_link": "https://github.com/spinje/pflow/pull/28",
      "pr_number": 28
    }
  },
  {
    "draft": "Fixed an issue where SIGPIPE would kill the process when a subprocess ignored stdin [#26](https://github.com/spinje/pflow/pull/26)",
    "index": 60,
    "context": {
      "commit_message": "Merge pull request #26 from spinje/fix/silent-workflow-failure",
      "task_number": null,
      "files_changed": ".taskmaster/knowledge/pitfalls.md, src/pflow/cli/main.py, tests/test_integration/test_sigpipe_regression.py, tests/test_nodes/test_shell/test_shell_sigpipe.py",
      "pr_title": "fix: prevent SIGPIPE from killing process when subprocess ignores stdin (fixes #25)",
      "pr_body": "## Summary\n\nFixes a critical bug where shell nodes would cause **silent exit 141** when the subprocess doesn't consume all its stdin data. This happened when:\n- Large stdin data (>16KB on macOS, >64KB on Linux) was passed to a shell command\n- The command didn't read stdin (e.g., `echo '[]'` in a conditional branch)\n- SIGPIPE was set to `SIG_DFL`, which immediately terminates Python\n\n## Root Cause\n\nThe signal handler `signal.signal(signal.SIGPIPE, signal.SIG_DFL)` caused Python to terminate with exit 141 when `subprocess.run()` tried to write to a pipe that was closed because the subprocess exited without consuming stdin.\n\n**Example trigger**: A workflow with conditional shell logic:\n```bash\ncase '${describe_images}' in\n  *[Ff]alse*) echo '[]' ;;    # Doesn't read stdin → SIGPIPE\n  *) grep | jq ;;              # Reads all stdin → works fine\nesac\n```\n\n## The Fix\n\nChange `SIG_DFL` to `SIG_IGN`:\n```python\nsignal.signal(signal.SIGPIPE, signal.SIG_IGN)\n```\n\nWith `SIG_IGN`, `subprocess.run()` handles broken pipes gracefully internally.\n\n## Changes\n\n| File | Changes |\n|------|---------|\n| `src/pflow/cli/main.py` | `SIG_DFL` → `SIG_IGN` with detailed comment |\n| `tests/test_nodes/test_shell/test_shell_sigpipe.py` | 9 unit tests |\n| `tests/test_integration/test_sigpipe_regression.py` | 7 integration tests |\n| `.taskmaster/knowledge/pitfalls.md` | Corrected SIGPIPE guidance |\n\n## Testing\n\n```bash\n# Run the new regression tests\nmake test\n\n# All 16 SIGPIPE tests pass\npytest tests/test_nodes/test_shell/test_shell_sigpipe.py tests/test_integration/test_sigpipe_regression.py -v\n```\n\n### Test Coverage\n\nThe tests verify:\n- ✅ Large stdin (20KB+) with commands that ignore stdin\n- ✅ Partial stdin consumption (`head -n 1`)\n- ✅ Conditional shell commands (the exact bug pattern)\n- ✅ Template resolution with boolean `False`\n- ✅ Meta-test verifying SIGPIPE is set to `SIG_IGN`\n\n### Verified the tests catch regression\n\n```bash\n# With SIG_DFL (the bug): Exit 141\n# With SIG_IGN (the fix): Exit 0\n```\n\nFixes #25",
      "pr_link": "https://github.com/spinje/pflow/pull/26",
      "pr_number": 26
    }
  },
  {
    "draft": "Fixed batch template validation to allow dotted references like ${item.field}",
    "index": 61,
    "context": {
      "commit_message": "fix: batch template validation now allows dotted refs like ${item.field}",
      "task_number": null,
      "files_changed": "src/pflow/core/workflow_data_flow.py, tests/test_core/test_workflow_data_flow.py",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Fixed LLM usage tracking for batch node inner executions [#22](https://github.com/spinje/pflow/pull/22)",
    "index": 65,
    "context": {
      "commit_message": "Merge pull request #22 from spinje/fix/llm-costs-batch",
      "task_number": null,
      "files_changed": "src/pflow/runtime/batch_node.py, tests/test_runtime/test_batch_node.py",
      "pr_title": "fix: capture LLM usage from batch node inner executions",
      "pr_body": "## Summary\n\nFixes #21 - LLM costs were not being tracked for batch node execution because the `llm_usage` data written by inner LLM nodes to the per-item isolated context (`item_shared`) was discarded after each item's execution.\n\n## Root Cause\n\nThe wrapper chain applies `InstrumentedNodeWrapper` around `PflowBatchNode`, not the inner LLM node:\n```\nInstrumentedNodeWrapper (outermost)\n  └─ PflowBatchNode\n     └─ NamespacedNodeWrapper  \n        └─ TemplateAwareNodeWrapper\n           └─ LLMNode (innermost)\n```\n\nWhen batch executes each item, the inner LLM node writes `llm_usage` to `item_shared`, but this context is discarded after execution. The outer `InstrumentedNodeWrapper` never sees this data.\n\n## Changes\n\n- Added `_capture_item_llm_usage()` method to `PflowBatchNode` that captures `llm_usage` from `item_shared` after each inner node execution\n- Initializes `__llm_calls__` list in `prep()` before batch execution starts (critical for shallow copy to share the list reference)\n- Captures work for both sequential and parallel execution modes\n- Handles both root-level (`shared[\"llm_usage\"]`) and namespaced (`shared[node_id][\"llm_usage\"]`) locations\n- Adds `node_id` and `batch_item_index` to each captured LLM call for tracing\n\n```\n src/pflow/runtime/batch_node.py       |  46 ++++++\n tests/test_runtime/test_batch_node.py | 258 ++++++++++++++++++++++++++++++++++\n 2 files changed, 304 insertions(+)\n```\n\n## Testing\n\nAdded 7 new tests:\n- `test_batch_captures_inner_node_llm_usage_sequential` - Sequential capture\n- `test_batch_captures_inner_node_llm_usage_parallel` - Parallel capture with thread safety\n- `test_batch_captures_namespaced_llm_usage` - Namespaced location support\n- `test_batch_initializes_llm_calls_list` - List initialization before batch\n- `test_batch_no_llm_usage_no_crash` - Non-LLM nodes don't crash\n- `test_llm_calls_contain_all_required_fields_for_metrics` - All required fields present for cost calculation\n- Original `test_llm_calls_accumulated` still passes\n\nRun `make test` to verify all 3685 tests pass.",
      "pr_link": "https://github.com/spinje/pflow/pull/22",
      "pr_number": 22
    }
  },
  {
    "draft": "Added display of batch node progress as items complete [#20](https://github.com/spinje/pflow/pull/20)",
    "index": 67,
    "context": {
      "commit_message": "Merge pull request #20 from spinje/feat/batch-progress-display",
      "task_number": null,
      "files_changed": ".taskmaster/tasks/task_100/task-100.md, .taskmaster/tasks/task_101/task-101.md, CLAUDE.md, examples/real-workflows/README.md, examples/real-workflows/webpage-to-markdown.json ... and 5 more files",
      "pr_title": "feat: display batch node progress as items complete",
      "pr_body": "## Summary\n\nAdd real-time progress updates for batch node execution, showing item completion status as they happen instead of blocking silently until all items complete.\n\n**Before:**\n```\n  convert-sections... ✓ 24.9s\n```\n\n**After:**\n```\n  convert-sections... 1/8 ✓\n  convert-sections... 2/8 ✓     (updates in place via \\r)\n  convert-sections... 3/8 ✗     (shows failures)\n  ...\n  convert-sections... 8/8 ✓ 24.9s\n```\n\n## Changes\n\n- **output_controller.py**: Add `_handle_batch_progress()` method using carriage return for in-place line updates; extend `_handle_node_complete()` to handle batch nodes (only shows timing, not duplicate checkmark)\n- **batch_node.py**: Call progress callback after each item in `_exec_sequential()` and `_collect_parallel_results()`\n- **instrumented_wrapper.py**: Detect batch nodes via `batch_metadata` in output and pass `is_batch`, `batch_total`, `batch_success_count` to completion callback\n- **Tests**: Add `TestBatchProgressCallbacks` (6 tests) and `TestBatchProgressDisplay` (8 tests)\n\n```\n src/pflow/core/output_controller.py      |  71 ++++++++++--\n src/pflow/runtime/batch_node.py          |  57 +++++++++-\n src/pflow/runtime/instrumented_wrapper.py|  15 ++-\n tests/test_core/test_output_controller.py| 193 +++++++++++++++++++++++++++++++\n tests/test_runtime/test_batch_node.py    | 185 +++++++++++++++++++++++++++++\n 5 files changed, 508 insertions(+), 13 deletions(-)\n```\n\n## Testing\n\nAll 3677 tests pass:\n```bash\nmake test\n```\n\nFixes #17",
      "pr_link": "https://github.com/spinje/pflow/pull/20",
      "pr_number": 20
    }
  },
  {
    "draft": "Allowed arrays and dictionaries to serialize to JSON when used in string template contexts [#19](https://github.com/spinje/pflow/pull/19)",
    "index": 68,
    "context": {
      "commit_message": "Merge pull request #19 from spinje/fix/template-variable-mismatch",
      "task_number": null,
      "files_changed": ".taskmaster/bugfix/bugfix-log.md, .taskmaster/knowledge/pitfalls.md, .taskmaster/tasks/task_96/implementation/progress-log.md, architecture/reference/template-variables.md, examples/legacy/fix_issue_workflow.json ... and 21 more files",
      "pr_title": "fix: allow arrays/dicts to serialize to JSON in string contexts",
      "pr_body": "## Summary\n\nFixes #18 - Arrays and dicts can now be embedded in shell command strings (and other string parameters) by auto-serializing to JSON.\n\n## Changes\n\n- **Type checker update**: Added `str`, `string` to compatible types for `dict`, `list`, `array`, `object`\n- **Shell-unsafe JSON warning**: New `_warn_shell_unsafe_json()` method warns when JSON contains characters that could break shell parsing (`'`, `` ` ``, `$(`)\n- **Documentation**: Updated `architecture/reference/template-variables.md` with new type matrix and \"Shell Command Limitations\" section\n- **Tests updated**: Modified tests that expected dict→str to fail, added new tests for list→str\n\n## Behavior\n\n| Template | Data | Result |\n|----------|------|--------|\n| `\"echo '${items}'\"` | `[\"a\", \"b\"]` | ✅ Works → `echo '[\"a\", \"b\"]'` |\n| `\"${items}\"` | `[\"a\", \"b\"]` | ❌ Blocked (simple template, runtime check preserved) |\n| `\"echo '${data}'\"` | `{\"msg\": \"it's\"}` | ⚠️ Warning about shell-unsafe chars |\n\n## Guardrails Preserved\n\n1. **Simple templates** (`${items}` alone) → runtime check still blocks dict/list→str\n2. **Shell-unsafe JSON** → warning emitted, suggests using `stdin`\n3. **Type mismatches for non-string targets** (dict→int) → still caught\n\n## File Changes\n\n```\n architecture/reference/template-variables.md       | 62 ++++++++++-\n src/pflow/nodes/shell/shell.py                     | 44 ++++++++\n src/pflow/runtime/template_validator.py            |  2 +-\n src/pflow/runtime/type_checker.py                  |  8 +-\n tests/test_runtime/test_template_validator_types.py| 73 ++++++++++---\n tests/test_runtime/test_type_checker.py            | 38 ++++---\n 6 files changed, 190 insertions(+), 37 deletions(-)\n```\n\n## Testing\n\nRun `make test` to verify all tests pass (3612 passed).\nRun `make check` to verify linting and type checks pass.",
      "pr_link": "https://github.com/spinje/pflow/pull/19",
      "pr_number": 19
    }
  },
  {
    "draft": "Added batch processing with sequential and parallel execution [#11](https://github.com/spinje/pflow/pull/11)",
    "index": 79,
    "context": {
      "commit_message": "Merge pull request #11 from spinje/feat/batch-processing",
      "task_number": 96,
      "files_changed": ".taskmaster/tasks/task_39/starting-context/task-39-handover-01.md, .taskmaster/tasks/task_39/starting-context/task-39-handover-from-task-96.md, .taskmaster/tasks/task_96/implementation/batch-error-display-fix.md, .taskmaster/tasks/task_96/implementation/batch-error-display-implementation-plan.md, .taskmaster/tasks/task_96/implementation/implementation-plan.md ... and 32 more files",
      "pr_title": "feat: add batch processing with sequential and parallel execution",
      "pr_body": "## Summary\n\nAdds batch processing capability to pflow workflows, enabling a single node to process multiple items from an array with isolated contexts per item. Supports both sequential and parallel execution modes.\n\n**Task 96: Support Batch Processing in Workflows**\n\n## Changes\n\n### Phase 1 - Sequential Batch Processing\n- Add `batch` configuration to IR schema (`items`, `as`, `error_handling`)\n- Implement `PflowBatchNode` with isolated context per item\n- Integrate batch wrapper in compiler between Namespace and Instrumented wrappers\n- Two-layer error detection (exceptions + result dict error key)\n\n### Phase 2 - Parallel Batch Processing\n- Add `parallel`, `max_concurrent`, `max_retries`, `retry_wait` to schema\n- Refactor to inherit from `Node` (not `BatchNode`) for thread safety\n- Implement `ThreadPoolExecutor` with deep copy per thread\n- Thread-safe retry with local variable (avoids `self.cur_retry` race condition)\n- Deep copy node chain per thread (avoids `TemplateAwareNodeWrapper` race condition)\n- Preserve result ordering despite async completion\n\n### Key Design Decisions\n- Batch wrapper **outside** namespace for root-level item injection\n- Shallow copy of shared store (shares `__llm_calls__` for LLM tracking)\n- Deep copy of node chain per thread for isolation\n- Same `_exec_single()` code path for both sequential and parallel modes\n\n## Example IR Syntax\n\n```json\n{\n  \"id\": \"summarize\",\n  \"type\": \"llm\",\n  \"batch\": {\n    \"items\": \"${list_files.files}\",\n    \"as\": \"file\",\n    \"parallel\": true,\n    \"max_concurrent\": 5,\n    \"max_retries\": 3,\n    \"error_handling\": \"continue\"\n  },\n  \"params\": {\"prompt\": \"Summarize: ${file}\"}\n}\n```\n\n## Files Changed\n\n| File | Purpose |\n|------|---------|\n| `src/pflow/core/ir_schema.py` | Add `BATCH_CONFIG_SCHEMA` with Phase 1+2 fields |\n| `src/pflow/runtime/batch_node.py` | Core `PflowBatchNode` implementation |\n| `src/pflow/runtime/compiler.py` | Integrate batch wrapper in compilation chain |\n| `examples/batch-test.json` | Sequential batch example |\n| `examples/batch-test-parallel.json` | Parallel batch example |\n\n## Documentation\n\n- `.taskmaster/tasks/task_96/task-96.md` - Task specification\n- `.taskmaster/tasks/task_96/implementation/progress-log.md` - Implementation progress\n- `.taskmaster/tasks/task_96/research/task-39-synergy-analysis.md` - Synergy with Task 39 (fan-out)\n\n## Testing\n\n- **98 batch-related tests** added (26 schema + 57 batch node + 15 compiler)\n- All 3562 project tests pass\n- `make check` passes (linting, type checking)\n\n```bash\nmake test  # Run all tests\nmake check # Run linting and type checks\n```\n\n## Task 39 Synergy\n\nThis implementation establishes patterns that Task 39 (Task Parallelism / Fan-out) will reuse:\n- Deep copy per thread pattern for isolation\n- ThreadPoolExecutor usage\n- Error handling modes (fail_fast/continue)\n\nTask 39's `ParallelGroupNode` will be ~20 lines using these established patterns.",
      "pr_link": "https://github.com/spinje/pflow/pull/11",
      "pr_number": 11
    }
  },
  {
    "draft": "Migrated claude-code node to Claude Agent SDK and streamlined parameters [#10](https://github.com/spinje/pflow/pull/10)",
    "index": 80,
    "context": {
      "commit_message": "Merge pull request #10 from spinje/feat/claude-agent-sdk-migration",
      "task_number": null,
      "files_changed": ".taskmaster/tasks/task_99/starting-context/task-99-handover.md, .taskmaster/tasks/task_99/task-99.md, NOTICE, docs/reference/nodes/claude-code.mdx, pyproject.toml ... and 3 more files",
      "pr_title": "feat(claude-code): migrate to Claude Agent SDK and streamline parameters",
      "pr_body": "## Summary\n\nMigrates the ClaudeCodeNode from the deprecated `claude-code-sdk` to the new `claude-agent-sdk` v0.1.18, streamlines parameter naming to match the SDK, and adds sandbox support for command execution isolation.\n\n## Breaking changes\n\n| Before | After | Reason |\n|--------|-------|--------|\n| `task` | `prompt` | Matches SDK naming |\n| `working_directory` | `cwd` | Matches SDK naming |\n| `context` | *(removed)* | Users include context in prompt directly |\n\n## Changes\n\n- **SDK Migration**: Update from `claude-code-sdk>=0.0.25` to `claude-agent-sdk>=0.1.18`\n- **New sandbox parameter**: Add command isolation configuration with options:\n  - `enabled` - Enable sandbox mode\n  - `autoAllowBashIfSandboxed` - Auto-allow bash when sandboxed\n  - `excludedCommands` - Commands that bypass sandbox (e.g., `[\"docker\"]`)\n  - `allowUnsandboxedCommands` - Allow model to request unsandboxed execution\n  - `network` - Network settings (allowLocalBinding, allowUnixSockets)\n- **Parameter streamlining**: Rename parameters to match Claude Agent SDK naming conventions\n- **Documentation**: Complete rewrite of `docs/reference/nodes/claude-code.mdx` with:\n  - Updated parameter tables\n  - Sandbox configuration documentation\n  - Links to official Claude Agent SDK docs\n  - Expanded tools table (Glob, Grep, LS, WebFetch, WebSearch)\n\n## Files changed\n\n```\npyproject.toml                          - SDK dependency update\nNOTICE                                  - License attribution update\nsrc/pflow/nodes/claude/claude_code.py   - Node implementation\ntests/test_nodes/test_claude/...        - Updated tests (41 pass)\ndocs/reference/nodes/claude-code.mdx    - User documentation\nuv.lock                                 - Lockfile update\n```\n\n## Testing\n\n```bash\nmake test   # 41 tests pass\nmake check  # All linting/type checks pass\n```\n\nManually verified the node works with:\n- `prompt` parameter (basic execution)\n- `cwd` parameter (working directory)\n- `sandbox` parameter validation",
      "pr_link": "https://github.com/spinje/pflow/pull/10",
      "pr_number": 10
    }
  },
  {
    "draft": "Enabled all tools by default and added session and timeout parameters to the Claude Code node",
    "index": 81,
    "context": {
      "commit_message": "feat(claude-code): enable all tools by default and add session/timeout params",
      "task_number": null,
      "files_changed": ".taskmaster/tasks/task_99/starting-context/task-99-handover.md, .taskmaster/tasks/task_99/task-99.md, scratchpads/super-code-reviewer/research-findings.md, src/pflow/nodes/claude/claude_code.py, tests/test_nodes/test_claude/test_claude_code.py",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  },
  {
    "draft": "Fixed the planner to handle missing file paths gracefully",
    "index": 82,
    "context": {
      "commit_message": "fix: planner now handles missing file paths gracefully",
      "task_number": null,
      "files_changed": "src/pflow/planning/prompts/requirements_analysis.md, src/pflow/planning/prompts/workflow_system_overview.md",
      "pr_title": "",
      "pr_body": "",
      "pr_link": "",
      "pr_number": null
    }
  }
]
```
