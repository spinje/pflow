# v0.8.0 Release Context

Generated: 2026-02-10
This file contains implementation context for AI agents and release verification.

---

## Changelog

## v0.8.0 (2026-02-10)

- Renamed PyPI package from `pflow` to `pflow-cli` to resolve naming conflicts.
- Changed LLM node output to always return raw strings instead of automatically parsing JSON, preventing silent data loss when prose contains code blocks.
- Added `pflow skill` command group to publish workflows as AI agent skills for Claude Code, Cursor, Codex, and Copilot [#81](https://github.com/spinje/pflow/pull/81) ([Task 119](.taskmaster/tasks/task_119/task-review.md))
- Added `pflow workflow history` command to view execution logs and previous inputs [#81](https://github.com/spinje/pflow/pull/81)
- Added execution duration tracking (last run and running average) to workflow metadata.
- Fixed CLI parameter parsing to respect declared input types, preventing numeric strings (e.g., Discord IDs) from being coerced to integers [#84](https://github.com/spinje/pflow/pull/84)
- Fixed contradictory validation error messages when accessing outputs from batch processing nodes [#86](https://github.com/spinje/pflow/pull/86)
- Fixed environment variable expansion in MCP server configurations to correctly resolve `${VAR}` in URLs and `settings.json` references.
- Fixed code node runtime errors to display workflow file line numbers instead of code-block relative lines.
- Improved workflow discovery matching accuracy by including node IDs and input names in the LLM context.
- Improved markdown parser error messages to identify nested backticks as the likely cause of untagged code blocks.

---

## Skipped Changes (Verification)

Review these to ensure nothing was incorrectly classified as internal:

- add demo to readme
- improve example release workflow
- docs fixes
- pre-release cleanup: remove ephemeral artifacts from completed tasks
- remove rm hook
- cleanup
- docs: update all docs to match new writing style guide
- add code node docs and task-124 spec
- more work on changelog workflow
- update instructions for writing documentation
- docs: add documentation for code node
- version 0.7.0 changelog generated, added slack message after generating a changelog
- remove old claude commands
- update readme
- remove cursor rules
- research for task 123
- update license
- task 123: OAuth authentication for MCP HTTP servers (spec + research)
- update all agents to opus 4.6
- flesh out task 77
- update task 46 spec: generate plain Python, not PocketFlow code
- task 122 openclaw integration
- refactor: chunked docs summarization, node reordering and renaming in generate-changelog workflow
- docs: braindump for JSON auto-parse design audit (Task 66)
- docs: document JSON auto-parsing and type coercion as cross-cutting concern
- docs: add task for node testability
- working on generate-changelog workflow
- research: Add TypedDict output field validation suggestion for Task 112
- docs: Elevate run-step to in-scope for Task 106
- docs: Note browsing node workflows context gap for Task 59
- mistake to avoid for agents
- update instructions for when to execute directly
- docs: Add batch output structure warning to agent instructions
- use opus 4.6 for review
- perf: Reduce timeout test from 0.5s to 0.05s to fix parallel test slowdown
- minor fix for agent instrcutions
- Merge pull request #82 from spinje/docs/steps-component-usage
- research for task 59
- minor fix for agent instructions
- Remove dead code: unused exceptions, error classes, and functions
- instructions improvements
- update claude.md so its up to date
- Upgrade mypy, ruff, click, pydantic-settings, and dev tools
- Upgrade ruff 0.11→0.15 and fix new lint rules
- Update CLAUDE.md to reflect recent task completions
- Add SECURITY.md and update CONTRIBUTING.md for open source

---

## Documentation Changes

### docs/CLAUDE.md

*Commits: a7cf397, 37a08a9*

- Overhauled documentation philosophy to focus on users reading workflows built by agents rather than writing them manually.
- Added comprehensive "Voice and Substance" guidelines emphasizing technical mechanism over marketing evaluation and specific "Banned words" (e.g., seamless, powerful, leverage).
- Introduced "Quality tests" (Tired engineer test, 7am test) to ensure documentation clarity and density.
- Standardized the skeleton for Node reference pages to include Note blocks for agent context, parameter tables, and workflow snippets.
- Updated installation examples to use `pflow-cli` instead of git URLs:
  ```bash
  uv tool install pflow-cli
  ```

### docs/docs.json

*Commits: 4f699a7, 37a08a9, 157b4db*

- Added new navigation entries for skill publishing, CLI skill reference, and code node reference.
- Disabled the Blog and Website links in the navigation bar and sidebar anchors.

### docs/guides/debugging.mdx

*Commits: 388639f, 548f17a*

- Updated description and tone to align with the new technical writing style guide.
- Refined the explanation of the self-healing system, framing structured errors as the primary interface for AI agents.
- Added context on why pflow errors are highly specific due to the finite nature of node types and outputs.

### docs/guides/publishing-skills.mdx

*Commits: 548f17a, 157b4db*

- Added new guide for "Agent Skills," explaining how to make workflows automatically available to AI agents via symlinks.
- Added instructions for publishing workflows to specific tools:
  ```bash
  pflow skill save my-workflow --cursor --copilot
  ```
- Detailed the difference between Project skills (stored in `.claude/skills/`) and Personal skills (stored in home directory).
- Explained the "enrichment" process which automatically adds `## Usage` sections and instructions to published workflows.

### docs/guides/using-pflow.mdx

*Commits: 548f17a*

- Updated terminology from "executable pipelines" to "executable workflows"
- Simplified the API key setup instruction to focus on "discovery"
- Refined the product description to focus on repeated execution, consistency, and building a personal workflow library

### docs/how-it-works/batch-processing.mdx

*Commits: 548f17a*

- Updated batch processing definition to emphasize its declarative nature, concurrency, and error collection
- Clarified that node results explicitly pair the original `item` with outputs to preserve data lineage for downstream nodes

### docs/how-it-works/template-variables.mdx

*Commits: 548f17a*

- Refined the description of template variables to focus on the elimination of manual "glue code"
- Added a section explaining how explicit node output declarations allow pflow to catch field mismatches before runtime

### docs/index.mdx

*Commits: 548f17a*

- Added a conceptual comparison describing pflow as "shell scripts, but for AI operations"
- Refined the explanation of agent inefficiencies, focusing on inconsistency and token costs
- Clarified that workflows are compiled into reusable `.pflow.md` files
- Updated the "Agent-driven" section to highlight workflow discovery and chaining

### docs/integrations/claude-code.mdx

*Commits: 548f17a, 5cd866f, 157b4db*

- Migrated the MCP installation and verification guide to a structured `<Steps>` component
- Added a new section on publishing workflows as Claude Code skills:
  ```bash
  pflow skill save my-workflow
  pflow skill save my-workflow --personal
  ```

### docs/integrations/claude-desktop.mdx

*Commits: 548f17a, 5cd866f*

- Migrated the four-step MCP configuration and verification process to a `<Steps>` component

### docs/integrations/cursor.mdx

*Commits: 5cd866f, 157b4db*

- Reorganized the setup guide into `<Tabs>` for MCP server vs. CLI access and added `<Steps>` for configuration
- Added a new section on publishing workflows as Cursor-specific skills:
  ```bash
  pflow skill save my-workflow --cursor
  pflow skill save my-workflow --cursor --personal
  ```

### docs/integrations/vscode.mdx

*Commits: 548f17a, 5cd866f*

- Updated writing style for clarity and brevity
- Refactored manual installation steps into a structured `<Steps>` component

### docs/integrations/windsurf.mdx

*Commits: 548f17a, 5cd866f*

- Updated terminology from "execute" to "run" per new style guide
- Refactored manual installation steps into a structured `<Steps>` component

### docs/quickstart.mdx

*Commits: 548f17a, 5cd866f*

- Refactored the entire installation and setup flow into a `<Steps>` component
- Updated installation commands to use the `pflow-cli` package instead of git URLs:
  ```bash
  uv tool install pflow-cli
  pipx install pflow-cli
  ```
- Moved optional configuration and conceptual explanations into `<Accordion>` components
- Refined messaging around the benefits and costs of the LLM discovery feature

### docs/reference/cli/index.mdx

*Commits: 548f17a, 157b4db*

- Added documentation for the new `pflow skill` command:
  ```bash
  # Publish workflows as AI agent skills
  pflow skill
  ```
- Updated terminology from "execute" to "run" across command descriptions
- Expanded the `--validate-only` flag description to explain specific validation checks like template errors and type mismatches

### docs/reference/cli/mcp.mdx

*Commits: 548f17a*

- Minor terminology updates ("Executing" changed to "Running")

### docs/reference/cli/registry.mdx

*Commits: 548f17a*

- Updated terminology from "execute" to "run"
- Added clarification that `structure` mode protects data privacy by ensuring agents only see types and paths, not actual runtime data

### docs/reference/cli/settings.mdx

*Commits: 548f17a*

- Clarified the benefit of `structure` mode for data privacy and token efficiency when working with AI agents

### docs/reference/cli/skill.mdx

*Commits: 157b4db*

- Added comprehensive reference for the new `pflow skill` command group used to publish workflows to AI agents
- Documented supported integrations for Claude Code, Cursor, Codex, and Copilot
- Added documentation for `save`, `list`, and `remove` subcommands with usage examples:
  ```bash
  pflow skill save pr-analyzer --cursor --copilot
  ```
- Explained the "Workflow enrichment" process which automatically adds usage instructions and metadata to workflow files
- Added guidance on managing project-specific vs. personal (global) skills

### docs/reference/cli/workflow.mdx

*Commits: 548f17a, 157b4db*

- Added documentation for the `history` subcommand to track execution logs and previous inputs
- Included an example of viewing execution history for a workflow:
  ```bash
  pflow workflow history release-announcements
  ```

### docs/reference/experimental.mdx

*Commits: 548f17a*

- Updated descriptions for experimental features to align with the new documentation writing style guide
- Refined the recommendation for using external agents like Claude Code or Cursor to save on API costs

### docs/reference/nodes/code.mdx

*Commits: 0695b65, 53721e2*

- Added comprehensive documentation for the new Code node used for Python-based data transformation
- Detailed mandatory type annotations for all input variables and the `result` variable to enable build-time validation
- Documented supported Python types and the behavior of the `object` type which skips validation
- Added a warning that templates must be defined in the `inputs` dictionary rather than directly in the code block
- Included several usage examples including data filtering, merging sources, and using the standard library:
  ```python code
  users: list
  result: list = [u for u in users if u['status'] == 'active']
  ```
- Added security disclosure regarding non-sandboxed execution and instructions for managing third-party dependencies via `pipx` or `uv`

### docs/reference/nodes/file.mdx

*Commits: 548f17a*

- Updated description to emphasize piping output to Code or LLM nodes for content processing
- Refined language to match the new documentation style guide

### docs/reference/nodes/http.mdx

*Commits: 548f17a*

- Updated node description to highlight built-in authentication handling
- Refined language to match the new documentation style guide

### docs/reference/nodes/index.mdx

*Commits: 548f17a, 53721e2*

- Added the Code node to the reference index, node gallery, and summary table
- Added documentation regarding build-time workflow validation for templates and type mismatches
- Minor phrasing updates to the JSON auto-parsing explanation

### docs/reference/nodes/llm.mdx

*Commits: 548f17a, b7b0095*

- Updated description to focus on reasoning-based use cases like summarization and classification
- Changed the `response` output type from `any` to `str`
- Removed the automatic JSON parsing section to prevent data loss; parsing is now handled on-demand via template dot notation:
  ```markdown
  ${extract.response.people}
  ```

### docs/reference/nodes/mcp.mdx

*Commits: 388639f, 548f17a*

- Updated style to emphasize that MCP tools share the same validation and error handling as built-in nodes
- Added a direct link to the `pflow mcp` CLI reference documentation

### docs/reference/nodes/shell.mdx

*Commits: 4f699a7, 548f17a*

- Updated the shell node description to explicitly mention protections against fork bombs and recursive deletes.
- Clarified that using `stdin` prevents shell escaping issues specifically related to special characters in JSON.
- Specified that command validation occurs during workflow creation rather than at runtime.

### docs/roadmap.mdx

*Commits: 548f17a*

- Refined roadmap terminology for discovery and workflows to align with the new style guide.
- Added a section describing the iterative feedback loop between agent usage and workflow engine improvements.
- Updated the long-term ideas and contact sections with more concise phrasing.

---

## Draft Entries with Context

### [1/11] Fixed code node runtime errors to display workflow file line numbers and labeled fields (Location, Source) for clarity

*Commit: `9b6382e` fix: show workflow file line numbers in code node errors

Closes #89

Code node runtime errors now display the .pflow.md file line number
instead of only the code-block-relative line. Error output uses labeled
fields (Location, Source) for clarity.*
Files: src/pflow/core/ir_schema.py, src/pflow/core/markdown_parser.py, src/pflow/nodes/python/python_code.py, src/pflow/runtime/compiler.py, tests/test_core/test_markdown_parser.py ... and 1 more files

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [2/11] Fixed environment variable expansion in MCP server configurations to include URLs and variables defined via `pflow settings`

*Commit: `e567f7e` fix: expand env vars in entire MCP server config, including URLs and settings.json

${VAR} references in mcp-servers.json URLs were never expanded — the literal
string was sent to the HTTP client. Additionally, env vars set via
`pflow settings set-env` were ignored everywhere in the MCP config pipeline
(only os.environ was checked).

Fix: expand the entire server config dict once at entry points (discovery and
node prep) with include_settings=True and raise_on_missing=True, instead of
expanding individual fields scattered across build_auth_headers. This ensures
all fields (URL, headers, auth, env) are resolved from both os.environ and
settings.json, and missing vars produce a clear error with instructions.

Closes #88*
Files: src/pflow/mcp/auth_utils.py, src/pflow/mcp/discovery.py, src/pflow/nodes/mcp/node.py, tests/test_mcp/test_auth_utils.py, tests/test_mcp/test_env_var_defaults.py ... and 4 more files

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [3/11] Renamed package from `pflow` to `pflow-cli` and updated version to 0.8.0

*Commit: `12919ba` prepare pflow-cli for PyPI release

- Rename package from pflow to pflow-cli (pflow taken on PyPI)
- Bump version to 0.8.0
- Add proper metadata: description, keywords, classifiers, URLs
- Add sdist exclusions to reduce package from 18MB to 547KB
- Switch release workflow from PyPI token to OIDC Trusted Publishers
- Read version from importlib.metadata instead of hardcoding
- Update README with new install instructions*
Files: .github/workflows/on-release-main.yml, .taskmaster/tasks/task_49/implementation/progress-log.md, README.md, pyproject.toml, src/pflow/cli/main.py ... and 2 more files

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [4/11] Fixed silent data loss in the LLM node by removing automatic JSON parsing that discarded prose content when code blocks were present

*Commit: `b7b0095` fix: remove auto-JSON-parsing from LLM node, prevent silent data loss

Fixes #87

LLMNode.parse_json_response() silently discarded prose responses when
they contained JSON code blocks as examples — extracting the first
valid JSON block and dropping all surrounding text.

Replace with _strip_code_block() that only removes code fence syntax
(a transport artifact) without parsing JSON. Response is always stored
as a string. Downstream template system handles JSON access on demand
via dot notation (${node.response.field}).

- Remove parse_json_response and its json/Union imports
- Add _strip_code_block with startswith+endswith guard (never silently
  drops content)
- Change interface type from any to str
- Update docs and architecture reference*
Files: architecture/core-concepts/data-type-coercion.md, docs/reference/nodes/llm.mdx, src/pflow/nodes/llm/llm.py, tests/test_nodes/test_llm/test_llm.py

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [5/11] Improved workflow discovery matching accuracy by including node IDs and input names in the discovery context

*Commit: `2abeac2` fix: Improve workflow discovery matching accuracy

The LLM-powered workflow discovery was producing 5-10% lower confidence
scores after the migration to .pflow.md format. Three root causes:

1. Flow strings showed node types (shell → shell → code → llm) instead
   of descriptive node IDs (get-remote → classify-commits → write-changelog),
   giving the LLM almost no signal for complex workflows.

2. Workflow inputs were missing entirely from the discovery context.
   Input names like repo_path or target_url are the strongest signal
   for what a workflow is meant to do.

3. The prompt referenced Capabilities/For metadata fields that are
   no longer populated (MetadataGenerationNode is gated by Task 107),
   telling the LLM to verify against data that doesn't exist.

Changes:
- _build_node_flow: emit node IDs instead of types
- build_workflows_context: add Inputs/Optional lines from IR
- _adapt_prompt_to_context: strip Capabilities/For references from
  the prompt when those fields are absent from the context
- Update discovery prompt Step 2 to reference inputs and ID-based flows*
Task: 107
Files: src/pflow/planning/context_builder.py, src/pflow/planning/nodes.py, src/pflow/planning/prompts/discovery.md, tests/test_planning/unit/test_discovery_prompt_adaptation.py

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [6/11] Improved markdown parser error messages for bare code blocks to identify nested backticks as the likely cause

*Commit: `09740f2` feat: Detect nested backticks pattern in bare code block error

When a bare (untagged) code block follows a tagged block in the same
entity, the error now identifies the likely cause: nested ``` inside
the preceding block closed it prematurely. The message references
both the offending line and the original block's tag and line number.*
Files: src/pflow/core/markdown_parser.py, tests/test_core/test_markdown_parser.py

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [7/11] Resolved contradictory error message for batch node output access [#86](https://github.com/spinje/pflow/pull/86)

*Commit: `25c41ac` Merge pull request #86 from spinje/fix/batch-node-error-message

fix: Resolve contradictory error message for batch node output access (Fixes #85)*
PR: #86 — fix: Resolve contradictory error message for batch node output access (Fixes #85) (https://github.com/spinje/pflow/pull/86)
Files: src/pflow/runtime/template_validator.py, tests/test_runtime/test_template_validator.py

**PR Description**

## Summary

When accessing inner node outputs (like `llm_usage`) on a batch node, the validator produced a contradictory error: "does not output 'llm_usage'" while listing `${node.llm_usage}` as available with a checkmark. The "Common fix" suggested changing X to X.

Root cause: two separate code paths for determining node outputs — validation correctly handled batch, but error message generation re-derived outputs from the registry ignoring batch config.

## Changes

- Thread pre-computed `node_outputs` through the error generation chain (`_create_template_error` → `_create_node_reference_error` → `_get_node_outputs_description`) so error messages use the same source of truth as validation
- Rewrite `_get_node_outputs_description()` to use `node_outputs` instead of re-querying the registry
- Add batch-aware error routing: detect `is_batch_output` flag, check if attempted field exists in inner node's `items.structure`
- Add `_format_batch_inner_field_error()` for targeted "exists inside results" messages with corrected path
- Add `_build_paths_from_entries()` helper for consistent path flattening from node_outputs
- Replace 50-line registry fallback with simple one-liner (the old code preserved the exact bug pattern)
- Dynamic arrow alignment in batch error messages

## Explanation

The validator already computed the correct batch-aware outputs in `_extract_node_outputs()` and passed the result (`node_outputs`) to `_create_template_error()`. But this dict was never forwarded to the downstream error formatting functions, which independently re-queried the registry using just the inner node type (e.g., `"llm"`), producing the generic LLM interface instead of the batch-wrapped structure.

The fix threads `node_outputs` through the full error chain, eliminating the second code path entirely. This means any future changes to how outputs are computed automatically reflect in error messages — no parallel logic to keep in sync.

For batch nodes, the error now distinguishes two cases:
1. **Inner field exists** (e.g., `llm_usage` for LLM nodes) → Shows "uses batch processing" with corrected path `${node.results[0].llm_usage}`
2. **Field doesn't exist** (e.g., `foobar`) → Shows "does not output" with actual batch outputs listed

**Before:**
```
Node 'generate' (type: llm) does not output 'llm_usage'

Available outputs from 'generate':
  ✓ ${generate.llm_usage} (dict)         ← contradiction
  ...
Common fix: Change ${generate.llm_usage} to ${generate.llm_usage}   ← circular
```

**After (inner field exists):**
```
Node 'generate' uses batch processing.
'llm_usage' is not available at the top level — batch wraps outputs in a 'results' array.

  ${generate.results}                → list of all results
  ${generate.results[0].llm_usage}   → first item's llm_usage

To aggregate across items, pass ${generate.results} to a code node and iterate.
```

**After (field doesn't exist):**
```
Node 'generate' (type: llm, batch) does not output 'foobar'

Available outputs from 'generate':
  ✓ ${generate.results} (array)
  ✓ ${generate.results[0].response} (any)
  ✓ ${generate.results[0].llm_usage} (dict)
  ...
```

## Testing

5 tests covering all error paths (1 updated, 4 new):

| Test | Scenario |
|------|----------|
| `test_batch_does_not_expose_inner_outputs` | `${batch.response}` — strengthened assertions |
| `test_batch_error_shows_correct_path_for_llm_usage` | `${batch.llm_usage}` — the reported bug |
| `test_batch_error_for_nested_inner_path` | `${batch.llm_usage.input_tokens}` — nested sub-field |
| `test_batch_error_for_nonexistent_field` | `${batch.foobar}` — field missing entirely |
| `test_non_batch_error_message_uses_node_outputs` | Non-batch regression |

Run `make test` to verify all 3719 tests pass.

```
src/pflow/runtime/template_validator.py       | 234 +++++++++++------
tests/test_runtime/test_template_validator.py | 165 ++++++++++++-
2 files changed, 347 insertions(+), 52 deletions(-)
```

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [8/11] Improved "Code block has no tag" error message and agent instructions with guidance on nesting code blocks using 4+ backticks or tildes

*Commit: `e39e940` docs: Improve nested backticks guidance in error message and agent instructions

- Add tip to "Code block has no tag" error about using 4+ backticks
  or tildes for outer fence when nesting code examples
- Add "Nesting backticks" section to agent instructions with example
- Clean up awkward backtick escaping in key rules section*
Files: src/pflow/cli/resources/cli-agent-instructions.md, src/pflow/core/markdown_parser.py

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [9/11] Added execution duration tracking and running average to workflow metadata and CLI describe/history commands

*Commit: `5713603` feat: Add execution duration tracking to workflow metadata

Track last and average execution duration in workflow frontmatter:
- last_execution_duration_seconds: Duration of most recent run (rounded to 2 decimals)
- average_execution_duration_seconds: Running average across all executions

Display duration in workflow describe/history commands:
- Detailed: Shows duration and average on separate lines
- Compact: Shows "Duration: 1.52s (avg: 1.35s)"

Average is computed incrementally using running average formula,
no historical data storage required.*
Files: src/pflow/core/workflow_manager.py, src/pflow/execution/executor_service.py, src/pflow/execution/formatters/history_formatter.py, tests/test_execution/test_executor_service.py

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [10/11] Fixed CLI parameters to respect declared input types [#84](https://github.com/spinje/pflow/pull/84)

*Commit: `792fdea` Merge pull request #84 from spinje/fix/numeric-string-coercion

fix: Respect declared input types for CLI parameters*
PR: #84 — fix: Respect declared input types for CLI parameters (https://github.com/spinje/pflow/pull/84)
Files: .taskmaster/tasks/task_120/starting-context/braindump-numeric-string-coercion-context.md, .taskmaster/tasks/task_120/task-120.md, CLAUDE.md, src/pflow/core/CLAUDE.md, src/pflow/core/ir_schema.py ... and 8 more files

**PR Description**

## Summary

Fixes #83 - Numeric string inputs (like Discord snowflake IDs) were silently coerced to `int` despite being declared as `type: string` in workflow inputs.

## Changes

- **Type coercion in `prepare_inputs()`**: Added `coerce_input_to_declared_type()` that converts CLI-inferred values to match declared input types (e.g., int → string when `type: string`)
- **Template resolver fix**: Changed JSON auto-parsing to only parse containers (dict/list), not primitives - prevents `"1234"` from becoming `1234`
- **IR schema expansion**: Accept Python type aliases (`str`, `int`, `float`, `bool`, `dict`, `list`) alongside JSON Schema types
- **Helper function extraction**: Reduced `prepare_inputs()` complexity from 12 to 7 by extracting `_resolve_missing_input()` and `_coerce_provided_input()`

## Explanation

The bug had two root causes:

1. **CLI parsing**: `infer_type()` converts `channel_id="1458..."` to an integer before the workflow is loaded, so the declared `type: string` was never consulted
2. **Template resolution**: JSON auto-parsing in `resolve_nested()` converted string values like `"42"` back to integers

The fix addresses both:
1. `prepare_inputs()` now coerces values to match declared types after CLI parsing
2. Template resolver only auto-parses JSON into containers (dict/list), preserving primitive strings

Python type aliases were added to the IR schema so users can write `type: int` or `type: str` naturally, especially as we add more code node languages (TypeScript planned).

## Testing

- 69 new tests covering all coercion scenarios
- All 3712 tests pass
- Manual verification with Discord snowflake ID workflow

Run `make test` to verify all tests pass.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

### [11/11] Added `pflow skill` command to publish workflows as AI agent skills [#81](https://github.com/spinje/pflow/pull/81)

*Commit: `de8cd60` Merge pull request #81 from spinje/feat/task-119-publish-workflows-as-skills

feat: Add pflow skill command to publish workflows as AI agent skills*
Task: [119](.taskmaster/tasks/task_119/task-review.md)
PR: #81 — feat: Add pflow skill command to publish workflows as AI agent skills (https://github.com/spinje/pflow/pull/81)
Files: .taskmaster/tasks/task_119/implementation/implementation-plan.md, .taskmaster/tasks/task_119/implementation/progress-log.md, .taskmaster/tasks/task_119/implementation/workflow-history-spec.md, .taskmaster/tasks/task_119/task-119.md, .taskmaster/tasks/task_119/task-review.md ... and 27 more files

**PR Description**

## Summary

Implements Task 119 - publish saved workflows as AI agent skills for Claude Code, Cursor, Codex, and Copilot.

Skills are symlinks from tool-specific directories to saved workflows in `~/.pflow/workflows/`. The workflow file is enriched with a `## Usage` section and frontmatter metadata that AI tools can read.

## Task

119

## Changes

**New CLI commands:**
- `pflow skill save <name> [--personal] [--cursor/--codex/--copilot]` - Publish workflow as skill
- `pflow skill list` - List all pflow-managed skills across tools
- `pflow skill remove <name> [--personal] [--cursor/--codex/--copilot]` - Remove skill symlinks
- `pflow workflow history <name>` - Show execution history and last used inputs

**Core features:**
- Symlink-based architecture (single source of truth)
- Multi-target support (Claude Code, Cursor, Codex, Copilot)
- Idempotent operations (no `--force` needed)
- Workflow enrichment with `## Usage` section and frontmatter metadata
- Re-save detection hook to restore enrichment after `workflow save --force`
- Reserved name "skill" to prevent workflow naming conflicts

## Explanation

The design uses symlinks rather than copies so that changes to the saved workflow automatically propagate to all tools. The `## Usage` section is injected into the workflow file itself (before `## Steps`) and is silently ignored by the markdown parser during execution.

Multi-target support was added to make pflow useful beyond Claude Code. The same symlink pattern works for all tools - only the directory paths differ.

The `--force` flag was intentionally removed from the spec because symlink creation is naturally idempotent (pointing to the same file), and enrichment replaces rather than duplicates the `## Usage` section.

## Created Docs

- `.taskmaster/tasks/task_119/task-119.md` - Task specification
- `.taskmaster/tasks/task_119/task-review.md` - Implementation review
- `.taskmaster/tasks/task_119/implementation/progress-log.md` - Development log

## Testing

- 46 new tests added (24 service + 14 CLI + 5 formatter + 3 workflow manager)
- All 3663 tests pass
- All lint/type checks pass

Run `make test` to verify.

**Task 119 Review**

# Task 119 Review: Publish Workflows as Claude Code Skills

## Metadata
- **Implementation Date**: 2026-02-05
- **Final Test Count**: 3666 passed, 516 skipped

## Executive Summary

Added `pflow skill` CLI command group that publishes saved workflows as AI agent skills. Skills are symlinks from tool-specific directories (Claude Code, Cursor, Codex, Copilot) to saved workflows in `~/.pflow/workflows/`. The workflow file is enriched with a `## Usage` section and frontmatter metadata. Also added `pflow workflow history` command for retrieving execution history.

## Implementation Overview

### What Was Built

1. **`pflow skill save <name> [--personal] [--cursor] [--codex] [--copilot]`**
   - Enriches saved workflow with `name`/`description` frontmatter and `## Usage` section
   - Creates symlink: `{tool}/.skills/{name}/SKILL.md` → `~/.pflow/workflows/{name}.pflow.md`
   - Idempotent: if skill exists, just re-enriches (no `--force` needed)
   - Multi-target: combine flags to save to multiple tools in one command

2. **`pflow skill list`**
   - Scans all tool directories for pflow-managed symlinks
   - Groups output by workflow name
   - Shows broken links with fix commands

3. **`pflow skill remove <name> [--personal] [--cursor] [--codex] [--copilot]`**
   - Removes symlinks from specified tool directories
   - Leaves saved workflow enrichment intact

4. **`pflow workflow history <name>`**
   - Shows execution count, last run timestamp, status
   - Displays last used inputs (for parameter reuse)

5. **Re-save detection hook**
   - After `pflow workflow save --force`, re-enriches if workflow is a skill
   - Prevents `## Usage` loss when overwriting

### Key Deviation from Spec

**Removed `--force` flag**: Original spec required `--force` to overwrite existing skills. This was wrong because:
- Symlinks point to the same file — removing and recreating is a no-op
- The enrichment (`## Usage` injection) is idempotent — replaces, doesn't duplicate
- Simpler UX: `skill save` just works

**Added multi-target support**: Extended beyond Claude Code to support Cursor, Codex, and Copilot. Same symlink pattern, different directories.

## Files Modified/Created

### Core Changes

| File | Change |
|------|--------|
| `src/pflow/core/skill_service.py` | **NEW** — All business logic: enrichment, symlinks, scanning (~430 lines) |
| `src/pflow/cli/skills.py` | **NEW** — CLI command group with target_options decorator (~300 lines) |
| `src/pflow/cli/main_wrapper.py` | Added `"skill"` routing block |
| `src/pflow/core/workflow_manager.py` | Added `"skill"` to RESERVED_NAMES, frontmatter name override in `load()`/`list_all()` |
| `src/pflow/core/workflow_save_service.py` | Added `"skill"` to reserved names, re-enrich hook after save |
| `src/pflow/execution/formatters/history_formatter.py` | Added `format_workflow_history()` function |
| `src/pflow/cli/commands/workflow.py` | Added `history` subcommand |

### Test Files

| File | Coverage |
|------|----------|
| `tests/test_core/test_skill_service.py` | 24 tests — enrichment, symlinks, scanning, re-enrichment |
| `tests/test_cli/test_skills.py` | 14 tests — CLI commands with mocked services |
| `tests/test_execution/formatters/test_history_formatter.py` | 2 tests — formatter behavior |
| `tests/test_cli/test_workflow_commands.py` | 3 new tests — history CLI command |

## Integration Points & Dependencies

### Incoming Dependencies

- **AI coding tools** → skill files via symlinks (`SKILL.md`)
- **`pflow workflow save --force`** → `re_enrich_if_skill()` hook

### Outgoing Dependencies

- **`WorkflowManager`** — `_split_frontmatter_and_body()`, `_serialize_with_frontmatter()` for enrichment
- **markdown parser** — Enriched files must still parse (## Usage is ignored)

### Shared Store Keys

None — this feature doesn't touch execution.

## Architectural Decisions & Tradeoffs

### Key Decisions

| Decision | Reasoning | Alternative |
|----------|-----------|-------------|
| Symlinks not copies | Single source of truth, auto-updates | Copies would drift |
| Enrich saved file, not skill file | Symlink points to saved file, skill IS the saved file | Separate skill files would duplicate |
| No `--force` flag | Symlink recreation is no-op, enrichment is idempotent | Force flag implied destructive behavior that didn't exist |
| Multi-target via flags | One command saves to multiple tools | Separate commands per tool would be tedious |
| `SKILL_TARGETS` dict | Single source of truth for tool configs | Hardcoded paths would diverge |
| `target_options` decorator | DRY — adds same flags to save/remove | Duplicate click.option() calls |

### Frontmatter Position

`name` and `description` are placed **at TOP of frontmatter** using dict merge:
```python
frontmatter = {"name": name, "description": description, **frontmatter}
```
This ensures Claude Code (which reads YAML top-to-bottom) sees skill metadata first.

### Technical Debt

None incurred — implementation is clean and follows existing patterns.

## Testing Implementation

### Test Strategy

- **Service layer**: Direct tests using `tmp_path` for file isolation
- **CLI layer**: Mock service functions at import boundary
- **End-to-end**: `test_skill_symlink_readable_as_valid_skill` — full agent experience

### Critical Test Cases

| Test | What It Validates |
|------|-------------------|
| `test_skill_symlink_readable_as_valid_skill` | Full agent experience: save → enrich → symlink → read via symlink → valid skill content |
| `test_enrich_replaces_existing_usage` | ## Usage replacement (not duplication) on re-enrichment |
| `test_find_pflow_skills_ignores_non_pflow` | Only pflow symlinks returned, not native skills |
| `test_re_enrich_restores_usage_section_after_resave` | Re-save hook actually restores enrichment |

## Unexpected Discoveries

### Gotchas

1. **`_split_frontmatter_and_body()` is effectively static** — Doesn't use `self`, so we can call it on any WorkflowManager instance without initialization concerns.

2. **## Usage is silently ignored by parser** — Unknown `##` sections are skipped during parse, so enrichment doesn't break execution.

3. **Symlink target resolution** — Must handle both absolute and relative targets. Used `os.readlink()` + conditional resolution.

4. **Copilot has different personal/project paths** — `.github/skills/` (project) vs `~/.copilot/skills/` (personal). All other tools use same subdir for both.

### Edge Cases

- **Broken symlinks**: Show `[broken link]` in list, provide fix commands
- **Same workflow in multiple tools/scopes**: All shown, grouped by workflow name
- **Workflow deleted after skill creation**: Symlink becomes broken, detectable via `is_valid`

## Patterns Established

### Config-Driven Target System

```python
SKILL_TARGETS: dict[str, tuple[str, str]] = {
    "claude": (".claude/skills", ".claude/skills"),
    "cursor": (".cursor/skills", ".cursor/skills"),
    "codex": (".agents/skills", ".agents/skills"),
    "copilot": (".github/skills", ".copilot/skills"),
}
TARGET_LABELS: dict[str, str] = {...}
DEFAULT_TARGET = "claude"
```

Adding a new tool = update two dicts + add one line in `target_options` decorator.

### Flag Decorator Pattern

```python
def target_options(action: str) -> Callable[[F], F]:
    """Decorator that adds --personal, --cursor, --codex, --copilot options."""
    def decorator(func: F) -> F:
        func = click.option("--copilot", ...)(func)
        # ... more options
        return func
    return decorator

@skill.command(name="save")
@target_options("Save to")  # Help text varies by action
def save_skill(...):
```

### Idempotent Operations

The `skill save` command demonstrates idempotent design:
- If skill doesn't exist → creates symlink → "Published"
- If skill exists → just re-enriches → "Updated"
- No `--force` needed — the operation is always safe

## Documentation

### New Files
| File | Purpose |
|------|---------|
| `docs/reference/cli/skill.mdx` | Full CLI reference for `pflow skill` commands |
| `docs/guides/publishing-skills.mdx` | Guide explaining why and how to publish skills |

### Modified Files
| File | Change |
|------|--------|
| `docs/reference/cli/index.mdx` | Added `pflow skill` card and Related link |
| `docs/reference/cli/workflow.mdx` | Added `history` command documentation |
| `docs/integrations/claude-code.mdx` | Added "Publishing workflows as skills" section |
| `docs/integrations/cursor.mdx` | Added "Publishing workflows as skills" section |
| `docs/docs.json` | Added `skill` to CLI nav, `publishing-skills` to Guides nav |

### Documentation Approach
- Reference page has full command details; guide explains value proposition
- Both link to [Anthropic's Agent Skills docs](https://docs.anthropic.com/en/docs/agents-and-tools/agent-skills) instead of re-explaining the concept
- Key message: skills are auto-loaded at agent startup — no need for explicit discovery commands
- Used `<Steps>` component for the 2-step publishing workflow

## Breaking Changes

None — new feature, no existing interfaces modified.

## AI Agent Guidance

### Quick Start for Related Tasks

1. **Read first**: `src/pflow/core/skill_service.py` — all business logic
2. **Then**: `src/pflow/cli/skills.py` — CLI integration
3. **Pattern to follow**: `SKILL_TARGETS` config dict for adding new tools

### How to Add a New Tool Target

1. Add to `SKILL_TARGETS` in `skill_service.py`:
   ```python
   "newtool": (".newtool/skills", ".newtool/skills"),
   ```
2. Add to `TARGET_LABELS`:
   ```python
   "newtool": "New Tool",
   ```
3. Add flag in `target_options()` decorator in `skills.py`:
   ```python
   func = click.option("--newtool", ...)(func)
   ```
4. Add to `_get_targets_from_flags()`:
   ```python
   ("newtool", newtool)
   ```

### Common Pitfalls

1. **Don't use `--force` pattern for symlinks** — Symlinks pointing to same file don't need force semantics
2. **Test with real symlinks** — Don't mock `os.symlink()`, use `tmp_path` for real file operations
3. **Check both symlink existence AND broken state** — `path.exists()` returns False for broken symlinks, use `path.is_symlink()` to detect them

### Test-First Recommendations

When modifying skill functionality:
1. Run `pytest tests/test_core/test_skill_service.py -v` — service logic
2. Run `pytest tests/test_cli/test_skills.py -v` — CLI behavior
3. The end-to-end test `test_skill_symlink_readable_as_valid_skill` is the most important — it validates the full agent experience

---

*Generated from implementation context of Task 119*